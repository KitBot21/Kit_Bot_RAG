#!/usr/bin/env python3
"""
RAG Demo: Qdrant Retrieval + LLM Generation
"""
import pandas as pd
from pathlib import Path
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from openai import OpenAI
import os
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

PROJECT_ROOT = Path(__file__).resolve().parent

class RAGSystem:
    def __init__(self, collection_name='kit_corpus_bge_filtered', 
                 retriever_model='BAAI/bge-m3',
                 llm_provider='openai',  # 'openai' or 'ollama'
                 llm_model='gpt-4o-mini'):
        """
        RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        
        Args:
            collection_name: Qdrant ì»¬ë ‰ì…˜ ì´ë¦„
            retriever_model: ì„ë² ë”© ëª¨ë¸
            llm_provider: LLM ì œê³µì ('openai' or 'ollama')
            llm_model: LLM ëª¨ë¸ ì´ë¦„
        """
        print("ğŸš€ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        
        # Retriever ë¡œë“œ
        print(f"  ğŸ“¥ Retriever ë¡œë”©: {retriever_model}")
        self.retriever = SentenceTransformer(retriever_model)
        
        # Qdrant í´ë¼ì´ì–¸íŠ¸
        self.qdrant_client = QdrantClient('localhost', port=6333)
        self.collection_name = collection_name
        
        # LLM ì„¤ì •
        self.llm_provider = llm_provider
        self.llm_model = llm_model
        
        if llm_provider == 'openai':
            self.llm_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        elif llm_provider == 'ollama':
            # OllamaëŠ” ë¡œì»¬ì—ì„œ ì‹¤í–‰ (http://localhost:11434)
            self.llm_client = OpenAI(
                base_url='http://localhost:11434/v1',
                api_key='ollama'  # OllamaëŠ” API í‚¤ ë¶ˆí•„ìš”
            )
        
        print(f"  ğŸ¤– LLM: {llm_provider}/{llm_model}")
        print("âœ… RAG ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!\n")
    
    def retrieve(self, query, top_k=3):
        """
        ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ë¬¸ì„œ ê²€ìƒ‰
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            top_k: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
            
        Returns:
            List of (text, score, metadata)
        """
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_vector = self.retriever.encode(query, normalize_embeddings=True).tolist()
        
        # Qdrant ê²€ìƒ‰
        search_result = self.qdrant_client.search(
            collection_name=self.collection_name,
            query_vector=query_vector,
            limit=top_k
        )
        
        # ê²°ê³¼ í¬ë§·íŒ…
        results = []
        for hit in search_result:
            results.append({
                'text': hit.payload.get('text', ''),
                'score': hit.score,
                'chunk_id': hit.payload.get('chunk_id', ''),
                'title': hit.payload.get('title', ''),
                'url': hit.payload.get('url', '')
            })
        
        return results
    
    def generate(self, query, contexts, stream=False):
        """
        ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ LLM ë‹µë³€ ìƒì„±
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            contexts: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            stream: ìŠ¤íŠ¸ë¦¬ë° ì—¬ë¶€
            
        Returns:
            LLM ìƒì„± ë‹µë³€
        """
        # ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ ìƒì„± (ì¶œì²˜ ì •ë³´ í¬í•¨)
        context_str = "\n\n".join([
            f"[ë¬¸ì„œ {i+1}]\nì œëª©: {ctx.get('title', 'ì œëª©ì—†ìŒ')}\në‚´ìš©: {ctx['text']}" 
            for i, ctx in enumerate(contexts)
        ])
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ê°œì„ ëœ ë²„ì „)
        system_prompt = """ë‹¹ì‹ ì€ ê¸ˆì˜¤ê³µê³¼ëŒ€í•™êµ í•™ìƒë“¤ì„ ë•ëŠ” ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ Kit_Botì…ë‹ˆë‹¤.
ì œê³µëœ ë¬¸ì„œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìƒì„¸í•˜ë©° ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ë‹µë³€ ì‘ì„± ê°€ì´ë“œë¼ì¸:
1. **ì •í™•ì„±**: ì œê³µëœ ë¬¸ì„œì— ìˆëŠ” ì •ë³´ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
2. **ì™„ì„±ë„**: ì§ˆë¬¸ì— ëŒ€í•œ ì™„ì „í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”. ê´€ë ¨ëœ ëª¨ë“  ì„¸ë¶€ì‚¬í•­ì„ í¬í•¨í•˜ì„¸ìš”
3. **êµ¬ì¡°í™”**: ë³µì¡í•œ ì •ë³´ëŠ” ë²ˆí˜¸ë‚˜ ê¸€ë¨¸ë¦¬ ê¸°í˜¸ë¡œ êµ¬ì¡°í™”í•˜ì—¬ ì œì‹œí•˜ì„¸ìš”
4. **ì‹¤ìš©ì„±**: 
   - ì ˆì°¨ë‚˜ ë°©ë²•ì„ ì„¤ëª…í•  ë•ŒëŠ” ë‹¨ê³„ë³„ë¡œ ëª…í™•í•˜ê²Œ ì•ˆë‚´í•˜ì„¸ìš”
   - ë‚ ì§œ, ì‹œê°„, ê¸ˆì•¡, ì—°ë½ì²˜ ë“± êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ë¹ ì§ì—†ì´ ì œê³µí•˜ì„¸ìš”
   - ê´€ë ¨ URLì´ë‚˜ ì—°ë½ì²˜ê°€ ìˆë‹¤ë©´ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”
5. **í•œê³„ ì¸ì •**: ë¬¸ì„œì— ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ "ì œê³µëœ ì •ë³´ë¡œëŠ” [êµ¬ì²´ì  ë¶€ë¶„]ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ëª…í™•íˆ ë°íˆì„¸ìš”

ë‹µë³€ í˜•ì‹:
- ì§ì ‘ì ì´ê³  ëª…í™•í•œ ë‹µë³€ìœ¼ë¡œ ì‹œì‘í•˜ì„¸ìš”
- í•„ìš”ì‹œ ì„¸ë¶€ ì‚¬í•­ì„ ì¶”ê°€ë¡œ ì„¤ëª…í•˜ì„¸ìš”
- í•™ìƒ ì…ì¥ì—ì„œ ì¶”ê°€ë¡œ í•„ìš”í•œ ì •ë³´ê°€ ìˆë‹¤ë©´ í•¨ê»˜ ì•ˆë‚´í•˜ì„¸ìš”"""

        user_prompt = f"""ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ìƒì„¸í•˜ê³  ì™„ì „í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

<ì°¸ê³  ë¬¸ì„œ>
{context_str}
</ì°¸ê³  ë¬¸ì„œ>

<í•™ìƒ ì§ˆë¬¸>
{query}
</í•™ìƒ ì§ˆë¬¸>

ë‹µë³€:"""
        
        # LLM í˜¸ì¶œ
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        if stream:
            # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ
            response = self.llm_client.chat.completions.create(
                model=self.llm_model,
                messages=messages,
                stream=True,
                temperature=0.3,
                max_tokens=800  # ì¦ê°€: ë” ìƒì„¸í•œ ë‹µë³€
            )
            return response
        else:
            # ì¼ë°˜ ëª¨ë“œ
            response = self.llm_client.chat.completions.create(
                model=self.llm_model,
                messages=messages,
                temperature=0.3,
                max_tokens=800  # ì¦ê°€: ë” ìƒì„¸í•œ ë‹µë³€
            )
            return response.choices[0].message.content
    
    def query(self, question, top_k=5, verbose=True):  # Top-3 â†’ Top-5ë¡œ ì¦ê°€
        """
        ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            top_k: ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜ (ê¸°ë³¸ê°’ 5ë¡œ ì¦ê°€)
            verbose: ìƒì„¸ ì •ë³´ ì¶œë ¥ ì—¬ë¶€
            
        Returns:
            ë‹µë³€ ë° ê²€ìƒ‰ ê²°ê³¼
        """
        if verbose:
            print(f"\n{'='*80}")
            print(f"â“ ì§ˆë¬¸: {question}")
            print(f"{'='*80}\n")
        
        # 1. ê²€ìƒ‰
        if verbose:
            print(f"ğŸ” ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘... (Top-{top_k})")
        
        contexts = self.retrieve(question, top_k=top_k)
        
        if verbose:
            print(f"\nğŸ“š ê²€ìƒ‰ëœ ë¬¸ì„œ:")
            for i, ctx in enumerate(contexts):
                print(f"\n[ë¬¸ì„œ {i+1}] (ìœ ì‚¬ë„: {ctx['score']:.3f})")
                print(f"ì œëª©: {ctx['title']}")
                print(f"ë‚´ìš©: {ctx['text'][:200]}...")
                if ctx['url']:
                    print(f"URL: {ctx['url']}")
        
        # 2. ë‹µë³€ ìƒì„±
        if verbose:
            print(f"\nğŸ¤– LLM ë‹µë³€ ìƒì„± ì¤‘...")
        
        answer = self.generate(question, contexts)
        
        if verbose:
            print(f"\n{'='*80}")
            print(f"ğŸ’¬ ë‹µë³€:")
            print(f"{'='*80}")
            print(answer)
            print(f"{'='*80}\n")
        
        return {
            'question': question,
            'answer': answer,
            'contexts': contexts
        }

def main():
    """ëŒ€í™”í˜• RAG ë°ëª¨"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ê¸ˆì˜¤ê³µëŒ€ RAG ì±—ë´‡')
    parser.add_argument('--provider', default='openai', choices=['openai', 'ollama'],
                        help='LLM ì œê³µì')
    parser.add_argument('--model', default='gpt-4o-mini',
                        help='LLM ëª¨ë¸ ì´ë¦„')
    parser.add_argument('--top-k', type=int, default=5,  # ê¸°ë³¸ê°’ 3 â†’ 5
                        help='ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜')
    parser.add_argument('--query', type=str, default=None,
                        help='ë‹¨ì¼ ì§ˆë¬¸ (ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ëŒ€í™”í˜• ëª¨ë“œ)')
    args = parser.parse_args()
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    rag = RAGSystem(
        llm_provider=args.provider,
        llm_model=args.model
    )
    
    if args.query:
        # ë‹¨ì¼ ì§ˆë¬¸ ëª¨ë“œ
        rag.query(args.query, top_k=args.top_k)
    else:
        # ëŒ€í™”í˜• ëª¨ë“œ
        print("\n" + "="*80)
        print("ğŸ“ ê¸ˆì˜¤ê³µëŒ€ AI ì–´ì‹œìŠ¤í„´íŠ¸")
        print("="*80)
        print("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: 'quit' ë˜ëŠ” 'exit')\n")
        
        while True:
            try:
                question = input("â“ ì§ˆë¬¸: ").strip()
                
                if question.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
                    print("\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                
                if not question:
                    continue
                
                rag.query(question, top_k=args.top_k)
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}\n")

if __name__ == "__main__":
    main()
