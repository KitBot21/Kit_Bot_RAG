# rag_core_hybrid.py - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë²„ì „ (BM25 + Semantic)
import os
import json
from typing import List, Dict, Any, Tuple, Optional
from dotenv import load_dotenv
from datetime import datetime
import pytz
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.http import models as qm
from openai import OpenAI
from rank_bm25 import BM25Okapi
import re

from core.router import classify_query_intent, rerank_with_boost

load_dotenv()

# --------------------
# í™˜ê²½ ì„¤ì •
# --------------------
QDRANT_URL = os.getenv("QDRANT_URL", "http://localhost:6333")
COLLECTION_NAME = os.getenv("QDRANT_COLLECTION", "kitbot_docs_bge")
EMBED_MODEL_NAME = os.getenv("EMBED_MODEL_NAME", "BAAI/bge-m3")
OPENAI_MODEL = os.getenv("OPENAI_CHAT_MODEL", "gpt-4o-mini")

# --------------------
# ì‹±ê¸€í†¤
# --------------------
_qdrant_client: QdrantClient | None = None
_embed_model: SentenceTransformer | None = None
_llm_client: OpenAI | None = None
_bm25_index = None
_bm25_documents = None


def get_qdrant_client() -> QdrantClient:
    global _qdrant_client
    if _qdrant_client is None:
        _qdrant_client = QdrantClient(url=QDRANT_URL)
    return _qdrant_client


def get_embed_model() -> SentenceTransformer:
    global _embed_model
    if _embed_model is None:
        print("â³ ìž„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...", EMBED_MODEL_NAME)
        _embed_model = SentenceTransformer(EMBED_MODEL_NAME)
    return _embed_model


def get_llm_client() -> OpenAI:
    global _llm_client
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise RuntimeError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
    if _llm_client is None:
        _llm_client = OpenAI(api_key=api_key)
    return _llm_client


# --------------------
# BM25 ì¸ë±ìŠ¤ êµ¬ì¶•
# --------------------
def tokenize_korean(text: str) -> List[str]:
    """ê°œì„ ëœ í•œêµ­ì–´ í† í¬ë‚˜ì´ì € (í˜•íƒœì†Œ ë¶„ì„ + N-gram)"""
    # 1. ê¸°ë³¸ ì •ì œ
    text = re.sub(r'[^\w\sê°€-íž£]', ' ', text)
    text = text.lower()
    
    # 2. ê³µë°± ê¸°ë°˜ í† í°í™”
    tokens = text.split()
    
    # 3. ì¶”ê°€ N-gram ìƒì„± (2-3ê¸€ìž ë‹¨ìœ„)
    ngrams = []
    for token in tokens:
        if len(token) >= 2:
            # 2-gram
            for i in range(len(token) - 1):
                ngrams.append(token[i:i+2])
            # 3-gram
            if len(token) >= 3:
                for i in range(len(token) - 2):
                    ngrams.append(token[i:i+3])
    
    return tokens + ngrams


def build_bm25_index():
    """Qdrantì—ì„œ ëª¨ë“  ë¬¸ì„œë¥¼ ë¡œë“œí•˜ì—¬ BM25 ì¸ë±ìŠ¤ êµ¬ì¶•"""
    global _bm25_index, _bm25_documents
    
    if _bm25_index is not None:
        return _bm25_index, _bm25_documents
    
    print("ðŸ” BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
    client = get_qdrant_client()
    
    # Qdrantì—ì„œ ëª¨ë“  ë¬¸ì„œ ìŠ¤í¬ë¡¤
    documents = []
    offset = None
    batch_size = 100
    
    while True:
        result = client.scroll(
            collection_name=COLLECTION_NAME,
            limit=batch_size,
            offset=offset,
            with_payload=True,
            with_vectors=False  # ë²¡í„°ëŠ” í•„ìš” ì—†ìŒ
        )
        
        points, next_offset = result
        
        if not points:
            break
        
        for point in points:
            payload = point.payload or {}
            text = (
                payload.get("chunk_text") or 
                payload.get("text") or 
                payload.get("main_text") or 
                payload.get("content") or ""
            )
            
            if text.strip():
                documents.append({
                    'id': point.id,
                    'text': text,
                    'payload': payload,
                    'score': getattr(point, 'score', 0.0)
                })
        
        if next_offset is None:
            break
        offset = next_offset
    
    print(f"   âœ… {len(documents)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
    
    # BM25 ì¸ë±ìŠ¤ ìƒì„±
    tokenized_corpus = [tokenize_korean(doc['text']) for doc in documents]
    _bm25_index = BM25Okapi(tokenized_corpus)
    _bm25_documents = documents
    
    print(f"   âœ… BM25 ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
    
    return _bm25_index, _bm25_documents


# --------------------
# í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
# --------------------
def hybrid_search(query: str, top_k: int = 5, alpha: float = 0.85) -> List[Any]:
    """
    ê°œì„ ëœ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: BM25 í‚¤ì›Œë“œ ê²€ìƒ‰ + BGE-M3 ì‹œë§¨í‹± ê²€ìƒ‰
    
    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        top_k: ìµœì¢… ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
        alpha: ì‹œë§¨í‹± ê²€ìƒ‰ ê°€ì¤‘ì¹˜ (0~1, 0=BM25ë§Œ, 1=ì‹œë§¨í‹±ë§Œ, ìµœì ê°’ 0.85)
    
    Returns:
        ìž¬ì •ë ¬ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    """
    client = get_qdrant_client()
    model = get_embed_model()
    
    # 1. ì‹œë§¨í‹± ê²€ìƒ‰ (BGE-M3)
    query_vec = model.encode(query).tolist()
    semantic_limit = top_k * 5  # ë” ë§Žì´ ê°€ì ¸ì™€ì„œ í•˜ì´ë¸Œë¦¬ë“œ ê²°í•©
    
    semantic_results = client.query_points(
        collection_name=COLLECTION_NAME,
        query=query_vec,
        limit=semantic_limit,
        with_payload=True,
    )
    
    # ì‹œë§¨í‹± ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (ID -> score)
    semantic_scores = {}
    semantic_docs = {}
    for point in semantic_results.points:
        semantic_scores[str(point.id)] = point.score
        semantic_docs[str(point.id)] = point
    
    # 2. BM25 í‚¤ì›Œë“œ ê²€ìƒ‰
    bm25_index, bm25_documents = build_bm25_index()
    tokenized_query = tokenize_korean(query)
    bm25_scores = bm25_index.get_scores(tokenized_query)
    
    # BM25 ì ìˆ˜ ê°œì„ ëœ ì •ê·œí™” (Min-Max Scaling)
    if len(bm25_scores) > 0:
        min_score = min(bm25_scores)
        max_score = max(bm25_scores)
        
        if max_score > min_score:
            # Min-Max ì •ê·œí™” (0~1)
            bm25_scores_normalized = (bm25_scores - min_score) / (max_score - min_score)
        else:
            # ëª¨ë“  ì ìˆ˜ê°€ ë™ì¼í•œ ê²½ìš°
            bm25_scores_normalized = bm25_scores / max(max_score, 1.0)
    else:
        bm25_scores_normalized = bm25_scores
    
    # BM25 ìƒìœ„ ë¬¸ì„œë§Œ ì„ íƒ (íš¨ìœ¨ì„± ê°œì„ )
    top_bm25_indices = sorted(range(len(bm25_scores_normalized)), 
                               key=lambda i: bm25_scores_normalized[i], 
                               reverse=True)[:semantic_limit]
    
    # BM25 ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (ìƒìœ„ ë¬¸ì„œë§Œ)
    bm25_score_dict = {}
    for idx in top_bm25_indices:
        doc_id = str(bm25_documents[idx]['id'])
        bm25_score_dict[doc_id] = float(bm25_scores_normalized[idx])
    
    # 3. í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ ê²°í•©)
    all_doc_ids = set(semantic_scores.keys()) | set(bm25_score_dict.keys())
    
    hybrid_scores = {}
    for doc_id in all_doc_ids:
        sem_score = semantic_scores.get(doc_id, 0.0)
        bm25_score = bm25_score_dict.get(doc_id, 0.0)
        
        # ê°€ì¤‘ ê²°í•©: alpha * semantic + (1-alpha) * bm25
        hybrid_scores[doc_id] = alpha * sem_score + (1 - alpha) * bm25_score
    
    # 4. ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
    sorted_doc_ids = sorted(hybrid_scores.keys(), key=lambda x: hybrid_scores[x], reverse=True)
    
    # 5. ìƒìœ„ ë¬¸ì„œ ì„ íƒ ë° ScoredPoint í˜•íƒœë¡œ ë³€í™˜
    final_results = []
    for doc_id in sorted_doc_ids[:top_k * 3]:  # boostë¥¼ ìœ„í•´ 3ë°°ìˆ˜ ê°€ì ¸ì˜¤ê¸°
        if doc_id in semantic_docs:
            point = semantic_docs[doc_id]
            # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ë¡œ ì—…ë°ì´íŠ¸
            point.score = hybrid_scores[doc_id]
            final_results.append(point)
        else:
            # BM25ì—ë§Œ ìžˆëŠ” ê²½ìš° (ì‹œë§¨í‹± ê²€ìƒ‰ì— ì—†ì—ˆë˜ ë¬¸ì„œ)
            # bm25_documentsì—ì„œ ì°¾ì•„ì„œ ë³€í™˜
            for bm25_doc in bm25_documents:
                if str(bm25_doc['id']) == doc_id:
                    # ìž„ì‹œ ScoredPoint ê°ì²´ ìƒì„±
                    class TempPoint:
                        def __init__(self, id, payload, score):
                            self.id = id
                            self.payload = payload
                            self.score = score
                    
                    point = TempPoint(
                        id=bm25_doc['id'],
                        payload=bm25_doc['payload'],
                        score=hybrid_scores[doc_id]
                    )
                    final_results.append(point)
                    break
    
    # 6. Boost ìž¬ì •ë ¬ ì ìš©
    intent = classify_query_intent(query)
    boosted_hits = rerank_with_boost(final_results, intent=intent, top_k=top_k)
    
    print(f"   ðŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (alpha={alpha}): ì‹œë§¨í‹± {len(semantic_results.points)}ê°œ + BM25 ìƒìœ„ {len(top_bm25_indices)}ê°œ â†’ ìµœì¢… {len(boosted_hits)}ê°œ")
    
    return boosted_hits


# --------------------
# ê²€ìƒ‰ ë‹¨ê³„ (APIìš©)
# --------------------
def retrieve_points(query: str, top_k: int = 5):
    """ê°œì„ ëœ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‚¬ìš© (alpha=0.85, ìµœì  ê· í˜•ì )"""
    return hybrid_search(query, top_k, alpha=0.85)


# --------------------
# ê²€ìƒ‰ ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ ë¸”ë¡ìœ¼ë¡œ ë³€í™˜
# --------------------
def build_context_blocks(points) -> str:
    blocks = []

    for i, p in enumerate(points):
        payload = p.payload or {}

        text = (
            payload.get("chunk_text")
            or payload.get("text")
            or payload.get("main_text")
            or payload.get("content")
            or ""
        )

        if not text.strip():
            continue

        meta = (
            f"[{i+1}] site={payload.get('site')} | "
            f"board={payload.get('board_name')} | "
            f"title={payload.get('title')} | "
            f"date={payload.get('created_at')} | "
            f"url={payload.get('url')}"
        )

        block = meta + "\n" + text
        blocks.append(block)

    return "\n\n---\n\n".join(blocks)


# --------------------
# LLM í˜¸ì¶œ
# --------------------
def call_llm(system_msg: str, user_msg: str, model: str = None) -> str:
    if model is None:
        model = OPENAI_MODEL

    client = get_llm_client()
    resp = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_msg},
        ],
        temperature=0.7,
        max_tokens=1000,
    )
    return resp.choices[0].message.content


# --------------------
# ì¼ì • ì •ë³´ ì¶”ì¶œ
# --------------------
def extract_schedule_info(answer: str) -> Dict[str, Optional[str]]:
    import re
    
    result = {
        "scheduleTitle": None,
        "startDate": None,
        "endDate": None
    }
    
    date_pattern = r'\d{4}[-ë…„]\s?\d{1,2}[-ì›”]\s?\d{1,2}ì¼?'
    dates = re.findall(date_pattern, answer)
    
    if dates:
        result["startDate"] = dates[0].replace('ë…„', '-').replace('ì›”', '-').replace('ì¼', '').replace(' ', '')
        if len(dates) > 1:
            result["endDate"] = dates[1].replace('ë…„', '-').replace('ì›”', '-').replace('ì¼', '').replace(' ', '')
    
    lines = answer.split('\n')
    if lines:
        first_line = lines[0].strip()
        if len(first_line) > 0 and len(first_line) < 100:
            result["scheduleTitle"] = first_line
    
    return result


# --------------------
# RAG with Sources
# --------------------
def rag_with_sources(query: str, top_k: int = 5):
    from core.router import classify_query_intent
    intent = classify_query_intent(query)

    if intent == "chitchat":
        system_msg = "ë„ˆëŠ” ê¸ˆì˜¤ê³µëŒ€ í•™ìƒë“¤ì„ ë•ëŠ” ì¹œì ˆí•œ AI ì±—ë´‡ 'KIT-BOT'ì´ì•¼. í•™ìƒì—ê²Œ ë‹¤ì •í•˜ê²Œ ëŒ€ë‹µí•´ì¤˜."
        answer = call_llm(system_msg, query)
        return answer, [], {"scheduleTitle": None, "startDate": None, "endDate": None}
    
    points = retrieve_points(query, top_k)
    
    SIMILARITY_THRESHOLD = 0.4

    if not points or points[0].score < SIMILARITY_THRESHOLD:
        print(f"   ðŸ“‰ ê²€ìƒ‰ ì ìˆ˜ ë¯¸ë‹¬: {points[0].score if points else 0} < {SIMILARITY_THRESHOLD}")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. í•™êµ ì •ë³´ì™€ ê´€ë ¨ì´ ì—†ê±°ë‚˜, í•´ë‹¹ ë‚´ìš©ì„ ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", [], {"scheduleTitle": None, "startDate": None, "endDate": None}

    context_text = build_context_blocks(points)
    
    now = datetime.now(pytz.timezone('Asia/Seoul'))
    today_str = now.strftime("%Yë…„ %mì›” %dì¼")
    current_year = now.year

    system_msg = (
        f"ë‹¹ì‹ ì€ êµ­ë¦½ê¸ˆì˜¤ê³µê³¼ëŒ€í•™êµ í•™ìƒë“¤ì„ ë•ëŠ” **ë‹¤ì •í•˜ê³  ì¹œì ˆí•œ AI ë©˜í†  'KIT-BOT'**ìž…ë‹ˆë‹¤.\n"
        f"í˜„ìž¬ ì‹œê°ì€ **{today_str}**ì´ì—ìš”.\n\n"
        "í•™ìƒì˜ ì§ˆë¬¸ì— ëŒ€í•´ [ê²€ìƒ‰ëœ ë¬¸ì„œ]ë¥¼ ê¼¼ê¼¼ížˆ í™•ì¸í•´ì„œ, **ë”°ëœ»í•˜ê³  ìƒëƒ¥í•œ ë§íˆ¬(í•´ìš”ì²´)**ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\n\n"
        
        "## 1. ë‹µë³€ ê°€ëŠ¥ ì—¬ë¶€ íŒë‹¨ (ê°€ìž¥ ì¤‘ìš”!)\n"
        "   - ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ê°€ [ê²€ìƒ‰ëœ ë¬¸ì„œ]ì— **ëª…í™•í•˜ê²Œ í¬í•¨ë˜ì–´ ìžˆì§€ ì•Šë‹¤ë©´**, ì–µì§€ë¡œ ì§€ì–´ë‚´ê±°ë‚˜ ë¹„ìŠ·í•œ ë‚´ìš©ì„ ë¬´ë¦¬í•˜ê²Œ ì—°ê²°í•˜ì§€ ë§ˆì„¸ìš”.\n"
        "   - ì •ë³´ê°€ ì—†ì„ ë•ŒëŠ” **'ì£„ì†¡í•˜ì§€ë§Œ, í•´ë‹¹ ë‚´ìš©ì€ í•™êµ ê³µì§€ë‚˜ ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ê°€ ì—†ë„¤ìš” ðŸ˜¥. í˜¹ì‹œ ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ë‹¤ì‹œ ì§ˆë¬¸í•´ ì£¼ì‹œê² ì–´ìš”?'**ë¼ê³  ì†”ì§í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\n\n"

        "## 2. ì„¼ìŠ¤ ìžˆëŠ” ì‹œê°„ í™•ì¸ (Time Awareness)\n"
        f"   - ë¬¸ì„œ ë‚´ìš©ì´ **ì˜¬í•´({current_year}ë…„)** ê²ƒì¸ì§€ ê¼­ í™•ì¸í•´ ì£¼ì„¸ìš”.\n"
        f"   - ë§Œì•½ ì˜¬í•´ ìµœì‹  ê³µì§€ê°€ ì—†ê³  ìž‘ë…„ ìžë£Œë§Œ ìžˆë‹¤ë©´, **'ì•„ì‰½ê²Œë„ ì•„ì§ {current_year}ë…„ë„ ê³µì§€ëŠ” ì˜¬ë¼ì˜¤ì§€ ì•Šì•˜ì–´ìš”.'**ë¼ê³  ì•ˆë‚´í•´ ì£¼ì„¸ìš”.\n\n"
        
        "## 3. ë³´ê¸° íŽ¸í•˜ê³  ì¹œì ˆí•œ ì„¤ëª…\n"
        "   - ë‚ ì§œ, ìž¥ì†Œ, ì „í™”ë²ˆí˜¸ ê°™ì€ í•µì‹¬ ì •ë³´ëŠ” **êµµê²Œ(**)** í‘œì‹œí•´ì„œ ëˆˆì— ìž˜ ë„ê²Œ í•´ì£¼ì„¸ìš”.\n"
        "   - ë³µìž¡í•œ ë‚´ìš©ì€ **ë¦¬ìŠ¤íŠ¸**ë¡œ ê¹”ë”í•˜ê²Œ ì •ë¦¬í•´ ì£¼ëŠ” ì„¼ìŠ¤ë¥¼ ë°œíœ˜í•´ ì£¼ì„¸ìš”.\n\n"
        
        "## 4. ë§ˆë¬´ë¦¬\n"
        "   - ë‹µë³€ ëì—ëŠ” **'ë” ê¶ê¸ˆí•œ ì ì´ ìžˆìœ¼ë©´ ì–¸ì œë“  ë¬¼ì–´ë´ ì£¼ì„¸ìš”!'** ë©˜íŠ¸ë¥¼ ë§ë¶™ì—¬ ì£¼ì„¸ìš”.\n"
    )

    user_msg = (
        f"ì§ˆë¬¸: {query}\n\n"
        f"--- ê²€ìƒ‰ëœ ë¬¸ì„œ ì‹œìž‘ ---\n"
        f"{context_text}\n"
        f"--- ê²€ìƒ‰ëœ ë¬¸ì„œ ë ---\n\n"
        f"ìœ„ ë¬¸ì„œë¥¼ ì •ë°€í•˜ê²Œ ë¶„ì„í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì¤˜."
    )
    
    answer = call_llm(system_msg, user_msg)

    schedule_data = {"scheduleTitle": None, "startDate": None, "endDate": None}

    negative_keywords = [
        "ì£„ì†¡í•˜ì§€ë§Œ", "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤", "ë¬¸ì„œì— ì—†ìŠµë‹ˆë‹¤"
    ]
    
    if any(neg in answer for neg in negative_keywords):
        final_sources = []
    else:
        final_sources = []
        for p in points:
            payload = p.payload or {}
            final_sources.append({
                "title": payload.get("title"),
                "url": payload.get("url"),
                "text": payload.get("text")
            })

    if final_sources and (intent == "schedule" or "202" in answer):
        extracted = extract_schedule_info(answer)
        if extracted.get("startDate"):
            schedule_data = extracted

    return answer, final_sources, schedule_data


def generate_answer(query: str, top_k: int = 5) -> str:
    answer, _, _ = rag_with_sources(query, top_k)
    return answer
