# rag_core_full.py - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ + ë¦¬ë­ì»¤ (ìµœê³  ì„±ëŠ¥)
import os
import json
from typing import List, Dict, Any, Tuple, Optional
from dotenv import load_dotenv
from datetime import datetime
import pytz
from sentence_transformers import SentenceTransformer, CrossEncoder
from qdrant_client import QdrantClient
from qdrant_client.http import models as qm
from openai import OpenAI
from rank_bm25 import BM25Okapi
import re

from core.router import classify_query_intent

load_dotenv()

# --------------------
# í™˜ê²½ ì„¤ì •
# --------------------
QDRANT_URL = os.getenv("QDRANT_URL", "http://localhost:6333")
COLLECTION_NAME = os.getenv("QDRANT_COLLECTION", "kitbot_docs_bge")
EMBED_MODEL_NAME = os.getenv("EMBED_MODEL_NAME", "BAAI/bge-m3")
OPENAI_MODEL = os.getenv("OPENAI_CHAT_MODEL", "gpt-4o-mini")

# --------------------
# ì‹±ê¸€í†¤
# --------------------
_qdrant_client: QdrantClient | None = None
_embed_model: SentenceTransformer | None = None
_llm_client: OpenAI | None = None
_reranker_model: CrossEncoder | None = None
_bm25_index = None
_bm25_documents = None


def get_qdrant_client() -> QdrantClient:
    global _qdrant_client
    if _qdrant_client is None:
        _qdrant_client = QdrantClient(url=QDRANT_URL)
    return _qdrant_client


def get_embed_model() -> SentenceTransformer:
    global _embed_model
    if _embed_model is None:
        print("â³ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...", EMBED_MODEL_NAME)
        _embed_model = SentenceTransformer(EMBED_MODEL_NAME)
    return _embed_model


def get_llm_client() -> OpenAI:
    global _llm_client
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise RuntimeError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
    if _llm_client is None:
        _llm_client = OpenAI(api_key=api_key)
    return _llm_client


def get_reranker_model() -> CrossEncoder:
    """BGE-reranker-v2-m3 ëª¨ë¸ ë¡œë“œ"""
    global _reranker_model
    if _reranker_model is None:
        print("â³ ë¦¬ë­ì»¤ ëª¨ë¸ ë¡œë”© ì¤‘... BAAI/bge-reranker-v2-m3")
        _reranker_model = CrossEncoder('BAAI/bge-reranker-v2-m3', max_length=512)
    return _reranker_model


# --------------------
# BM25 ì¸ë±ìŠ¤ êµ¬ì¶•
# --------------------
def tokenize_korean(text: str) -> List[str]:
    """ê°œì„ ëœ í•œêµ­ì–´ í† í¬ë‚˜ì´ì € (í˜•íƒœì†Œ ë¶„ì„ + N-gram)"""
    # 1. ê¸°ë³¸ ì •ì œ
    text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
    text = text.lower()
    
    # 2. ê³µë°± ê¸°ë°˜ í† í°í™”
    tokens = text.split()
    
    # 3. ì¶”ê°€ N-gram ìƒì„± (2-3ê¸€ì ë‹¨ìœ„)
    ngrams = []
    for token in tokens:
        if len(token) >= 2:
            # 2-gram
            for i in range(len(token) - 1):
                ngrams.append(token[i:i+2])
            # 3-gram
            if len(token) >= 3:
                for i in range(len(token) - 2):
                    ngrams.append(token[i:i+3])
    
    return tokens + ngrams


def build_bm25_index():
    """Qdrantì—ì„œ ëª¨ë“  ë¬¸ì„œë¥¼ ë¡œë“œí•˜ì—¬ BM25 ì¸ë±ìŠ¤ êµ¬ì¶•"""
    global _bm25_index, _bm25_documents
    
    if _bm25_index is not None:
        return _bm25_index, _bm25_documents
    
    print("ğŸ” BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
    client = get_qdrant_client()
    
    documents = []
    offset = None
    batch_size = 100
    
    while True:
        result = client.scroll(
            collection_name=COLLECTION_NAME,
            limit=batch_size,
            offset=offset,
            with_payload=True,
            with_vectors=False
        )
        
        points, next_offset = result
        
        if not points:
            break
        
        for point in points:
            payload = point.payload or {}
            text = (
                payload.get("chunk_text") or 
                payload.get("text") or 
                payload.get("main_text") or 
                payload.get("content") or ""
            )
            
            if text.strip():
                documents.append({
                    'id': point.id,
                    'text': text,
                    'payload': payload,
                    'score': getattr(point, 'score', 0.0)
                })
        
        if next_offset is None:
            break
        offset = next_offset
    
    print(f"   âœ… {len(documents)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
    
    tokenized_corpus = [tokenize_korean(doc['text']) for doc in documents]
    _bm25_index = BM25Okapi(tokenized_corpus)
    _bm25_documents = documents
    
    print(f"   âœ… BM25 ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
    
    return _bm25_index, _bm25_documents


# --------------------
# í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ + ë¦¬ë­ì»¤
# --------------------
def hybrid_search_with_reranker(query: str, top_k: int = 5, alpha: float = 0.85) -> List[Any]:
    """
    ê°œì„ ëœ Full íŒŒì´í”„ë¼ì¸:
    1) ê°œì„ ëœ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (BM25 + Semantic, alpha=0.85)
    2) CrossEncoder ë¦¬ë­ì»¤ë¡œ ì¬ì •ë ¬
    3) ìƒìœ„ top_kê°œ ë°˜í™˜
    """
    client = get_qdrant_client()
    model = get_embed_model()
    reranker = get_reranker_model()
    
    # 1. ì‹œë§¨í‹± ê²€ìƒ‰ (ë” ë§ì´ ê°€ì ¸ì˜¤ê¸°)
    query_vec = model.encode(query).tolist()
    semantic_limit = 50  # ë¦¬ë­ì»¤ë¥¼ ìœ„í•´ ì¶©ë¶„íˆ ë§ì´
    
    semantic_results = client.query_points(
        collection_name=COLLECTION_NAME,
        query=query_vec,
        limit=semantic_limit,
        with_payload=True,
    )
    
    semantic_scores = {}
    semantic_docs = {}
    for point in semantic_results.points:
        semantic_scores[str(point.id)] = point.score
        semantic_docs[str(point.id)] = point
    
    # 2. BM25 ê²€ìƒ‰
    bm25_index, bm25_documents = build_bm25_index()
    tokenized_query = tokenize_korean(query)
    bm25_scores = bm25_index.get_scores(tokenized_query)
    
    # ê°œì„ ëœ Min-Max ì •ê·œí™”
    if len(bm25_scores) > 0:
        min_score = min(bm25_scores)
        max_score = max(bm25_scores)
        
        if max_score > min_score:
            bm25_scores_normalized = (bm25_scores - min_score) / (max_score - min_score)
        else:
            bm25_scores_normalized = bm25_scores / max(max_score, 1.0)
    else:
        bm25_scores_normalized = bm25_scores
    
    # BM25 ìƒìœ„ ë¬¸ì„œë§Œ ì„ íƒ
    top_bm25_indices = sorted(range(len(bm25_scores_normalized)), 
                               key=lambda i: bm25_scores_normalized[i], 
                               reverse=True)[:semantic_limit]
    
    bm25_score_dict = {}
    for idx in top_bm25_indices:
        doc_id = str(bm25_documents[idx]['id'])
        bm25_score_dict[doc_id] = float(bm25_scores_normalized[idx])
    
    # 3. í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°
    all_doc_ids = set(semantic_scores.keys()) | set(bm25_score_dict.keys())
    
    hybrid_scores = {}
    for doc_id in all_doc_ids:
        sem_score = semantic_scores.get(doc_id, 0.0)
        bm25_score = bm25_score_dict.get(doc_id, 0.0)
        hybrid_scores[doc_id] = alpha * sem_score + (1 - alpha) * bm25_score
    
    # 4. ìƒìœ„ í›„ë³´ ì„ íƒ (ë¦¬ë­í‚¹ ì „)
    sorted_doc_ids = sorted(hybrid_scores.keys(), key=lambda x: hybrid_scores[x], reverse=True)
    
    candidates = []
    rerank_limit = 30  # ë¦¬ë­ì»¤ì— ë” ë§ì€ í›„ë³´ ì œê³µ
    for doc_id in sorted_doc_ids[:rerank_limit]:
        if doc_id in semantic_docs:
            point = semantic_docs[doc_id]
            candidates.append(point)
        else:
            for bm25_doc in bm25_documents:
                if str(bm25_doc['id']) == doc_id:
                    class TempPoint:
                        def __init__(self, id, payload, score):
                            self.id = id
                            self.payload = payload
                            self.score = score
                    
                    point = TempPoint(
                        id=bm25_doc['id'],
                        payload=bm25_doc['payload'],
                        score=hybrid_scores[doc_id]
                    )
                    candidates.append(point)
                    break
    
    if not candidates:
        return []
    
    # 5. ë¦¬ë­ì»¤ ì ìš© (í…ìŠ¤íŠ¸ ê¸¸ì´ ì¦ê°€)
    pairs = []
    for point in candidates:
        payload = point.payload or {}
        text = (
            payload.get("chunk_text") or 
            payload.get("text") or 
            payload.get("main_text") or 
            payload.get("content") or ""
        )
        # í…ìŠ¤íŠ¸ ê¸¸ì´ë¥¼ ëŠ˜ë ¤ì„œ ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ ì œê³µ
        pairs.append([query, text[:1024]])  # 512 â†’ 1024
    
    reranker_scores = reranker.predict(pairs)
    
    # 6. ë¦¬ë­ì»¤ ì ìˆ˜ë¡œ ìµœì¢… ì •ë ¬
    scored_points = list(zip(candidates, reranker_scores))
    scored_points.sort(key=lambda x: x[1], reverse=True)
    
    final_results = []
    for point, score in scored_points[:top_k]:
        point.score = float(score)
        final_results.append(point)
    
    print(f"   ğŸ” Full ê²€ìƒ‰ (alpha={alpha}): í•˜ì´ë¸Œë¦¬ë“œ {len(candidates)}ê°œ â†’ ë¦¬ë­í‚¹ â†’ ìµœì¢… {len(final_results)}ê°œ")
    
    return final_results


# --------------------
# ê²€ìƒ‰ ë‹¨ê³„
# --------------------
def retrieve_points(query: str, top_k: int = 5):
    """ê°œì„ ëœ í•˜ì´ë¸Œë¦¬ë“œ (alpha=0.85) + ë¦¬ë­ì»¤ ì‚¬ìš©"""
    return hybrid_search_with_reranker(query, top_k, alpha=0.85)


# --------------------
# ê²€ìƒ‰ ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ ë¸”ë¡ìœ¼ë¡œ ë³€í™˜
# --------------------
def build_context_blocks(points) -> str:
    blocks = []

    for i, p in enumerate(points):
        payload = p.payload or {}

        text = (
            payload.get("chunk_text")
            or payload.get("text")
            or payload.get("main_text")
            or payload.get("content")
            or ""
        )

        if not text.strip():
            continue

        meta = (
            f"[{i+1}] site={payload.get('site')} | "
            f"board={payload.get('board_name')} | "
            f"title={payload.get('title')} | "
            f"date={payload.get('created_at')} | "
            f"url={payload.get('url')}"
        )

        block = meta + "\n" + text
        blocks.append(block)

    return "\n\n---\n\n".join(blocks)


# --------------------
# LLM í˜¸ì¶œ
# --------------------
def call_llm(system_msg: str, user_msg: str, model: str = None) -> str:
    if model is None:
        model = OPENAI_MODEL

    client = get_llm_client()
    resp = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_msg},
        ],
        temperature=0.7,
        max_tokens=1000,
    )
    return resp.choices[0].message.content


# --------------------
# ì¼ì • ì •ë³´ ì¶”ì¶œ
# --------------------
def extract_schedule_info(answer_text: str):
    """
    LLMì´ ìƒì„±í•œ ë‹µë³€ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ì¼ì • ì œëª©, ì‹œì‘ì¼, ì¢…ë£Œì¼ì„ JSONìœ¼ë¡œ ì¶”ì¶œ
    (ë² ì´ìŠ¤ë¼ì¸ê³¼ ë™ì¼í•œ ë°©ì‹: LLM ê¸°ë°˜ ì¶”ì¶œ)
    """
    client = get_llm_client()
    now = datetime.now()
    current_year = now.year
    today_str = now.strftime("%Y-%m-%d")

    # ì¼ì • ì¶”ì¶œ ì „ìš© í”„ë¡¬í”„íŠ¸
    extraction_prompt = (
        f"í˜„ì¬ ì—°ë„ëŠ” {current_year}ë…„ì´ê³ , ì˜¤ëŠ˜ì€ {today_str}ì…ë‹ˆë‹¤.\n"
        "ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ **í•˜ë‚˜ì˜ í•µì‹¬ ì¼ì •**ì„ ì°¾ì•„ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”.\n\n"
        f"í…ìŠ¤íŠ¸: \"{answer_text}\"\n\n"
        "## ê·œì¹™\n"
        "1. **scheduleTitle**: ì¼ì •ì˜ í•µì‹¬ ì œëª© (ì˜ˆ: '2025-1í•™ê¸° ìˆ˜ê°•ì‹ ì²­', 'ì¤‘ê°„ê³ ì‚¬ ê¸°ê°„').\n"
        "2. **startDate**: ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹). ì—°ë„ê°€ ì—†ìœ¼ë©´ í˜„ì¬/ë¯¸ë˜ ê¸°ì¤€ìœ¼ë¡œ ì¶”ë¡ .\n"
        "3. **endDate**: ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹). **ì¢…ë£Œì¼ì´ ëª…ì‹œë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì‹œì‘ì¼ê³¼ ê°™ë‹¤ë©´ startDateì™€ ë™ì¼í•˜ê²Œ ì‘ì„±.**\n"
        "4. ë§Œì•½ í…ìŠ¤íŠ¸ì— ëª…í™•í•œ ë‚ ì§œ ì •ë³´ê°€ ì—†ë‹¤ë©´ ëª¨ë“  í•„ë“œë¥¼ nullë¡œ ë°˜í™˜í•˜ì„¸ìš”.\n"
        "5. ì˜¤ì§ JSON ë°ì´í„°ë§Œ ì¶œë ¥í•˜ì„¸ìš”. (Markdown backticks ì—†ì´)\n\n"
        "Example output: {\"scheduleTitle\": \"ìˆ˜ê°•ì‹ ì²­\", \"startDate\": \"2025-02-10\", \"endDate\": \"2025-02-14\"}"
    )

    try:
        resp = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": "You are a JSON extractor."},
                {"role": "user", "content": extraction_prompt}
            ],
            temperature=0,
        )
        
        content = resp.choices[0].message.content.strip()
        # í˜¹ì‹œ ëª¨ë¥¼ Markdown backtick ì œê±° (```json ... ```)
        if content.startswith("```"):
            content = content.replace("```json", "").replace("```", "")
        
        data = json.loads(content)
        return data
        
    except Exception as e:
        print(f"âš ï¸ ì¼ì • ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return {"scheduleTitle": None, "startDate": None, "endDate": None}


# --------------------
# RAG with Sources
# --------------------
def rag_with_sources(query: str, top_k: int = 5):
    from core.router import classify_query_intent
    intent = classify_query_intent(query)

    if intent == "chitchat":
        system_msg = "ë„ˆëŠ” ê¸ˆì˜¤ê³µëŒ€ í•™ìƒë“¤ì„ ë•ëŠ” ì¹œì ˆí•œ AI ì±—ë´‡ 'KIT-BOT'ì´ì•¼. í•™ìƒì—ê²Œ ë‹¤ì •í•˜ê²Œ ëŒ€ë‹µí•´ì¤˜."
        answer = call_llm(system_msg, query)
        return answer, [], {"scheduleTitle": None, "startDate": None, "endDate": None}
    
    points = retrieve_points(query, top_k)
    
    SIMILARITY_THRESHOLD = 0.4

    if not points or points[0].score < SIMILARITY_THRESHOLD:
        print(f"   ğŸ“‰ ê²€ìƒ‰ ì ìˆ˜ ë¯¸ë‹¬: {points[0].score if points else 0} < {SIMILARITY_THRESHOLD}")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. í•™êµ ì •ë³´ì™€ ê´€ë ¨ì´ ì—†ê±°ë‚˜, í•´ë‹¹ ë‚´ìš©ì„ ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", [], {"scheduleTitle": None, "startDate": None, "endDate": None}

    context_text = build_context_blocks(points)
    
    now = datetime.now(pytz.timezone('Asia/Seoul'))
    today_str = now.strftime("%Yë…„ %mì›” %dì¼")
    current_year = now.year

    system_msg = (
        f"ë‹¹ì‹ ì€ êµ­ë¦½ê¸ˆì˜¤ê³µê³¼ëŒ€í•™êµ í•™ìƒë“¤ì„ ë•ëŠ” **ë‹¤ì •í•˜ê³  ì¹œì ˆí•œ AI ë©˜í†  'KIT-BOT'**ì…ë‹ˆë‹¤.\n"
        f"í˜„ì¬ ì‹œê°ì€ **{today_str}**ì´ì—ìš”.\n\n"
        "í•™ìƒì˜ ì§ˆë¬¸ì— ëŒ€í•´ [ê²€ìƒ‰ëœ ë¬¸ì„œ]ë¥¼ ê¼¼ê¼¼íˆ í™•ì¸í•´ì„œ, **ë”°ëœ»í•˜ê³  ìƒëƒ¥í•œ ë§íˆ¬(í•´ìš”ì²´)**ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\n\n"
        
        "## 1. ë‹µë³€ ê°€ëŠ¥ ì—¬ë¶€ íŒë‹¨ (ê°€ì¥ ì¤‘ìš”!)\n"
        "   - ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ê°€ [ê²€ìƒ‰ëœ ë¬¸ì„œ]ì— **ëª…í™•í•˜ê²Œ í¬í•¨ë˜ì–´ ìˆì§€ ì•Šë‹¤ë©´**, ì–µì§€ë¡œ ì§€ì–´ë‚´ê±°ë‚˜ ë¹„ìŠ·í•œ ë‚´ìš©ì„ ë¬´ë¦¬í•˜ê²Œ ì—°ê²°í•˜ì§€ ë§ˆì„¸ìš”.\n"
        "   - ì •ë³´ê°€ ì—†ì„ ë•ŒëŠ” **'ì£„ì†¡í•˜ì§€ë§Œ, í•´ë‹¹ ë‚´ìš©ì€ í•™êµ ê³µì§€ë‚˜ ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ê°€ ì—†ë„¤ìš” ğŸ˜¥. í˜¹ì‹œ ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ë‹¤ì‹œ ì§ˆë¬¸í•´ ì£¼ì‹œê² ì–´ìš”?'**ë¼ê³  ì†”ì§í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\n"
        "   - ìœ¤ë¦¬ì ìœ¼ë¡œ ë¬¸ì œê°€ ë˜ê±°ë‚˜ í•™êµì™€ ë¬´ê´€í•œ ì§ˆë¬¸(í•µë¬´ê¸°, ì •ì¹˜ ë“±)ì—ë„ ì •ì¤‘í•˜ê²Œ ê±°ì ˆí•´ ì£¼ì„¸ìš”.\n\n"

        "## 2. ì„¼ìŠ¤ ìˆëŠ” ì‹œê°„ í™•ì¸ (Time Awareness)\n"
        f"   - ë¬¸ì„œ ë‚´ìš©ì´ **ì˜¬í•´({current_year}ë…„)** ê²ƒì¸ì§€ ê¼­ í™•ì¸í•´ ì£¼ì„¸ìš”.\n"
        f"   - ë§Œì•½ ì˜¬í•´ ìµœì‹  ê³µì§€ê°€ ì—†ê³  ì‘ë…„ ìë£Œë§Œ ìˆë‹¤ë©´, **'ì•„ì‰½ê²Œë„ ì•„ì§ {current_year}ë…„ë„ ê³µì§€ëŠ” ì˜¬ë¼ì˜¤ì§€ ì•Šì•˜ì–´ìš”. ëŒ€ì‹  ì‘ë…„({current_year-1}ë…„) ì¼ì •ì„ ì°¸ê³ ìš©ìœ¼ë¡œ ì•Œë ¤ë“œë¦´ê²Œìš”!'**ë¼ê³  ì•ˆë‚´í•´ ì£¼ì„¸ìš”.\n"
        "   - ì´ë¯¸ ì§€ë‚œ ì¼ì •ì´ë¼ë©´ **'í•´ë‹¹ ì¼ì •ì€ ì•„ì‰½ê²Œë„ ë§ˆê°ë˜ì—ˆì–´ìš”.'**ë¼ê³  ì•Œë ¤ì£¼ì„¸ìš”.\n\n"
        
        "## 3. ë³´ê¸° í¸í•˜ê³  ì¹œì ˆí•œ ì„¤ëª…\n"
        "   - ë‚ ì§œ, ì¥ì†Œ, ì „í™”ë²ˆí˜¸ ê°™ì€ í•µì‹¬ ì •ë³´ëŠ” **êµµê²Œ(**)** í‘œì‹œí•´ì„œ ëˆˆì— ì˜ ë„ê²Œ í•´ì£¼ì„¸ìš”.\n"
        "   - ë³µì¡í•œ ë‚´ìš©ì€ **ë¦¬ìŠ¤íŠ¸**ë¡œ ê¹”ë”í•˜ê²Œ ì •ë¦¬í•´ ì£¼ëŠ” ì„¼ìŠ¤ë¥¼ ë°œíœ˜í•´ ì£¼ì„¸ìš”.\n"
        "   - ì ì ˆí•œ **ì´ëª¨ì§€(ğŸ“…, ğŸšŒ, ğŸ˜Š ë“±)**ë¥¼ ì„ì–´ì„œ ë‹µë³€ì´ ë”±ë”±í•´ì§€ì§€ ì•Šë„ë¡ í•´ì£¼ì„¸ìš”.\n\n"
        
        "## 4. ë§ˆë¬´ë¦¬\n"
        "   - ë‹µë³€ ëì—ëŠ” **'ë” ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ ì–¸ì œë“  ë¬¼ì–´ë´ ì£¼ì„¸ìš”!'** ë©˜íŠ¸ë¥¼ ë§ë¶™ì—¬ ì£¼ì„¸ìš”.\n"
        "   - (ë‹¨, ë‹µë³€ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°ì—ëŠ” ì¶œì²˜ë‚˜ ì‘ì› ë¬¸êµ¬ë¥¼ ìƒëµí•˜ê³  ê°„ê²°í•˜ê²Œ ëë‚´ì„¸ìš”.)"
    )

    user_msg = (
        f"ì§ˆë¬¸: {query}\n\n"
        f"--- ê²€ìƒ‰ëœ ë¬¸ì„œ ì‹œì‘ ---\n"
        f"{context_text}\n"
        f"--- ê²€ìƒ‰ëœ ë¬¸ì„œ ë ---\n\n"
        f"ìœ„ ë¬¸ì„œë¥¼ ì •ë°€í•˜ê²Œ ë¶„ì„í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì¤˜."
    )
    
    answer = call_llm(system_msg, user_msg)

    schedule_data = {"scheduleTitle": None, "startDate": None, "endDate": None}

    negative_keywords = [
        "ì£„ì†¡í•˜ì§€ë§Œ", "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤", "ë¬¸ì„œì— ì—†ìŠµë‹ˆë‹¤"
    ]
    
    if any(neg in answer for neg in negative_keywords):
        final_sources = []
    else:
        final_sources = []
        for p in points:
            payload = p.payload or {}
            final_sources.append({
                "title": payload.get("title"),
                "url": payload.get("url"),
                "text": payload.get("text")
            })

    if final_sources and (intent == "schedule" or "202" in answer):
        extracted = extract_schedule_info(answer)
        if extracted.get("startDate"):
            schedule_data = extracted

    return answer, final_sources, schedule_data


def generate_answer(query: str, top_k: int = 5) -> str:
    answer, _, _ = rag_with_sources(query, top_k)
    return answer
