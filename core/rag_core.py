# rag_core.py
import os
import json
from typing import List, Dict, Any, Tuple, Optional
from dotenv import load_dotenv
from datetime import datetime
import pytz
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.http import models as qm
from openai import OpenAI

from core.router import classify_query_intent, rerank_with_boost

load_dotenv()

# --------------------
# í™˜ê²½ ì„¤ì •
# --------------------
QDRANT_URL = os.getenv("QDRANT_URL", "http://localhost:6333")
COLLECTION_NAME = os.getenv("QDRANT_COLLECTION", "kitbot_docs_bge")
EMBED_MODEL_NAME = os.getenv("EMBED_MODEL_NAME", "BAAI/bge-m3")
OPENAI_MODEL = os.getenv("OPENAI_CHAT_MODEL", "gpt-4o-mini")

# --------------------
# ì‹±ê¸€í†¤
# --------------------
_qdrant_client: QdrantClient | None = None
_embed_model: SentenceTransformer | None = None
_llm_client: OpenAI | None = None


def get_qdrant_client() -> QdrantClient:
    global _qdrant_client
    if _qdrant_client is None:
        _qdrant_client = QdrantClient(url=QDRANT_URL)
    return _qdrant_client


def get_embed_model() -> SentenceTransformer:
    global _embed_model
    if _embed_model is None:
        print("â³ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...", EMBED_MODEL_NAME)
        _embed_model = SentenceTransformer(EMBED_MODEL_NAME)
    return _embed_model


def get_llm_client() -> OpenAI:
    global _llm_client
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise RuntimeError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
    if _llm_client is None:
        _llm_client = OpenAI(api_key=api_key)
    return _llm_client


# --------------------
# Boost ê¸°ë°˜ ê²€ìƒ‰
# --------------------
def search_with_boost(query: str, top_k: int = 5) -> List[Any]:
    """
    1) ì¿¼ë¦¬ ì„ë² ë”©
    2) Qdrantì—ì„œ top_k*3ê°œ query_pointsë¡œ ê²€ìƒ‰
    3) router.rerank_with_boostë¡œ ì¬ì •ë ¬
    """
    intent = classify_query_intent(query)
    client = get_qdrant_client()
    model = get_embed_model()

    query_vec = model.encode(query).tolist()
    limit = max(top_k * 3, top_k)

    # qdrant-client 1.16.0 ì—ì„œëŠ” searchê°€ ì•„ë‹ˆë¼ query_points ì‚¬ìš©
    res = client.query_points(
        collection_name=COLLECTION_NAME,
        query=query_vec,          # ì˜ˆì „ ë²„ì „ ì‹œê·¸ë‹ˆì²˜: query=
        limit=limit,
        with_payload=True,
    )

    raw_hits = res.points  # ScoredPoint ë¦¬ìŠ¤íŠ¸

    # boost í›„ ì¬ì •ë ¬
    boosted_hits = rerank_with_boost(raw_hits, intent=intent, top_k=top_k)
    return boosted_hits


# --------------------
# ê²€ìƒ‰ ë‹¨ê³„ (ê¸°ì¡´ APIìš©)
# --------------------
def retrieve_points(query: str, top_k: int = 5):
    """
    API / CLI ëª¨ë‘ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì‹¤ì œ ê²€ìƒ‰ í•¨ìˆ˜
    â†’ ì´ì œ boost ê²€ìƒ‰ì„ í•­ìƒ ì‚¬ìš©í•˜ë„ë¡ í†µì¼
    """
    return search_with_boost(query, top_k)


# --------------------
# ê²€ìƒ‰ ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ ë¸”ë¡ìœ¼ë¡œ ë³€í™˜
# --------------------
def build_context_blocks(points) -> str:
    blocks = []

    for i, p in enumerate(points):
        payload = p.payload or {}

        text = (
            payload.get("chunk_text")
            or payload.get("text")
            or payload.get("main_text")
            or payload.get("content")
            or ""
        )

        if not text.strip():
            continue

        meta = (
            f"[{i+1}] site={payload.get('site')} | "
            f"board={payload.get('board_name')} | "
            f"title={payload.get('title')} | "
            f"date={payload.get('created_at')} | "
            f"url={payload.get('url')}"
        )

        block = meta + "\n" + text
        blocks.append(block)

    return "\n\n---\n\n".join(blocks)

# ğŸ”´ [New] LLMì„ ì´ìš©í•œ ìŠ¤ë§ˆíŠ¸ í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜
def extract_search_keyword_llm(query: str) -> str:
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ê²€ìƒ‰ìš© 'í•µì‹¬ ëª…ì‚¬ í‚¤ì›Œë“œ' 1ê°œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    (ë¹„ìš© ì ˆì•½ì„ ìœ„í•´ ì§§ì€ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)
    """
    client = get_llm_client()
    prompt = (
        f"ì§ˆë¬¸: \"{query}\"\n"
        "ìœ„ ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” **ê°€ì¥ ì¤‘ìš”í•œ ëª…ì‚¬ ë‹¨ì–´ 1ê°œ**ë§Œ ì¶”ì¶œí•´.\n"
        "ì¡°ì‚¬ë‚˜ ì„œìˆ ì–´ëŠ” ë¹¼ê³  ë‹¨ì–´ë§Œ ì¶œë ¥í•´.\n"
        "ì˜ˆì‹œ:\n"
        "- ì…”í‹€ë²„ìŠ¤ ì‹œê°„í‘œ -> ì…”í‹€ë²„ìŠ¤\n"
        "- êµ­ê°€ì¥í•™ê¸ˆ ì–¸ì œ ë“¤ì–´ì™€? -> êµ­ê°€ì¥í•™ê¸ˆ\n"
        "- ì•ˆë…• ë°˜ê°€ì›Œ -> ì¸ì‚¬\n"
        "í‚¤ì›Œë“œ:"
    )
    
    try:
        resp = client.chat.completions.create(
            model=OPENAI_MODEL, # gpt-4o ë˜ëŠ” gpt-3.5-turbo
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=20, 
        )
        return resp.choices[0].message.content.strip()
    except:
        return "ê²€ìƒ‰" # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
    
# --------------------
# LLM í˜¸ì¶œ
# --------------------
def call_llm(system_msg: str, user_msg: str) -> str:
    client = get_llm_client()

    resp = client.chat.completions.create(
        model=OPENAI_MODEL,
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_msg},
        ],
        temperature=0.2,
    )
    
    answer = resp.choices[0].message.content.strip()
    
    # ğŸ”´ [Fix] ì¤„ë°”ê¿ˆ ë¬¸ì(\n)ê°€ í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ì¶œë ¥ë˜ëŠ” í˜„ìƒ ë°©ì§€
    # (LLMì´ ê°€ë” "\\n"ìœ¼ë¡œ ì´ìŠ¤ì¼€ì´í”„í•´ì„œ ì¤„ ë•Œê°€ ìˆìŒ)
    answer = answer.replace("\\n", "\n")

    return answer

# ---------------------------------------------------------
# [New] ë‹µë³€ì—ì„œ ì¼ì • ì •ë³´(JSON) ì¶”ì¶œ í•¨ìˆ˜
# ---------------------------------------------------------
def extract_schedule_info(answer_text: str):
    """
    LLMì´ ìƒì„±í•œ ë‹µë³€ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ì¼ì • ì œëª©, ì‹œì‘ì¼, ì¢…ë£Œì¼ì„ JSONìœ¼ë¡œ ì¶”ì¶œ
    """
    client = get_llm_client()
    now = datetime.now()
    current_year = now.year
    today_str = now.strftime("%Y-%m-%d")

    # ì¼ì • ì¶”ì¶œ ì „ìš© í”„ë¡¬í”„íŠ¸
    extraction_prompt = (
        f"í˜„ì¬ ì—°ë„ëŠ” {current_year}ë…„ì´ê³ , ì˜¤ëŠ˜ì€ {today_str}ì…ë‹ˆë‹¤.\n"
        "ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ **í•˜ë‚˜ì˜ í•µì‹¬ ì¼ì •**ì„ ì°¾ì•„ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”.\n\n"
        f"í…ìŠ¤íŠ¸: \"{answer_text}\"\n\n"
        "## ê·œì¹™\n"
        "1. **scheduleTitle**: ì¼ì •ì˜ í•µì‹¬ ì œëª© (ì˜ˆ: '2025-1í•™ê¸° ìˆ˜ê°•ì‹ ì²­', 'ì¤‘ê°„ê³ ì‚¬ ê¸°ê°„').\n"
        "2. **startDate**: ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹). ì—°ë„ê°€ ì—†ìœ¼ë©´ í˜„ì¬/ë¯¸ë˜ ê¸°ì¤€ìœ¼ë¡œ ì¶”ë¡ .\n"
        "3. **endDate**: ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹). **ì¢…ë£Œì¼ì´ ëª…ì‹œë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì‹œì‘ì¼ê³¼ ê°™ë‹¤ë©´ startDateì™€ ë™ì¼í•˜ê²Œ ì‘ì„±.**\n"
        "4. ë§Œì•½ í…ìŠ¤íŠ¸ì— ëª…í™•í•œ ë‚ ì§œ ì •ë³´ê°€ ì—†ë‹¤ë©´ ëª¨ë“  í•„ë“œë¥¼ nullë¡œ ë°˜í™˜í•˜ì„¸ìš”.\n"
        "5. ì˜¤ì§ JSON ë°ì´í„°ë§Œ ì¶œë ¥í•˜ì„¸ìš”. (Markdown backticks ì—†ì´)\n\n"
        "Example output: {\"scheduleTitle\": \"ìˆ˜ê°•ì‹ ì²­\", \"startDate\": \"2025-02-10\", \"endDate\": \"2025-02-14\"}"
    )

    try:
        resp = client.chat.completions.create(
            model=OPENAI_MODEL, # gpt-4o ë˜ëŠ” gpt-3.5-turbo
            messages=[
                {"role": "system", "content": "You are a JSON extractor."},
                {"role": "user", "content": extraction_prompt}
            ],
            temperature=0, # ì •í™•ì„±ì„ ìœ„í•´ 0
        )
        
        content = resp.choices[0].message.content.strip()
        # í˜¹ì‹œ ëª¨ë¥¼ Markdown backtick ì œê±° (```json ... ```)
        if content.startswith("```"):
            content = content.replace("```json", "").replace("```", "")
        
        data = json.loads(content)
        return data
        
    except Exception as e:
        print(f"âš ï¸ ì¼ì • ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return {"scheduleTitle": None, "startDate": None, "endDate": None}
    
# --------------------
# ì¶œì²˜ + ë‹µë³€ ìƒì„±
# --------------------
def rag_with_sources(query: str, top_k: int = 5):
    # 0. ì˜ë„ íŒŒì•…
    from core.router import classify_query_intent
    intent = classify_query_intent(query)

    # 1. ì¼ìƒ ëŒ€í™”(Chit-chat) ì²˜ë¦¬ (ê²€ìƒ‰ ìƒëµ)
    if intent == "chitchat":
        system_msg = "ë„ˆëŠ” ê¸ˆì˜¤ê³µëŒ€ í•™ìƒë“¤ì„ ë•ëŠ” ì¹œì ˆí•œ AI ì±—ë´‡ 'KIT-Bot'ì´ì•¼. í•™ìƒì—ê²Œ ë‹¤ì •í•˜ê²Œ ëŒ€ë‹µí•´ì¤˜."
        answer = call_llm(system_msg, query)
        # ì¼ìƒ ëŒ€í™”ëŠ” ì¶œì²˜ ì—†ìŒ
        return answer, [], {"scheduleTitle": None, "startDate": None, "endDate": None}
    
    # 2. ê²€ìƒ‰ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    points = retrieve_points(query, top_k)
    
    SIMILARITY_THRESHOLD = 0.4

    if not points or points[0].score < SIMILARITY_THRESHOLD:
        # ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ë°”ë¡œ ì¢…ë£Œ
        print(f"   ğŸ“‰ ê²€ìƒ‰ ì ìˆ˜ ë¯¸ë‹¬: {points[0].score if points else 0} < {SIMILARITY_THRESHOLD}")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. í•™êµ ì •ë³´ì™€ ê´€ë ¨ì´ ì—†ê±°ë‚˜, í•´ë‹¹ ë‚´ìš©ì„ ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", [], {"scheduleTitle": None, "startDate": None, "endDate": None}

    context_text = build_context_blocks(points)
    
    # 1. ì˜¤ëŠ˜ ë‚ ì§œ ë° í˜„ì¬ ì—°ë„ êµ¬í•˜ê¸° (í•œêµ­ ì‹œê°„ ê¸°ì¤€)
    kst = pytz.timezone('Asia/Seoul')
    now = datetime.now(kst)
    today_str = now.strftime("%Yë…„ %mì›” %dì¼")
    current_year = now.year

    # 2. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— 'ê¸°ì¤€ ì‹œê°„'ê³¼ 'ì—„ê²©í•œ ì—°ë„ ë¹„êµ ì§€ì¹¨' ì£¼ì…
    # ---------------------------------------------------------
    # [Prompt Engineering] í”„ë¡¬í”„íŠ¸ ê³ ë„í™” (Time Awareness ê°•í™”)
    # ---------------------------------------------------------
    system_msg = (
        f"ë‹¹ì‹ ì€ êµ­ë¦½ê¸ˆì˜¤ê³µê³¼ëŒ€í•™êµ í•™ìƒë“¤ì„ ë•ëŠ” **ë‹¤ì •í•˜ê³  ì¹œì ˆí•œ AI ë©˜í†  'KIT-BOT'**ì…ë‹ˆë‹¤.\n"
        f"í˜„ì¬ ì‹œê°ì€ **{today_str}**ì´ì—ìš”.\n\n"
        "í•™ìƒì˜ ì§ˆë¬¸ì— ëŒ€í•´ [ê²€ìƒ‰ëœ ë¬¸ì„œ]ë¥¼ ê¼¼ê¼¼íˆ í™•ì¸í•´ì„œ, **ë”°ëœ»í•˜ê³  ìƒëƒ¥í•œ ë§íˆ¬(í•´ìš”ì²´)**ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\n\n"
        
        "## 1. ë‹µë³€ ê°€ëŠ¥ ì—¬ë¶€ íŒë‹¨ (ê°€ì¥ ì¤‘ìš”!)\n"
        "   - ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ê°€ [ê²€ìƒ‰ëœ ë¬¸ì„œ]ì— **ëª…í™•í•˜ê²Œ í¬í•¨ë˜ì–´ ìˆì§€ ì•Šë‹¤ë©´**, ì–µì§€ë¡œ ì§€ì–´ë‚´ê±°ë‚˜ ë¹„ìŠ·í•œ ë‚´ìš©ì„ ë¬´ë¦¬í•˜ê²Œ ì—°ê²°í•˜ì§€ ë§ˆì„¸ìš”.\n"
        "   - ì •ë³´ê°€ ì—†ì„ ë•ŒëŠ” **'ì£„ì†¡í•˜ì§€ë§Œ, í•´ë‹¹ ë‚´ìš©ì€ í•™êµ ê³µì§€ë‚˜ ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ê°€ ì—†ë„¤ìš” ğŸ˜¥. í˜¹ì‹œ ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ë‹¤ì‹œ ì§ˆë¬¸í•´ ì£¼ì‹œê² ì–´ìš”?'**ë¼ê³  ì†”ì§í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\n"
        "   - ìœ¤ë¦¬ì ìœ¼ë¡œ ë¬¸ì œê°€ ë˜ê±°ë‚˜ í•™êµì™€ ë¬´ê´€í•œ ì§ˆë¬¸(í•µë¬´ê¸°, ì •ì¹˜ ë“±)ì—ë„ ì •ì¤‘í•˜ê²Œ ê±°ì ˆí•´ ì£¼ì„¸ìš”.\n\n"

        "## 2. ì„¼ìŠ¤ ìˆëŠ” ì‹œê°„ í™•ì¸ (Time Awareness)\n"
        f"   - ë¬¸ì„œ ë‚´ìš©ì´ **ì˜¬í•´({current_year}ë…„)** ê²ƒì¸ì§€ ê¼­ í™•ì¸í•´ ì£¼ì„¸ìš”.\n"
        f"   - ë§Œì•½ ì˜¬í•´ ìµœì‹  ê³µì§€ê°€ ì—†ê³  ì‘ë…„ ìë£Œë§Œ ìˆë‹¤ë©´, **'ì•„ì‰½ê²Œë„ ì•„ì§ {current_year}ë…„ë„ ê³µì§€ëŠ” ì˜¬ë¼ì˜¤ì§€ ì•Šì•˜ì–´ìš”. ëŒ€ì‹  ì‘ë…„({current_year-1}ë…„) ì¼ì •ì„ ì°¸ê³ ìš©ìœ¼ë¡œ ì•Œë ¤ë“œë¦´ê²Œìš”!'**ë¼ê³  ì•ˆë‚´í•´ ì£¼ì„¸ìš”.\n"
        "   - ì´ë¯¸ ì§€ë‚œ ì¼ì •ì´ë¼ë©´ **'í•´ë‹¹ ì¼ì •ì€ ì•„ì‰½ê²Œë„ ë§ˆê°ë˜ì—ˆì–´ìš”.'**ë¼ê³  ì•Œë ¤ì£¼ì„¸ìš”.\n\n"
        
        "## 3. ë³´ê¸° í¸í•˜ê³  ì¹œì ˆí•œ ì„¤ëª…\n"
        "   - ë‚ ì§œ, ì¥ì†Œ, ì „í™”ë²ˆí˜¸ ê°™ì€ í•µì‹¬ ì •ë³´ëŠ” **êµµê²Œ(**)** í‘œì‹œí•´ì„œ ëˆˆì— ì˜ ë„ê²Œ í•´ì£¼ì„¸ìš”.\n"
        "   - ë³µì¡í•œ ë‚´ìš©ì€ **ë¦¬ìŠ¤íŠ¸**ë¡œ ê¹”ë”í•˜ê²Œ ì •ë¦¬í•´ ì£¼ëŠ” ì„¼ìŠ¤ë¥¼ ë°œíœ˜í•´ ì£¼ì„¸ìš”.\n"
        "   - ì ì ˆí•œ **ì´ëª¨ì§€(ğŸ“…, ğŸšŒ, ğŸ˜Š ë“±)**ë¥¼ ì„ì–´ì„œ ë‹µë³€ì´ ë”±ë”±í•´ì§€ì§€ ì•Šë„ë¡ í•´ì£¼ì„¸ìš”.\n\n"
        
        "## 4. ë§ˆë¬´ë¦¬\n"
        "   - ë‹µë³€ ëì—ëŠ” **'ë” ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ ì–¸ì œë“  ë¬¼ì–´ë´ ì£¼ì„¸ìš”!'** ë©˜íŠ¸ë¥¼ ë§ë¶™ì—¬ ì£¼ì„¸ìš”.\n"
        "   - (ë‹¨, ë‹µë³€ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°ì—ëŠ” ì¶œì²˜ë‚˜ ì‘ì› ë¬¸êµ¬ë¥¼ ìƒëµí•˜ê³  ê°„ê²°í•˜ê²Œ ëë‚´ì„¸ìš”.)"
    )

    user_msg = (
        f"ì§ˆë¬¸: {query}\n\n"
        f"--- ê²€ìƒ‰ëœ ë¬¸ì„œ ì‹œì‘ ---\n"
        f"{context_text}\n"
        f"--- ê²€ìƒ‰ëœ ë¬¸ì„œ ë ---\n\n"
        f"ìœ„ ë¬¸ì„œë¥¼ ì •ë°€í•˜ê²Œ ë¶„ì„í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì¤˜."
    )
    
    # 3. LLM í˜¸ì¶œ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    answer = call_llm(system_msg, user_msg)

    # ---------------------------------------------------------
    # [New] ì˜ë„ê°€ 'schedule'ì´ê±°ë‚˜ ë‹µë³€ì— ë‚ ì§œê°€ í¬í•¨ëœ ê²½ìš° -> ì¼ì • ì¶”ì¶œ ì‹œë„
    # ---------------------------------------------------------
    schedule_data = {"scheduleTitle": None, "startDate": None, "endDate": None}

    # í•™ì‚¬ì¼ì • ì˜ë„ì´ê±°ë‚˜, ë‹µë³€ì— "202X-" ê°™ì€ ë‚ ì§œ íŒ¨í„´ì´ ë³´ì´ë©´ ì¶”ì¶œ ì‹œë„
    negative_keywords = [
        "ì£„ì†¡í•˜ì§€ë§Œ", "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤", "ë¬¸ì„œì— ì—†ìŠµë‹ˆë‹¤", "ë„ì™€ë“œë¦´ ìˆ˜ ì—†ì–´ìš”", "ì œê³µí•  ìˆ˜ ì—†ì–´ìš”", "ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ìš”"
    ]
    
    if any(neg in answer for neg in negative_keywords):
        final_sources = [] # ì¶œì²˜ ìˆ¨ê¹€
    else:
        # ì •ìƒ ë‹µë³€ì´ë©´ ì¶œì²˜ ì •ë¦¬
        final_sources = []
        for p in points:
            payload = p.payload or {}
            final_sources.append({
                "title": payload.get("title"),
                "url": payload.get("url"),
                "text": payload.get("text")
            })

    # 6. ì¼ì • ì •ë³´ ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§)
    schedule_data = {"scheduleTitle": None, "startDate": None, "endDate": None}
    # ë‹µë³€ì´ ì„±ê³µí–ˆì„ ë•Œë§Œ ì¼ì • ì¶”ì¶œ ì‹œë„
    if final_sources and (intent == "schedule" or "202" in answer):
        extracted = extract_schedule_info(answer)
        if extracted.get("startDate"):
            schedule_data = extracted

    return answer, final_sources, schedule_data


# --------------------
# CLIìš© ê°„ë‹¨ ë˜í¼
# --------------------
def generate_answer(query: str, top_k: int = 5) -> str:
    answer, _ = rag_with_sources(query, top_k)
    return answer