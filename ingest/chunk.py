import json
from pathlib import Path
from langchain_text_splitters import RecursiveCharacterTextSplitter

# --------------------------------------------------------------------------
# 1. ì˜ë¯¸ ë‹¨ìœ„ ì²­í‚¹ ì„¤ì •
# --------------------------------------------------------------------------
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", " ", ""]
)

def build_header(doc):
    lines = [
        f"ë¬¸ì„œ ì œëª©: {doc.get('display_title') or doc.get('title')}",
        f"ê²Œì‹œíŒ: {doc.get('board_name')}",
        f"ì‘ì„±ì¼: {doc.get('created_at') or 'ì•Œ ìˆ˜ ì—†ìŒ'}",
        f"ì¶œì²˜ URL: {doc.get('url')}",
    ]
    return "\n".join(lines) + "\n\n"

def chunk_document(doc):
    header = build_header(doc)
    main_text = doc.get("main_text", "")
    
    if len(main_text) < 10:
        return []

    raw_chunks = text_splitter.split_text(main_text)

    chunk_docs = []
    for idx, chunk_text in enumerate(raw_chunks):
        final_text = header + chunk_text
        
        chunk_docs.append({
            "chunk_id": f"{doc['doc_id']}__{idx}",
            "doc_id": doc["doc_id"],
            "chunk_index": idx,
            "text": final_text,
            "metadata": {
                "site": doc.get("site"),
                "board_name": doc.get("board_name"),
                "title": doc.get("title"),
                "url": doc.get("url"),
                "created_at": doc.get("created_at"),
                "tags": doc.get("tags", []), 
                "source_type": doc.get("source_type", "page")
            }
        })

    return chunk_docs

def chunk_directory(unified_dir: str, chunk_output: str):
    input_path = Path(unified_dir)
    output_path = Path(chunk_output)
    output_path.mkdir(parents=True, exist_ok=True)

    count_docs = 0
    total_chunks = 0
    skipped_docs = 0  # ë³€ìˆ˜ ì´ˆê¸°í™”

    print(f"ğŸ“‚ ì²­í‚¹ ì‹œì‘: {input_path} -> {output_path}")

    files = list(input_path.glob("*.unified.json"))
    print(f"â„¹ï¸  ì²˜ë¦¬í•  íŒŒì¼ ìˆ˜: {len(files)}ê°œ")
    
    for path in files:
        try:
            with path.open(encoding="utf-8") as f:
                doc = json.load(f)

            # [ì¦ë¶„ ì²˜ë¦¬] ì²« ë²ˆì§¸ ì²­í¬ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            first_chunk_name = f"{doc['doc_id']}__0.json"
            first_chunk_path = output_path / first_chunk_name

            # íŒŒì¼ì´ ì´ë¯¸ ìˆê³ , ì›ë³¸(path)ì´ ì²­í¬(first_chunk_path)ë³´ë‹¤ ì˜¤ë˜ëœ ê²½ìš°(ë³€ê²½ ì—†ìŒ) -> ìŠ¤í‚µ
            if first_chunk_path.exists():
                if path.stat().st_mtime <= first_chunk_path.stat().st_mtime:
                    skipped_docs += 1
                    continue
                # else: ì›ë³¸ì´ ë” ìµœì‹ ì´ë©´(ìƒˆë¡œ ê°±ì‹ ë¨) -> ì§„í–‰ (ë®ì–´ì“°ê¸°)

            chunks = chunk_document(doc)

            for c in chunks:
                out_name = f"{c['chunk_id']}.json"
                if len(out_name) > 250:
                    out_name = c['chunk_id'][:240] + ".json"
                
                out_file = output_path / out_name
                with out_file.open("w", encoding="utf-8") as f:
                    json.dump(c, f, ensure_ascii=False, indent=2)

            count_docs += 1
            total_chunks += len(chunks)
            
            if count_docs % 1000 == 0:
                print(f"   ... {count_docs}ê°œ ë¬¸ì„œ ì‹ ê·œ ì²˜ë¦¬ ì™„ë£Œ")

        except Exception as e:
            print(f"âŒ Error processing {path.name}: {e}")

    print("=" * 60)
    print(f"âœ… ì²­í‚¹ ì™„ë£Œ!")
    print(f"ğŸ“„ ì‹ ê·œ ì²˜ë¦¬ ë¬¸ì„œ: {count_docs}ê°œ")
    print(f"â­ï¸ ê±´ë„ˆë›´ ë¬¸ì„œ: {skipped_docs}ê°œ")
    print(f"ğŸ§© ìƒì„±ëœ ì²­í¬: {total_chunks}ê°œ")
    print("=" * 60)

if __name__ == "__main__":
    chunk_directory("data/unified", "data/chunks")