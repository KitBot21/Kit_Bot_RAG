import uuid
import hashlib
from pathlib import Path
import json
from typing import List, Dict

from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.http import models as qm

# ===== ì„¤ì • =====
QDRANT_URL = "http://localhost:6333"
COLLECTION_NAME = "kitbot_docs_bge"
EMBED_MODEL_NAME = "BAAI/bge-m3"
VECTOR_DIM = 1024
BATCH_SIZE = 64

def get_project_paths():
    project_root = Path(__file__).resolve().parents[1]
    data_dir = project_root / "data"
    chunks_dir = data_dir / "chunks"
    log_path = data_dir / "embedded_log.txt"
    return project_root, data_dir, chunks_dir, log_path

def ensure_collection(client: QdrantClient, collection_name: str):
    if client.collection_exists(collection_name):
        print(f"â„¹ï¸  ì»¬ë ‰ì…˜ '{collection_name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. (ë°ì´í„° ì¶”ê°€/ê°±ì‹  ëª¨ë“œ)")
        return

    print(f"âš ï¸ ì»¬ë ‰ì…˜ '{collection_name}' ì—†ìŒ â†’ ìƒˆë¡œ ìƒì„±")
    client.create_collection(
        collection_name=collection_name,
        vectors_config=qm.VectorParams(
            size=VECTOR_DIM,
            distance=qm.Distance.COSINE,
        ),
    )
    print(f"âœ… ì»¬ë ‰ì…˜ '{collection_name}' ìƒì„± ì™„ë£Œ")

# [Update] IDì™€ Content Hashë¥¼ ê°™ì´ ë¡œë“œ
def load_processed_log(log_path: Path) -> Dict[str, str]:
    if not log_path.exists():
        return {}
    
    processed = {}
    try:
        with open(log_path, "r", encoding="utf-8") as f:
            for line in f:
                parts = line.strip().split("\t")
                if len(parts) >= 2:
                    # key: chunk_id, value: content_hash
                    processed[parts[0]] = parts[1]
    except Exception:
        pass # íŒŒì¼ì´ ê¹¨ì¡Œê±°ë‚˜ í¬ë§·ì´ ë‹¤ë¥´ë©´ ë¬´ì‹œ
    return processed

# [Update] IDì™€ Content Hashë¥¼ ê°™ì´ ì €ì¥
def save_processed_log(log_path: Path, items: List[tuple]):
    with open(log_path, "a", encoding="utf-8") as f:
        for cid, chash in items:
            f.write(f"{cid}\t{chash}\n")

def calculate_content_hash(text: str) -> str:
    return hashlib.md5(text.encode("utf-8")).hexdigest()

def load_chunks(chunks_dir: Path, processed_log: Dict[str, str]):
    files = sorted(chunks_dir.glob("*.json"))
    print(f"â„¹ï¸  ì²­í¬ íŒŒì¼ {len(files)}ê°œ ë°œê²¬")
    
    skipped = 0
    for path in files:
        try:
            with path.open(encoding="utf-8") as f:
                chunk = json.load(f)
            
            chunk_id = chunk["chunk_id"]
            text = chunk["text"]
            current_hash = calculate_content_hash(text)
            
            # [í•µì‹¬] IDê°€ ìˆê³ , ë‚´ìš© í•´ì‹œê°’ê¹Œì§€ ë˜‘ê°™ì•„ì•¼ ìŠ¤í‚µ!
            if chunk_id in processed_log:
                if processed_log[chunk_id] == current_hash:
                    skipped += 1
                    continue
                # IDëŠ” ìˆëŠ”ë° í•´ì‹œê°€ ë‹¤ë¥´ë©´? -> ë‚´ìš©ì´ ë°”ë€ ê²ƒ! (í†µê³¼ -> ì—…ë°ì´íŠ¸ ëŒ€ìƒ)

            # í•´ì‹œê°’ì„ ì²­í¬ ê°ì²´ì— ì„ì‹œ ì €ì¥ (ë‚˜ì¤‘ì— ë¡œê·¸ ì €ì¥ìš©)
            chunk["_content_hash"] = current_hash
            yield chunk
            
        except Exception:
            continue
    
    if skipped > 0:
        print(f"â­ï¸  ë³€ê²½ ì—†ëŠ” {skipped}ê°œ ì²­í¬ ìŠ¤í‚µí•¨")

def chunks_to_batches(iterable, batch_size: int):
    batch = []
    for item in iterable:
        batch.append(item)
        if len(batch) >= batch_size:
            yield batch
            batch = []
    if batch:
        yield batch

def generate_uuid_from_string(string: str) -> str:
    hash_value = hashlib.md5(string.encode("utf-8")).hexdigest()
    return str(uuid.UUID(hash_value))

def embed_and_upload(chunks_dir: Path = None):
    project_root, data_dir, default_chunks_dir, log_path = get_project_paths()
    if chunks_dir is None:
        chunks_dir = default_chunks_dir

    if not chunks_dir.exists():
        raise FileNotFoundError(f"ì²­í¬ ë””ë ‰í„°ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {chunks_dir}")

    # 1) ê¸°ì¡´ ë¡œê·¸ ë¡œë“œ (ID + Hash)
    processed_log = load_processed_log(log_path)
    print(f"ğŸ“‹ ê¸°ì¡´ ì™„ë£Œ ê¸°ë¡: {len(processed_log)}ê°œ ë¡œë“œë¨")

    print("â³ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...", EMBED_MODEL_NAME)
    model = SentenceTransformer(EMBED_MODEL_NAME)

    client = QdrantClient(url=QDRANT_URL)
    ensure_collection(client, COLLECTION_NAME)

    total_new_chunks = 0
    
    # 2) ë³€ê²½ëœ ê²ƒë§Œ ê³¨ë¼ë‚´ê¸°
    chunk_generator = load_chunks(chunks_dir, processed_log)

    for batch in chunks_to_batches(chunk_generator, BATCH_SIZE):
        texts: List[str] = [c["text"] for c in batch]
        vectors = model.encode(texts, batch_size=BATCH_SIZE, convert_to_numpy=True)

        points = []
        log_items = [] # (id, hash) íŠœí”Œ ì €ì¥

        for vec, chunk in zip(vectors, batch):
            meta = chunk.get("metadata", {})
            fixed_id = generate_uuid_from_string(chunk["chunk_id"])
            
            # ë¡œê·¸ì— ì €ì¥í•  ì •ë³´ ì¤€ë¹„
            log_items.append((chunk["chunk_id"], chunk["_content_hash"]))

            payload = {
                "chunk_id": chunk["chunk_id"],
                "doc_id": chunk["doc_id"],
                "chunk_index": chunk["chunk_index"],
                "text": chunk["text"],
                
                "site": meta.get("site"),
                "board_name": meta.get("board_name"),
                "title": meta.get("title"),
                "url": meta.get("url"),
                "created_at": meta.get("created_at"),
                
                "tags": meta.get("tags", []),
                "source_type": meta.get("source_type"),
                "file_name": meta.get("original_filename"),
                "parent_title": meta.get("parent_title")
            }

            points.append(
                qm.PointStruct(
                    id=fixed_id,
                    vector=vec.tolist(),
                    payload=payload,
                )
            )

        # Qdrant ì—…ë¡œë“œ (ë®ì–´ì“°ê¸°)
        client.upsert(
            collection_name=COLLECTION_NAME,
            points=points,
        )
        
        # [Update] ì²˜ë¦¬ëœ IDì™€ í•´ì‹œê°’ ê¸°ë¡
        save_processed_log(log_path, log_items)

        total_new_chunks += len(batch)
        print(f"âœ… ì—…ë°ì´íŠ¸ ë°°ì¹˜ {len(batch)}ê°œ ì™„ë£Œ (ëˆ„ì : {total_new_chunks})")

    if total_new_chunks == 0:
        print("âœ¨ ìƒˆë¡œ ì¶”ê°€/ë³€ê²½ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print(f"ğŸ‰ ì´ {total_new_chunks}ê°œ ì²­í¬ ì—…ë°ì´íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    _, _, chunks_dir, _ = get_project_paths()
    embed_and_upload(chunks_dir)