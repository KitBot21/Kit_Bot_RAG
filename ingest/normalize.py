# normalize.py
import json
import hashlib
import re
from pathlib import Path
from datetime import datetime
from urllib.parse import urlparse, parse_qs

# ---------------------------------------------------------
# ftfy: í…ìŠ¤íŠ¸ ê¹¨ì§ ìë™ ë³µêµ¬
# ---------------------------------------------------------
try:
    import ftfy
    def fix_text(text: str) -> str:
        if not text: return ""
        return ftfy.fix_text(text)
except ImportError:
    print("âš ï¸ ftfy ëª¨ë“ˆì´ ì—†ìŠµë‹ˆë‹¤. 'pip install ftfy'ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
    def fix_text(text: str) -> str:
        return text or ""

def get_valid_date(raw: dict, meta: dict):
    """
    ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ë‚ ì§œë¥¼ ì°¾ì•„ ê°€ì¥ í™•ì‹¤í•œ ê²ƒì„ ë°˜í™˜
    ìš°ì„ ìˆœìœ„: raw['created_at'] > meta['created_at'] > meta['post_date']
    """
    candidates = [
        raw.get("created_at"),
        meta.get("created_at"),
        meta.get("post_date")
    ]
    
    for date_str in candidates:
        if not date_str: continue
        # í¬ë§· ì •ê·œí™” (YYYY.MM.DD -> YYYY-MM-DD)
        s = str(date_str).strip().replace(".", "-")
        try:
            # ì‹œê°„ê¹Œì§€ ìˆëŠ” ê²½ìš° (ISO format) ì•ë¶€ë¶„ë§Œ ì ˆì‚­
            if "T" in s: s = s.split("T")[0]
            
            dt = datetime.strptime(s, "%Y-%m-%d")
            return dt.isoformat().split("T")[0], True
        except ValueError:
            continue
            
    return None, False

def extract_title_from_text(text: str):
    if not text: return None
    lines = [l.strip() for l in text.splitlines() if l.strip()]
    if not lines: return None
    return fix_text(lines[0])

def parse_created_at_from_meta(meta: dict):
    post_date = meta.get("post_date")
    if not post_date: return None, False
    s = post_date.strip().replace(".", "-")
    try:
        dt = datetime.strptime(s, "%Y-%m-%d")
        return dt.isoformat(), True
    except ValueError:
        return None, False

def parse_header_from_text(text: str):
    lines = [l.strip() for l in text.splitlines() if l.strip()]
    author = None
    view_count = None
    created_at_str = None 
    for i, line in enumerate(lines):
        if line == "ì‘ì„±ì" and i + 1 < len(lines):
            author = lines[i + 1]
        elif line == "ì¡°íšŒ" and i + 1 < len(lines):
            num = "".join(ch for ch in lines[i + 1] if ch.isdigit())
            view_count = int(num) if num else None
        elif line == "ì‘ì„±ì¼" and i + 1 < len(lines):
            created_at_str = lines[i + 1]
    return author, view_count, created_at_str

def clean_main_text(text: str):
    """
    í—¤ë”/ê¼¬ë¦¬ ì œê±° ë° [í‘œ ë°ì´í„°] íƒœê·¸ ì •ë¦¬ (ë‚´ìš© ì†Œì‹¤ ë°©ì§€ í¬í•¨)
    """
    if not text: return ""
    
    # ë°±ì—…ìš© ì›ë³¸
    original_text = text

    # 1) [í‘œ ë°ì´í„° ì‹œì‘/ë] íƒœê·¸ ì œê±°
    text = text.replace("[í‘œ ë°ì´í„° ì‹œì‘]", "").replace("[í‘œ ë°ì´í„° ë]", "")

    lines = [l.rstrip() for l in text.splitlines()]

    # 2) í—¤ë” ì¤„ ìŠ¤í‚µ
    start_idx = 0
    header_keys = {"ì‘ì„±ì", "ì¡°íšŒ", "ì‘ì„±ì¼"}
    found_header = False
    for i, line in enumerate(lines):
        if line.strip() in header_keys:
            found_header = True
        if found_header and line.strip() == "ì‘ì„±ì¼":
            if i + 1 < len(lines): start_idx = i + 1
            break
    body_lines = lines[start_idx:]

    # 3) ê¼¬ë¦¬ ë¶€ë¶„ ì˜ë¼ë‚´ê¸°
    tail_markers = {"ì´ì „ê¸€", "ë‹¤ìŒê¸€", "ëª©ë¡"}
    end_idx = len(body_lines)
    for i, line in enumerate(body_lines):
        # ì •í™•íˆ ì¼ì¹˜í•˜ê±°ë‚˜ ì‹œì‘í•˜ëŠ” ê²½ìš° ìë¦„
        if any(line.strip() == marker or line.strip().startswith(marker) for marker in tail_markers):
            end_idx = i
            break
    body_lines = body_lines[:end_idx]

    cleaned_text = "\n".join([l for l in body_lines if l.strip()])

    # ğŸ”´ [ì¤‘ìš”] ì •ì œí–ˆë”ë‹ˆ ë‚´ìš©ì´ ë‹¤ ë‚ ì•„ê°”ìœ¼ë©´(10ì ë¯¸ë§Œ), ì›ë³¸ í…ìŠ¤íŠ¸(íƒœê·¸ë§Œ ë—€ ê²ƒ) ë°˜í™˜
    if len(cleaned_text) < 10 and len(original_text) > 50:
        return text.strip()

    return cleaned_text

def infer_site_and_board_from_title(raw: dict):
    meta = raw.get("metadata", {})
    meta_title = fix_text(meta.get("title") or raw.get("title") or "")
    parts = [p.strip() for p in meta_title.split("|") if p.strip()]
    site = None
    board_name = None
    if parts:
        site = parts[0]
        if len(parts) >= 2: board_name = " | ".join(parts[1:])
    if not (site and board_name):
        u = urlparse(raw.get("url", ""))
        path_parts = [p for p in u.path.split("/") if p]
        if not site and path_parts: site = path_parts[0]
        if not board_name and len(path_parts) > 1: board_name = path_parts[1]
    return site, board_name

def parse_schedule_by_regex(text: str) -> str:
    """
    Regexë¡œ ë‚ ì§œ íŒ¨í„´ì´ ìˆëŠ” ë¼ì¸ë§Œ ì¶”ì¶œ (í‘œ ë°ì´í„°ê°€ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ëœ ê²½ìš°ì—ë„ ìœ íš¨)
    """
    lines = text.splitlines()
    sentences = []
    date_pattern = re.compile(r'(\d{1,2})[\./-](\d{1,2})')
    
    sentences.append("ì´ ë¬¸ì„œëŠ” ê¸ˆì˜¤ê³µëŒ€ í•™ì‚¬ì¼ì • ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.")
    
    for line in lines:
        line = line.strip()
        if not line: continue
        if date_pattern.search(line):
            # íŒŒì´í”„(|)ê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚´ë ¤ì„œ êµ¬ì¡° ìœ ì§€
            sentences.append(f"ì¼ì • ì •ë³´: {line}")
        
    if len(sentences) <= 1:
        return text
    return "\n".join(sentences)

def normalize_schedule_main_text(doc: dict) -> dict:
    """
    í•™ì‚¬ì¼ì • í˜ì´ì§€ main_textë¥¼ ìì—°ì–´ ë¬¸ì¥ìœ¼ë¡œ ì •ê·œí™”
    (ë‹¨ìˆœ ì¤„ë°”ê¿ˆ ë°ì´í„°ë¥¼ 'ì œëª©: ë‚ ì§œ' í˜•íƒœë¡œ ë³€í™˜)
    """
    text = (doc.get("main_text") or "").strip()
    url = (doc.get("url") or "").strip()
    board_name = (doc.get("board_name") or "").strip()
    title = (doc.get("title") or "").strip()

    # 1) í•™ì‚¬ì¼ì • ë¬¸ì„œ ì—¬ë¶€ íŒë‹¨
    is_schedule_page = (
        "schedule" in url
        or "í•™ì‚¬ì¼ì •" in board_name
        or "í•™ì‚¬ì¼ì •" in title
    )
    
    if not is_schedule_page or not text:
        return doc

    # 2) ë¼ì¸ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (ê³µë°± ë¼ì¸ ì œê±°)
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]

    # 3) 'ë²ˆí˜¸' í‚¤ì›Œë“œ ì°¾ê¸°
    try:
        header_idx = lines.index("ë²ˆí˜¸")
    except ValueError:
        # ë²ˆí˜¸ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ê·¸ëƒ¥ ì›ë³¸ ë¦¬í„´ (í˜¹ì€ Regex ì²˜ë¦¬)
        return doc

    # 4) í—¤ë” í™•ì¸ (ë²ˆí˜¸, ì œëª©, ì‹œì‘ì¼, ì¢…ë£Œì¼, ë“±ë¡ì¼, ì¡°íšŒ) -> ì´ 6ê°œ ì»¬ëŸ¼ êµ¬ì¡°
    # ì‹¤ì œ ë°ì´í„°ê°€ 6ì¤„ ë‹¨ìœ„ë¡œ ë°˜ë³µë˜ëŠ”ì§€ í™•ì¸
    
    summary_lines = []
    summary_lines.append(f"ì´ ë¬¸ì„œëŠ” {board_name} ì •ë³´ì…ë‹ˆë‹¤.")
    summary_lines.append("ì£¼ìš” ì¼ì •ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n")

    # í—¤ë” ë‹¤ìŒë¶€í„° ë°ì´í„° ì‹œì‘
    # ë³´í†µ í—¤ë”ê°€ 6ì¤„(ë²ˆí˜¸~ì¡°íšŒ)ì´ë¼ê³  ê°€ì •
    i = header_idx + 6 
    
    # ë°ì´í„° íŒŒì‹±
    while i + 5 < len(lines):
        # 6ê°œì”© ëŠì–´ì„œ ì½ê¸°
        chunk = lines[i : i + 6]
        
        # chunk êµ¬ì¡°: [ë²ˆí˜¸, ì œëª©, ì‹œì‘ì¼, ì¢…ë£Œì¼, ë“±ë¡ì¼, ì¡°íšŒìˆ˜]
        # ì˜ˆ: ['360', '2í•™ê¸° ê°œì‹œì¼', '2025-09-01', '2025-09-01', '2024-11-27', '0']
        
        row_title = chunk[1]
        start_date = chunk[2]
        end_date = chunk[3]
        
        # ë‚ ì§œ í˜•ì‹ì´ ë§ëŠ”ì§€ ê°„ë‹¨ ì²´í¬ (YYYY-MM-DD)
        if "-" in start_date:
            if start_date == end_date:
                sentence = f"â€¢ {row_title}: {start_date} (í•˜ë£¨)"
            else:
                sentence = f"â€¢ {row_title}: {start_date} ~ {end_date}"
            summary_lines.append(sentence)
        
        i += 6 # ë‹¤ìŒ 6ì¤„ë¡œ ì´ë™

    # ë³€í™˜ëœ ë‚´ìš©ì´ ìˆìœ¼ë©´ êµì²´
    if len(summary_lines) > 2:
        print(f"ğŸ“… í•™ì‚¬ì¼ì • ë³€í™˜ ì„±ê³µ: {len(summary_lines)-2}ê°œ ì¼ì • ì¶”ì¶œë¨ ({doc['doc_id']})")
        doc["main_text"] = "\n".join(summary_lines)
        
        # ê²€ìƒ‰ í‚¤ì›Œë“œ ë³´ê°•
        extra_keywords = ["í•™ì‚¬ì¼ì •", "ì¼ì •í‘œ", "ì£¼ìš”í•™ì‚¬ì¼ì •", "ê°œê°•", "ì¢…ê°•", "ì‹œí—˜ê¸°ê°„"]
        existing_tags = doc.get("tags") or []
        doc["tags"] = list(dict.fromkeys(existing_tags + extra_keywords))

    return doc

def make_doc_id_from_url(raw: dict):
    url = raw.get("url", "")
    u = urlparse(url)
    path_parts = [p for p in u.path.split("/") if p]
    host = (u.netloc or "site").split(".")[0]
    slug = path_parts[-1].split(".")[0] if path_parts else "root"
    qs = parse_qs(u.query)
    
    article_no = qs.get("articleNo", [None])[0]
    if article_no: return f"{host}_{slug}_{article_no}"
    
    offset = qs.get("article.offset", [None])[0]
    if offset: return f"{host}_{slug}_offset{offset}"
    
    page = qs.get("page", [None])[0]
    if page: return f"{host}_{slug}_p{page}"
    
    if u.query:
        h = hashlib.md5(u.query.encode("utf-8")).hexdigest()[:8]
        return f"{host}_{slug}_{h}"
    return f"{host}_{slug}"

def normalize_notice(raw: dict):
    meta = raw.get("metadata", {})
    
    # [Fix] ë‚ ì§œ ì¶”ì¶œ í•¨ìˆ˜ êµì²´
    created_at, has_date = get_valid_date(raw, meta)
    
    text_content = fix_text(raw.get("main_text", "") or raw.get("text", ""))  # <--- "main_text"ë¥¼ ë¨¼ì € ì°¾ë„ë¡ ìˆ˜ì •
    author_from_text, view_from_text, created_from_text = parse_header_from_text(text_content)

    # ë©”íƒ€ë°ì´í„°ì— ë‚ ì§œê°€ ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ì—ì„œ íŒŒì‹± ì‹œë„
    if not created_at and created_from_text:
        s = created_from_text.strip().replace(".", "-")
        try:
            dt = datetime.strptime(s, "%Y-%m-%d")
            created_at = dt.isoformat().split("T")[0]
            has_date = True
        except ValueError: pass

    site, board_name = infer_site_and_board_from_title(raw)
    final_title = fix_text(raw.get("title") or extract_title_from_text(text_content))
    main_text = clean_main_text(text_content)

    # ğŸ”´ [ì¤‘ìš” ìˆ˜ì •] attachmentsê°€ rootì— ìˆëŠ”ì§€, metadata ì•ˆì— ìˆëŠ”ì§€ ëª¨ë‘ í™•ì¸
    raw_attachments = raw.get("attachments") or meta.get("attachments") or []

    unified = {
        "doc_id": make_doc_id_from_url(raw),
        "source_type": "board",
        "site": fix_text(site),
        "board_name": fix_text(board_name),
        "title": final_title,
        "display_title": final_title,
        "author": fix_text(author_from_text),
        "url": raw.get("url"),
        "created_at": created_at,
        "updated_at": None,
        "has_explicit_date": has_date,
        "view_count": view_from_text,
        "doc_type": "html",
        "main_text": fix_text(main_text),
        
        # [Fix] ìˆ˜ì •ëœ ì²¨ë¶€íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©
        "attachments": raw_attachments,
        
        "images": [],
        "crawled_at": meta.get("crawled_at") or raw.get("crawled_at"),
        "source_meta": {
            "text_length": meta.get("text_length"),
            "raw_title": fix_text(raw.get("title")),
        },
    }
    unified = normalize_schedule_main_text(unified)
    return unified

def normalize_directory(input_dir: str, output_dir: str):
    input_dir = Path(input_dir)
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    count = 0
    skipped = 0
    # ğŸ”´ [ì¤‘ìš” ë³€ê²½] í•˜ìœ„ í´ë”ê¹Œì§€ ì¬ê·€ì ìœ¼ë¡œ íƒìƒ‰ (**/*.json)
    for path in input_dir.glob("**/*.json"):
        # [New] ì´ë¯¸ ë³€í™˜ëœ íŒŒì¼ì¸ì§€ í™•ì¸
        out_path = output_dir / f"{path.stem}.unified.json"
        if out_path.exists():
            if path.stat().st_mtime <= out_path.stat().st_mtime:
                skipped += 1
                continue

        try:
            with path.open(encoding="utf-8") as f:
                raw = json.load(f)
            
            unified = normalize_notice(raw)

            with out_path.open("w", encoding="utf-8") as f:
                json.dump(unified, f, ensure_ascii=False, indent=2)
            
            count += 1
        except Exception as e:
            print(f"âŒ Error processing {path}: {e}")

    print(f"âœ… ë³€í™˜ ì™„ë£Œ: {count}ê°œ (ê±´ë„ˆëœ€: {skipped}ê°œ) â†’ {output_dir}")

if __name__ == "__main__":
    normalize_directory("data/raw", "data/unified")