#!/usr/bin/env python3
"""
repeatCrawler.py (ì§„ì§œ ìµœì¢… í†µí•©ë³¸)

[ê¸°ëŠ¥ ëª©ë¡]
1. Requests Session + Retry (íƒ€ì„ì•„ì›ƒ ë°©ì§€)
2. ì•„ì´ì½˜ ì´ë¯¸ì§€ ìŠ¤í‚µ (ì†ë„ í–¥ìƒ)
3. HTML Table -> Text ë³€í™˜ (í•™ì‚¬ì¼ì • í’ˆì§ˆ)
4. articleNo ê¸°ì¤€ ì¤‘ë³µ ë°©ì§€ (ìƒë‹¨ ê³ ì • ê³µì§€ í•´ê²°)
5. ë‚ ì§œ ê¸°ì¤€ ì¡°ê¸° ì¢…ë£Œ (ê³¼ê±° ë°ì´í„° ìŠ¤í‚µ)
6. ì‹ë‹¹ ë©”ë‰´ í¬ë¡¤ë§ ë¡œì§ í¬í•¨ (ëˆ„ë½ ì—†ìŒ)
"""

import json
import sys
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import urllib
from urllib.parse import urljoin, parse_qs, urlparse, urlencode, urlunparse
from typing import Optional
from pathlib import Path
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from ftfy import fix_text
from dotenv import load_dotenv
import re
import time
import hashlib 
import mimetypes

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(dotenv_path=Path(__file__).parent.parent / ".env")

# crawler ëª¨ë“ˆ ì„í¬íŠ¸
sys.path.insert(0, str(Path(__file__).parent))

from filters.content_extractor import ContentExtractor
from filters.quality_filter import QualityFilter
from filters.date_filter import DateFilter
from storage.json_storage import JSONStorage
from storage.minio_storage import MinIOStorage
from sendToServer import check_and_notify
import logging
import hashlib

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

exclude_patterns = ["/cms/fileDownload.do"]

# ì•„ì´ì½˜ í•„í„°
ICON_IMAGE_KEYWORDS = [
    "/_res/ko/img/icon/", "/res/ko/img/common/", "logo", "btn", "btn-", 
    "bg_subvisual", "wa-mark", "bubble_tail", "btn_top_go", "icon",
    "insta", "youtube", "blog", "facebook", "twitter", "kakao", 
    "banner", "footer", "header", "arrow", "line", "bg_", "common"
]

class SimpleTestCrawler:
    def __init__(self, enable_minio: bool = False, output_dir: Optional[Path] = None):
        self.base_url = "https://www.kumoh.ac.kr"
        self.bus_base_url = "https://bus.kumoh.ac.kr"
        
        # ì„¸ì…˜ ì„¤ì •
        self.session = requests.Session()
        retries = Retry(total=5, backoff_factor=1, status_forcelist=[500, 502, 503, 504], allowed_methods=["GET"])
        adapter = HTTPAdapter(max_retries=retries)
        self.session.mount("https://", adapter)
        self.session.mount("http://", adapter)

        # MinIO ì„¤ì •
        self.enable_minio = enable_minio
        if enable_minio:
            try:
                self.minio = MinIOStorage.from_env()
                logger.info("âœ… MinIO ìŠ¤í† ë¦¬ì§€ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸  MinIO ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.enable_minio = False
                self.minio = None
        else:
            self.minio = None
        
        # URL ëª©ë¡
        self.target_urls = [    
            "https://www.kumoh.ac.kr/ko/restaurant01.do",
            "https://www.kumoh.ac.kr/ko/restaurant02.do",
            "https://www.kumoh.ac.kr/ko/restaurant04.do",
            "https://www.kumoh.ac.kr/ko/restaurant05.do",
            "https://www.kumoh.ac.kr/dorm/restaurant_menu01.do",
            "https://www.kumoh.ac.kr/dorm/restaurant_menu02.do",
            "https://www.kumoh.ac.kr/dorm/restaurant_menu03.do",
        ]
        
        self.board_urls = [
            {"url": "https://bus.kumoh.ac.kr/bus/notice.do", "name": "í†µí•™ë²„ìŠ¤ ê³µì§€", "max_pages": 0, "skip_date_filter": True},
            {"url": "https://www.kumoh.ac.kr/ko/sub06_01_01_01.do", "name": "ê³µì§€ì‚¬í•­ í•™ì‚¬ì•ˆë‚´", "max_pages": 0},
            {"url": "https://www.kumoh.ac.kr/ko/sub01_02_03.do", "name": "ì—…ë¬´ì¶”ì§„ë¹„ ì‚¬ìš©ë‚´ì—­", "max_pages": 0},
            {"url": "https://www.kumoh.ac.kr/ko/sub01_05_01.do", "name": "KIT Projects", "max_pages": 0, "skip_date_filter": True},
            {"url": "https://www.kumoh.ac.kr/ko/sub01_05_04.do", "name": "ë³´ë„ìë£Œ", "max_pages": 0},
            {"url": "https://www.kumoh.ac.kr/ko/sub06_01_01_02.do", "name": "ê³µì§€ì‚¬í•­ í–‰ì‚¬ì•ˆë‚´", "max_pages": 0},
            {"url": "https://www.kumoh.ac.kr/ko/sub06_01_01_03.do", "name": "ê³µì§€ì‚¬í•­ ì¼ë°˜ì†Œì‹", "max_pages": 0},
            {"url": "https://www.kumoh.ac.kr/ko/sub06_03_04_02.do", "name": "ì •ë³´ê³µìœ  ê¸ˆì˜¤ë³µë•ë°©", "max_pages": 0},
            # {"url": "https://www.kumoh.ac.kr/ko/sub06_03_04_04.do", "name": "ì •ë³´ê³µìœ  ì•„ë¥´ë°”ì´íŠ¸ì •ë³´", "max_pages": 0, "months_limit": 3},
            {"url": "https://www.kumoh.ac.kr/ko/sub06_03_05_01.do", "name": "ë¬¸í™”ì˜ˆìˆ ê³µê°„ í´ë˜ì‹ê°ìƒ", "max_pages": 0},
            {"url": "https://www.kumoh.ac.kr/ko/sub06_03_05_02.do", "name": "ë¬¸í™”ì˜ˆìˆ ê³µê°„ ê°¤ëŸ¬ë¦¬", "max_pages": 0},
            {"url": "https://www.kumoh.ac.kr/ko/sub06_05_02.do", "name": "ì´ì¥ì„ìš©í›„ë³´ìì¶”ì²œìœ„ì›íšŒ ê³µì§€ì‚¬í•­", "max_pages": 0},
            {"url": "https://www.kumoh.ac.kr/dorm/sub0401.do", "name": "ìƒí™œê´€ ê³µì§€ì‚¬í•­", "max_pages": 0},
            {"url": "https://www.kumoh.ac.kr/dorm/sub0407.do", "name": "ìƒí™œê´€ ì„ ë°œ ê³µì§€ì‚¬í•­", "max_pages": 0},
            {"url": "https://www.kumoh.ac.kr/dorm/sub0408.do", "name": "ìƒí™œê´€ ì…í‡´ì‚¬ ê³µì§€ì‚¬í•­", "max_pages": 0},
            {"url": "https://www.kumoh.ac.kr/dorm/sub0603.do", "name": "ì‹ í‰ë™ ì‹ ì²­ë°©ë²•", "max_pages": 0},
            {"url": "https://www.kumoh.ac.kr/ko/sub01_01_07_02.do", "name": "ëŒ€í•™ì†Œê°œ í˜„í™© ì¬ì •í˜„í™©", "max_pages": 0},
            {"url": "https://www.kumoh.ac.kr/ko/sub01_01_07_03.do", "name": "ëŒ€í•™ì†Œê°œ í˜„í™© ì¬ì •ìœ„ì›íšŒ íšŒì˜ë¡", "max_pages": 0},
            {"url": "https://www.kumoh.ac.kr/ko/sub01_01_07_04.do", "name": "ëŒ€í•™ì†Œê°œ í˜„í™© ëŒ€í•™í‰ì˜ì›íšŒ íšŒì˜ë¡", "max_pages": 0},
            {"url": "https://www.kumoh.ac.kr/ko/sub01_01_07_05.do", "name": "ëŒ€í•™ì†Œê°œ í˜„í™© ë“±ë¡ê¸ˆì‹¬ì˜ìœ„ì› íšŒì˜ë¡", "max_pages": 0},
            {"url": "https://www.kumoh.ac.kr/ko/sub01_01_08.do", "name": "ëŒ€í•™ì†Œê°œ UI", "max_pages": 0},
            {"url": "https://www.kumoh.ac.kr/ko/sub01_04.do", "name": "ëŒ€í•™ì†Œê°œ ê·œì •ì§‘", "max_pages": 0},
            {"url": "https://www.kumoh.ac.kr/ko/sub01_05_02.do", "name": "ëŒ€í•™ì†Œê°œ í™ë³´ KIT People", "max_pages": 0},
            {"url": "https://www.kumoh.ac.kr/ko/sub01_05_03.do", "name": "ëŒ€í•™ì†Œê°œ í™ë³´ KIT News", "max_pages": 0},
            {"url": "https://www.kumoh.ac.kr/ko/sub07_01_02.do", "name": "ê¸ˆì˜¤ì‹ ë¬¸ê³  ì²­íƒê¸ˆì§€ë²•ìë£Œì‹¤", "max_pages": 0},
            {"url": "https://www.kumoh.ac.kr/ko/sub07_01_03.do", "name": "ê¸ˆì˜¤ì‹ ë¬¸ê³  í–‰ë™ê°•ë ¹ìë£Œì‹¤", "max_pages": 0}
        ]
        
        self.quality_filter = QualityFilter(min_text_length=100, max_text_length=500000, min_word_count=20)
        self.date_filter = DateFilter(cutoff_date="2024-01-01")
        
        if output_dir is None:
            output_dir = Path(__file__).parent.parent / "data" / "raw" / "core"
        self.output_dir = Path(output_dir)
        self.storage = JSONStorage(self.output_dir, pretty_print=True)
        self.content_extractor = ContentExtractor(keep_links=True, keep_images=False)
        
        self.stats = {"total": 0, "success": 0, "failed": 0, "filtered": 0, "filtered_date": 0, "skipped": 0, "attachments_found": 0, "attachments_uploaded": 0}
        self.saved_pages = []
        self.existing_urls = set()
        self.collected_article_nos = set()
        self.index_meta: dict = {}
        self._load_existing_index()

    def _clean_url(self, url: str) -> str:
        parsed = urlparse(url)
        qs = parse_qs(parsed.query)
        
        # ë³€ë™ë˜ëŠ” íŒŒë¼ë¯¸í„° ì‚­ì œ
        for key in ['article.offset', 'articleLimit']:
            if key in qs: del qs[key]
        
        # ì¬ì¡°ë¦½
        new_query = urlencode(qs, doseq=True)
        return urlunparse(parsed._replace(query=new_query))
    
    def _get_article_no(self, url: str) -> Optional[str]:
        try:
            parsed = urlparse(url)
            qs = parse_qs(parsed.query)
            return qs.get('articleNo', [None])[0]
        except: return None

    def _load_existing_index(self):
        index_file = self.output_dir / "crawl_index.json"
        if index_file.exists():
            try:
                with open(index_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    for page in data.get('pages', []):
                        # ì €ì¥ëœ URLë„ ì„¸íƒí•´ì„œ ê¸°ì–µ
                        clean = self._clean_url(page['url'])
                        self.existing_urls.add(clean)
                        ano = self._get_article_no(clean)
                        if ano: self.collected_article_nos.add(ano)
            except: pass

    def _convert_tables_to_text(self, html_content: str) -> str:
        if not html_content: return ""
        soup = BeautifulSoup(html_content, "html.parser")
        tables = soup.find_all("table")
        if not tables: return html_content 
        for table in tables:
            rows_text = []
            for tr in table.find_all("tr"):
                cells = [cell.get_text(strip=True) for cell in tr.find_all(["th", "td"])]
                if any(cells): rows_text.append(" | ".join(cells))
            if rows_text:
                new_div = soup.new_tag("div")
                new_div.string = f"\n[í‘œ ë°ì´í„° ì‹œì‘]\n" + "\n".join(rows_text) + "\n[í‘œ ë°ì´í„° ë]\n"
                table.replace_with(new_div)
        return str(soup)

    def _is_file_exist(self, url: str) -> bool:
        clean = self._clean_url(url)
        url_hash = hashlib.sha256(clean.encode()).hexdigest()[:16]
        file_path = self.output_dir / "pages" / f"{url_hash}.json"
        return file_path.exists()
    
    def crawl_url(self, url: str, skip_date_filter: bool = False, context: dict | None = None) -> bool:
        # 1. [Fix] íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ë¶€í„° ê°€ì¥ ë¨¼ì € í™•ì¸ (ë¡œê·¸ ì¶œë ¥ ì—†ì´ ì¡°ìš©íˆ ê²€ì‚¬)
        if self._is_file_exist(url):
            logger.info(f"â­ï¸ íŒŒì¼ ì¡´ì¬í•¨ - ìŠ¤í‚µ: {url}")
            self.stats["skipped"] += 1
            return True  # ì´ë¯¸ ì„±ê³µí•œ ê²ƒìœ¼ë¡œ ê°„ì£¼

        # 2. [Fix] ì‹¤ì œë¡œ í¬ë¡¤ë§í•  ë•Œë§Œ Total ì¹´ìš´íŠ¸ ì¦ê°€ ë° ë¡œê·¸ ì¶œë ¥
        self.stats["total"] += 1
        context = context or {}
        logger.info(f"í¬ë¡¤ë§ ì‹œì‘: {url}")

        try:
            headers = {'User-Agent': 'KITBot/2.0'}
            response = self.session.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            html = response.text
            post_date = self._extract_post_date(html)
            
            if not skip_date_filter:
                date_str = post_date or self._extract_date_from_html(html)
                if date_str and not self.date_filter.is_recent(date_str):
                    logger.info(f"  â­ï¸  ë‚ ì§œ í•„í„°: {date_str} (2024-01-01 ì´ì „)")
                    self.stats["filtered"] += 1
                    self.stats["filtered_date"] += 1
                    return False
            
            author, view_count, created_at = None, None, post_date
            if context.get("source_type") == "board":
                author, view_count, b_created = self._extract_board_meta(html)
                if b_created: created_at = b_created
            
            is_quality, reason = self.quality_filter.is_high_quality(html, url)
            if not is_quality:
                self.stats["filtered"] += 1
                return False
            
            html_with_tables = self._convert_tables_to_text(html)
            content_data = self.content_extractor.extract_with_metadata(html_with_tables)
            attachments = self._process_attachments(url, html)
            board_title = self._extract_board_title(html) if context.get("source_type") == "board" else None
            title_for_json = board_title or content_data['title'] or context.get("board_name")
            
            metadata = {
                "text_length": len(content_data['text']), "word_count": content_data['word_count'], "title": title_for_json,
                "paragraphs": content_data['paragraphs'], "link_count": len(content_data['links']),
                "attachments_count": len(attachments), "attachments": attachments, "images": content_data['images'],
                "quality_check": reason, "crawled_at": datetime.now().isoformat(), "source_url": url,
                "source_type": context.get("source_type", "page"), "board_name": content_data['title'],
                "author": author, "view_count": view_count, "created_at": created_at, "has_explicit_date": bool(created_at)
            }
            
            filepath = self.storage.save_page(url, html, metadata, extracted_text=content_data['text'], title=title_for_json)
            self.saved_pages.append({"url": url, "file": filepath, "title": title_for_json, "text_length": len(content_data['text'])})
            self.stats["success"] += 1
            logger.info(f"âœ… ì €ì¥ ì™„ë£Œ: {Path(filepath).name}")

            try:
                # metadata['title']ì—ëŠ” ì´ë¯¸ ì •ì œëœ ì œëª©ì´ ë“¤ì–´ìˆìŠµë‹ˆë‹¤.
                check_and_notify(
                    url=url,
                    title=metadata["title"]
                )
            except Exception as e:
                logger.warning(f"âš ï¸ ì•ˆë“œë¡œì´ë“œ ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {e}")

            return True
            
        except Exception as e:
            logger.error(f"âŒ ì—ëŸ¬: {e}")
            self.stats["failed"] += 1
            return False

    def _extract_board_title(self, html):
        soup = BeautifulSoup(html, 'html.parser')
        head = soup.find('div', class_='title-area')
        if not head: return None
        for tag in ['h4', 'h3', 'strong']:
            el = head.find(tag)
            if el and el.get_text(strip=True): return el.get_text(strip=True)
        return None

    def _extract_board_meta(self, html):
        soup = BeautifulSoup(html, 'html.parser')
        info_div = soup.find('div', class_='board-view-information')
        author, view, created = None, None, None
        if not info_div: return None, None, None
        for dl in info_div.find_all('dl'):
            dt, dd = dl.find('dt'), dl.find('dd')
            if not dt or not dd: continue
            k, v = dt.get_text(strip=True), dd.get_text(strip=True)
            if 'ì‘ì„±ì' in k: author = v
            elif 'ì¡°íšŒ' in k: 
                d = ''.join(c for c in v if c.isdigit())
                if d: view = int(d)
            elif 'ì‘ì„±ì¼' in k:
                m = re.search(r'(\d{4})[.-](\d{2})[.-](\d{2})', v)
                if m: created = f"{m.group(1)}-{m.group(2)}-{m.group(3)}"
        return author, view, created

    def _process_attachments(self, page_url: str, html: str) -> list:
        # [Update] Content-Type ê¸°ë°˜ í™•ì¥ì ë³´ì • ë¡œì§ ì¶”ê°€
        attachments = []
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # 1. ì²¨ë¶€íŒŒì¼ (a íƒœê·¸)
            for link in soup.find_all('a', href=True):
                href = link['href']
                link_text = link.get_text(strip=True)
                is_download = (
                    'mode=download' in href or
                    'download' in href.lower() or
                    any(href.lower().endswith(x) for x in ['.pdf','.hwp','.docx','.xlsx','.zip'])
                )
                if any(p in href for p in exclude_patterns): is_download = False
                if not is_download: continue
                
                abs_url = urljoin(page_url, href)
                self.stats["attachments_found"] += 1
                att_info = {"page_url": page_url, "link_text": link_text, "download_url": abs_url, "detected_at": datetime.now().isoformat()}
                
                if self.enable_minio and self.minio:
                    try:
                        headers = {'User-Agent': 'KITBot/2.0', 'Referer': page_url}
                        resp = self.session.get(abs_url, headers=headers, timeout=30)
                        resp.raise_for_status()
                        
                        file_data = resp.content
                        content_type = resp.headers.get('Content-Type', '').split(';')[0].strip()
                        
                        # íŒŒì¼ëª… ê²°ì • ë¡œì§
                        content_disp = resp.headers.get('Content-Disposition', '')
                        if 'filename=' in content_disp: 
                            filename = content_disp.split('filename=')[-1].strip('"\'')
                        else:
                            filename = abs_url.split('/')[-1].split('?')[0]
                            # íŒŒì¼ëª…ì— í™•ì¥ìê°€ ì—†ê±°ë‚˜ .do ì¸ ê²½ìš°
                            if '.' not in filename or filename.endswith('.do'):
                                # ë§í¬ í…ìŠ¤íŠ¸ì— í™•ì¥ìê°€ ìˆìœ¼ë©´ ê·¸ê±¸ ì‚¬ìš© (ì˜ˆ: "ê³µì§€ì‚¬í•­.pdf")
                                if '.' in link_text:
                                    filename = link_text
                                else:
                                    # ê·¸ê²ƒë„ ì—†ìœ¼ë©´ í•´ì‹œê°’ ìƒì„±
                                    filename = f"file_{hashlib.md5(abs_url.encode()).hexdigest()[:8]}"

                        # URL ë””ì½”ë”© ë° ì •ì œ
                        try: filename = urllib.parse.unquote(filename)
                        except: pass
                        filename = fix_text(filename)
                        
                        # [í•µì‹¬] í™•ì¥ì ê°•ì œ ë³´ì • (MIME Type í™œìš©)
                        # ì˜ˆ: filenameì´ "image.do"ì¸ë° content_typeì´ "image/jpeg"ë©´ -> "image.jpg"ë¡œ ë³€ê²½
                        if filename.lower().endswith('.do') or '.' not in filename:
                            guessed_ext = mimetypes.guess_extension(content_type)
                            if guessed_ext:
                                if guessed_ext == '.jpe': guessed_ext = '.jpg' # ìœˆë„ìš° í˜¸í™˜
                                filename = Path(filename).stem + guessed_ext

                        clean_name = filename.replace('/', '_').replace('\\', '_')
                        obj_name = f"attachments/{clean_name}"
                        
                        success, res = self.minio.upload_file(resp.content, obj_name, content_type, metadata={"source_url": abs_url, "original_filename": filename})
                        if success: 
                            att_info.update({"minio_url": res, "minio_object": obj_name, "filename": clean_name, "status": "uploaded"})
                            self.stats["attachments_uploaded"] += 1
                            logger.info(f"   ğŸ“ ì²¨ë¶€íŒŒì¼ ì—…ë¡œë“œ: {clean_name}")
                    except Exception as e:
                        att_info.update({"status": "download_failed", "error": str(e)})
                else: att_info["status"] = "metadata_only"
                attachments.append(att_info)
                
            # 2. ì´ë¯¸ì§€ (img íƒœê·¸)
            image_exts = ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.bmp', '.svg']
            for img in soup.find_all('img', src=True):
                src, alt = img['src'], img.get('alt', '').strip()
                if any(k in src for k in ICON_IMAGE_KEYWORDS): continue
                abs_url = urljoin(page_url, src)
                self.stats["attachments_found"] += 1
                att_info = {"page_url": page_url, "link_text": alt, "download_url": abs_url, "type": "image"}
                
                if self.enable_minio and self.minio:
                    try:
                        resp = self.session.get(abs_url, headers={'User-Agent': 'KITBot/2.0'}, timeout=30)
                        content_type = resp.headers.get('Content-Type', '').split(';')[0].strip()
                        
                        # ê¸°ë³¸ íŒŒì¼ëª… ì¶”ì¶œ
                        fname = abs_url.split('/')[-1].split('?')[0]
                        if not fname: fname = "image"
                        
                        # [í•µì‹¬] í™•ì¥ì ë³´ì •
                        # .do ì´ê±°ë‚˜ í™•ì¥ìê°€ ì—†ìœ¼ë©´ MIME íƒ€ì…ìœ¼ë¡œ ì¶”ì¸¡
                        if fname.lower().endswith('.do') or '.' not in fname:
                            guessed_ext = mimetypes.guess_extension(content_type)
                            if guessed_ext:
                                if guessed_ext == '.jpe': guessed_ext = '.jpg'
                                fname = Path(fname).stem + guessed_ext
                            else:
                                # ì¶”ì¸¡ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
                                fname = Path(fname).stem + ".jpg"

                        clean_name = fname.replace('/', '_').replace('\\', '_')
                        obj_name = f"images/{clean_name}"
                        
                        success, res = self.minio.upload_file(resp.content, obj_name, content_type, metadata={"source_url": abs_url, "alt_text": alt})
                        if success:
                            att_info.update({"minio_url": res, "minio_object": obj_name, "filename": clean_name, "status": "uploaded"})
                            self.stats["attachments_uploaded"] += 1
                            logger.info(f"   ğŸ–¼ ì´ë¯¸ì§€ ì—…ë¡œë“œ: {clean_name}")
                    except Exception as e:
                        att_info.update({"status": "download_failed", "error": str(e)})
                else: att_info["status"] = "metadata_only"
                attachments.append(att_info)
        except Exception as e: logger.error(f"âŒ ì²¨ë¶€íŒŒì¼ ì²˜ë¦¬ ì—ëŸ¬: {e}")
        return attachments

    def _extract_post_date(self, html):
        soup = BeautifulSoup(html, 'html.parser')
        info = soup.find('div', class_='board-view-information')
        if not info: return None
        for dl in info.find_all('dl'):
            if 'ì‘ì„±ì¼' in dl.get_text():
                m = re.search(r'(\d{4})[.-](\d{2})[.-](\d{2})', dl.get_text())
                if m: return f"{m.group(1)}-{m.group(2)}-{m.group(3)}"
        return None

    def _extract_date_from_html(self, html): return self._extract_post_date(html)

    def crawl_list_page(self, url: str, max_pages: int = 10, skip_date_filter: bool = False, board_name: str = "ê²Œì‹œíŒ", custom_cutoff: str = None, max_items: int = 0):
        logger.info(f"\nğŸ“‹ [{board_name}] ë¦¬ìŠ¤íŠ¸ ë¶„ì„ ì‹œì‘")
        page_num = 0
        total_articles = 0
        base_url = self.bus_base_url if 'bus.kumoh.ac.kr' in url else self.base_url
        
        duplicate_strike = 0 # ì—°ì† ì¤‘ë³µ ì¹´ìš´í„°

        while True:
            if page_num == 0: page_url = url
            else:
                offset = page_num * 10
                page_url = f"{url}&article.offset={offset}" if '?' in url else f"{url}?article.offset={offset}"
            
            try:
                response = self.session.get(page_url, headers={'User-Agent': 'KITBot/2.0'}, timeout=30)
                soup = BeautifulSoup(response.text, 'html.parser')
                
                article_links = []
                
                # 1. ë¨¼ì € ë§í¬ ìˆ˜ì§‘ (ë‚ ì§œ ì²´í¬ ì „ì—)
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    if 'mode=view' in href or 'articleNo' in href:
                        if href.startswith('/'): full_url = f"{base_url}{href}"
                        elif href.startswith('?'): full_url = f"{url.split('?')[0]}{href}"
                        else: full_url = href
                        
                        clean_url = self._clean_url(full_url)
                        
                        # ê¸€ë²ˆí˜¸ ì¤‘ë³µ ì²´í¬
                        ano = self._get_article_no(clean_url)
                        if ano and ano in self.collected_article_nos:
                            duplicate_strike += 1
                            if duplicate_strike >= 5:
                                logger.info("ğŸ›‘ [Stop] ì¤‘ë³µ ê²Œì‹œê¸€ ì—°ì† ë°œê²¬. ìµœì‹  ê¸€ ìˆ˜ì§‘ ì™„ë£Œ.")
                                return
                            continue
                        
                        # íŒŒì¼ ì¡´ì¬ ì²´í¬
                        if self._is_file_exist(clean_url):
                            duplicate_strike += 1
                            if duplicate_strike >= 5:
                                logger.info("ğŸ›‘ [Stop] ì´ë¯¸ ìˆ˜ì§‘ëœ êµ¬ê°„(íŒŒì¼ ì¡´ì¬). ì¢…ë£Œ.")
                                return
                            continue

                        duplicate_strike = 0
                        if clean_url not in article_links: 
                            article_links.append(clean_url)
                
                # 2. ë‚ ì§œ ê¸°ì¤€ ì¡°ê¸° ì¢…ë£Œ ì²´í¬ (ë§í¬ ìˆ˜ì§‘ í›„)
                if not skip_date_filter and article_links:
                    old_cnt = 0
                    for row in soup.select('tbody tr'):
                        txt = row.get_text()
                        m = re.search(r'(\d{4})[.-](\d{2})[.-](\d{2})', txt)
                        if m:
                            d = f"{m.group(1)}-{m.group(2)}-{m.group(3)}"
                            if custom_cutoff and d < custom_cutoff: old_cnt += 1
                            elif not custom_cutoff and not self.date_filter.is_recent(d): old_cnt += 1
                    
                    if old_cnt >= 5:
                        logger.info(f"ğŸ›‘ [Stop] ê³¼ê±° ë°ì´í„°({old_cnt}ê°œ) êµ¬ê°„ ì§„ì…. ì´ë²ˆ í˜ì´ì§€ ìŠ¤í‚µí•˜ê³  ì¢…ë£Œ.")
                        break
                
                if not article_links:
                    logger.info(f"   í˜ì´ì§€ {page_num + 1}: ì‹ ê·œ ê¸€ ì—†ìŒ - ì¢…ë£Œ")
                    break

                logger.info(f"   í˜ì´ì§€ {page_num + 1}: {len(article_links)}ê°œ ì‹ ê·œ ê¸€ ìˆ˜ì§‘")
                
                for i, article_url in enumerate(article_links, 1):
                    # crawl_url í˜¸ì¶œ (ì„¸íƒëœ URL ì „ë‹¬)
                    success = self.crawl_url(article_url, skip_date_filter=skip_date_filter, 
                                           context={"source_type": "board", "board_name": board_name})
                    if success:
                        self.existing_urls.add(article_url)
                        ano = self._get_article_no(article_url)
                        if ano: self.collected_article_nos.add(ano)
                    time.sleep(0.5)
                
                page_num += 1
                if max_pages > 0 and page_num >= max_pages: break
                time.sleep(1)

            except Exception as e:
                logger.error(f"âŒ ì—ëŸ¬: {e}")
                break
        
        logger.info(f"\nâœ… [{board_name}] ì´ {total_articles}ê°œ ê²Œì‹œê¸€ í¬ë¡¤ë§ ì™„ë£Œ")

    # -------------------------------------------------------------------------
    # [Fix] Selenium ê¸°ë°˜ í•™ì‚¬ì¼ì • í¬ë¡¤ëŸ¬ (JS ë™ì  ë¡œë”© ì™„ë²½ ëŒ€ì‘)
    # -------------------------------------------------------------------------
    def crawl_yearly_schedule(self, target_years=[2025, 2026]):
        logger.info(f"\nğŸ“… í•™ì‚¬ì¼ì • ì—°ë„ë³„ ìˆ˜ì§‘ ì‹œì‘ (Selenium): {target_years}")
        base_url = "https://www.kumoh.ac.kr/ko/schedule.do"

        # 1. Selenium ì˜µì…˜ ì„¤ì • (í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ)
        chrome_options = Options()
        chrome_options.add_argument("--headless=new")  # ì°½ ë„ìš°ì§€ ì•ŠìŒ
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        
        # ë“œë¼ì´ë²„ ì‹¤í–‰
        try:
            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
        except Exception as e:
            logger.error(f"âŒ Selenium ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return

        for year in target_years:
            # srMonth=1ë¡œ ì„¤ì •í•˜ì—¬ í•´ë‹¹ ì—°ë„ ì ‘ê·¼ (ì‚¬ì´íŠ¸ íŠ¹ì„±ìƒ 1ì›”ë¡œ ê°€ë©´ ì—°ê°„ ë¦¬ìŠ¤íŠ¸ê°€ ë¡œë”©ë¨)
            page_url = f"{base_url}?mode=list&srYear={year}&srMonth=1"
            
            # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (ì¤‘ë³µ ìŠ¤í‚µ)
            if self._is_file_exist(page_url):
                logger.info(f"   â­ï¸ {year}ë…„ í•™ì‚¬ì¼ì • ì´ë¯¸ ì¡´ì¬í•¨ - ìŠ¤í‚µ")
                continue

            try:
                logger.info(f"   ğŸŒ ì ‘ì† ì¤‘... {page_url}")
                driver.get(page_url)
                
                # 2. JS ë Œë”ë§ ëŒ€ê¸° (ë°ì´í„°ê°€ ëœ° ë•Œê¹Œì§€ 3ì´ˆ ëŒ€ê¸°)
                time.sleep(3)
                
                # 3. HTML íŒŒì‹±
                html = driver.page_source
                soup = BeautifulSoup(html, 'html.parser')
                
                schedule_lines = []
                schedule_lines.append(f"{year}í•™ë…„ë„ í•™ì‚¬ì¼ì •")
                schedule_lines.append("êµ¬ì¡°: ê¸°ê°„ | ë‚´ìš©")
                schedule_lines.append("-" * 30)
                
                count = 0
                
                # 4. ì—°ê°„ ì¼ì • í…Œì´ë¸” íŒŒì‹± (.year-schedule í´ë˜ìŠ¤ ë‚´ë¶€)
                # êµ¬ì¡°: div.year-schedule > div.schedule-list > table > tbody > tr
                tbody = soup.select_one(".year-schedule .schedule-table tbody")
                
                if tbody:
                    current_month_label = ""
                    
                    for tr in tbody.select("tr"):
                        tds = tr.find_all("td")
                        
                        # case 1: ì›”(rowspan) / ì¼ì / ë‚´ìš© (3ì¹¸)
                        if len(tds) == 3:
                            current_month_label = tds[0].get_text(strip=True) # ì˜ˆ: "2025ë…„ 01ì›”"
                            date_text = tds[1].get_text(strip=True)           # ì˜ˆ: "01.01(ìˆ˜)"
                            content_text = tds[2].get_text(strip=True)        # ì˜ˆ: "ì‹ ì •"
                        
                        # case 2: ì¼ì / ë‚´ìš© (2ì¹¸, ì›”ì€ ìœ„ì—ì„œ ìƒì†)
                        elif len(tds) == 2:
                            date_text = tds[0].get_text(strip=True)
                            content_text = tds[1].get_text(strip=True)
                        else:
                            continue

                        if content_text:
                            # RAGê°€ ì½ê¸° ì¢‹ì€ í¬ë§·ìœ¼ë¡œ ë³€í™˜
                            # ì˜ˆ: "2025ë…„ 03ì›” | 03.02(ì›”) | 1í•™ê¸° ê°œê°•"
                            line = f"{current_month_label} | {date_text} | {content_text}"
                            schedule_lines.append(line)
                            count += 1
                
                # 5. ê²°ê³¼ ì €ì¥
                if count > 0:
                    logger.info(f"   âœ… {year}ë…„ ì¼ì • {count}ê°œ ìˆ˜ì§‘ ì™„ë£Œ (Selenium)")
                    final_text = "\n".join(schedule_lines)
                    
                    meta = {
                        "title": f"{year}í•™ë…„ë„ í•™ì‚¬ì¼ì •",
                        "crawled_at": datetime.now().isoformat(),
                        "type": "schedule_year",
                        "year": year,
                        "url": page_url # ì›ë³¸ ë§í¬
                    }
                    
                    fp = self.storage.save_page(page_url, html, meta, extracted_text=final_text, title=meta['title'])
                    self.saved_pages.append({"url": page_url, "file": fp})
                    self.stats["success"] += 1
                else:
                    logger.warning(f"   âš ï¸ {year}ë…„ ì¼ì • ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (í…Œì´ë¸” ë¹„ì–´ìˆìŒ)")

            except Exception as e:
                logger.error(f"   âŒ {year}ë…„ ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬: {e}")

        # ë¸Œë¼ìš°ì € ì¢…ë£Œ
        driver.quit()

    def crawl_restaurant_lists(self, url: str, max_pages: int = 1):
        logger.info(f"\nğŸ½ï¸ ì‹ë‹¹ ë©”ë‰´ ë¦¬ìŠ¤íŠ¸ í¬ë¡¤ë§: {url}")
        page_num = 0
        while page_num < max_pages:
            page_url = url
            try:
                response = self.session.get(page_url, headers={'User-Agent': 'KITBot/2.0'}, timeout=30)
                html = response.text
                soup = BeautifulSoup(html, 'html.parser')
                menu_text = self._extract_menu_table(soup)
                
                restaurant_name = "ì‹ë‹¹"
                if 'restaurant01' in url: restaurant_name = "í•™ìƒì‹ë‹¹"
                elif 'restaurant02' in url: restaurant_name = "êµì§ì›ì‹ë‹¹"
                elif 'restaurant04' in url: restaurant_name = "ë¶„ì‹ë‹¹"
                elif 'restaurant05' in url: restaurant_name = "ì‹ í‰ìº í¼ìŠ¤ì‹ë‹¹"
                elif 'restaurant_menu01' in url: restaurant_name = "í‘¸ë¦„ê´€"
                elif 'restaurant_menu02' in url: restaurant_name = "ì˜¤ë¦„ê´€1ë™"
                elif 'restaurant_menu03' in url: restaurant_name = "ì˜¤ë¦„ê´€2ë™"
                
                metadata = {"title": f"{restaurant_name} ë©”ë‰´", "crawled_at": datetime.now().isoformat(), "type": "restaurant_menu"}
                fp = self.storage.save_page(page_url, html, metadata, extracted_text=menu_text, title=metadata['title'])
                self.saved_pages.append({"url": page_url, "file": fp, "title": metadata['title']})
                self.stats["success"] += 1
                logger.info(f"   âœ… ì €ì¥ ì™„ë£Œ: {Path(fp).name} ({restaurant_name})")
                page_num += 1
            except Exception as e:
                logger.error(f"âŒ ì—ëŸ¬: {e}")
                break
    
    def _extract_menu_table(self, soup: BeautifulSoup) -> str:
        table = None
        for t in soup.find_all("table"):
            cap = t.find("caption")
            if cap and "ì‹ë‹¹ ë©”ë‰´ í‘œ" in cap.get_text(strip=True):
                table = t
                break
        if table is None: table = soup.find("table")
        if table is None: return ""
        thead = table.find("thead")
        if not thead: return ""
        ths = thead.find_all("th")
        day_labels = [th.get_text(" ", strip=True) for th in ths if th.get_text(strip=True)]
        num_days = len(day_labels)
        if num_days == 0: return ""
        per_day: list[dict[str, list[str]]] = [dict() for _ in range(num_days)]
        meal_order: list[str] = []
        tbody = table.find("tbody")
        if not tbody: return ""
        for row in tbody.find_all("tr"):
            tds = row.find_all("td")
            if not tds: continue
            for col_idx, td in enumerate(tds):
                if col_idx >= num_days: break
                p = td.find("p")
                if not p: continue
                meal_name = p.get_text(strip=True)
                if not meal_name: continue
                items = [li.get_text(strip=True) for li in td.find_all("li")]
                if not items: continue
                if meal_name not in meal_order: meal_order.append(meal_name)
                day_meals = per_day[col_idx]
                if meal_name not in day_meals: day_meals[meal_name] = []
                day_meals[meal_name].extend(items)
        lines: list[str] = []
        for day_idx, day_label in enumerate(day_labels):
            lines.append(f"[{day_label}]")
            day_meals = per_day[day_idx]
            for meal_name in meal_order:
                if meal_name in day_meals and day_meals[meal_name]:
                    menu_str = " / ".join(day_meals[meal_name])
                    lines.append(f"  {meal_name}: {menu_str}")
            lines.append("")
        return "\n".join(lines).strip()

    def run(self):
        print("="*80); print("RepeatCrawler (ìµœì¢… ì™„ì„±ë³¸)"); print("="*80)
        
        # 1. [New] í•™ì‚¬ì¼ì •ì€ Seleniumìœ¼ë¡œ ìˆ˜ì§‘
        self.crawl_yearly_schedule([2025, 2026])
        
        # 2. ì‹ë‹¹ ë©”ë‰´ (ê¸°ì¡´)
        for url in self.target_urls:
            if 'restaurant' in url: self.crawl_restaurant_lists(url)
            import time; time.sleep(1)
            
        # 3. ê²Œì‹œíŒ (ê¸°ì¡´)
        for board in self.board_urls:
            custom_cutoff = None
            if "months_limit" in board:
                limit_date = datetime.now() - timedelta(days=board["months_limit"] * 30)
                custom_cutoff = limit_date.strftime("%Y-%m-%d")
            
            self.crawl_list_page(
                board['url'], 
                board.get('max_pages', 0), 
                board.get('skip_date_filter', False), 
                board['name'], 
                custom_cutoff=custom_cutoff,
                max_items=board.get('max_items', 0)
            )
            import time; time.sleep(1)
        
        if self.saved_pages:
            self.storage.save_index({"crawl_date": datetime.now().isoformat(), "pages": self.saved_pages, "meta": self.index_meta})

def main():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--enable-minio', action='store_true')
    parser.add_argument('--output-dir', type=str)
    args = parser.parse_args()
    out = Path(args.output_dir) if args.output_dir else None
    SimpleTestCrawler(args.enable_minio, out).run()

if __name__ == "__main__":
    main()