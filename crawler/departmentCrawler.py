#!/usr/bin/env python3
"""
departmentCrawler.py

í•™ê³¼ ì†Œê°œ / ë™ì•„ë¦¬ ì†Œê°œ / êµìœ¡ê³¼ì •(ì •ì  í˜ì´ì§€ ìœ„ì£¼) 1íšŒì„± í¬ë¡¤ëŸ¬
- ìì£¼ ë³€í•˜ì§€ ì•ŠëŠ” ì •ì  ì •ë³´ìš©
- [Update] Session/Retry, í™•ì¥ì ë³´ì •(.do), ì•„ì´ì½˜ í•„í„° ì ìš©
"""

import sys
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from pathlib import Path
from datetime import datetime
from bs4 import BeautifulSoup, Tag
from dotenv import load_dotenv
from typing import Optional
import logging
import hashlib
import urllib.parse
from ftfy import fix_text
import mimetypes # [New] í™•ì¥ì ë³´ì •ìš©

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(dotenv_path=Path(__file__).parent.parent / ".env")

# crawler ëª¨ë“ˆ ì„í¬íŠ¸
sys.path.insert(0, str(Path(__file__).parent))

from filters.content_extractor import ContentExtractor
from filters.quality_filter import QualityFilter
from storage.json_storage import JSONStorage
from storage.minio_storage import MinIOStorage

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

exclude_patterns = ["/cms/fileDownload.do"]

# ì•„ì´ì½˜ í•„í„° (ì†ë„ í–¥ìƒ)
ICON_IMAGE_KEYWORDS = [
    "/_res/ko/img/icon/", "/res/ko/img/common/", "logo", "btn", "btn-", 
    "bg_subvisual", "wa-mark", "bubble_tail", "btn_top_go", "icon",
    "insta", "youtube", "blog", "facebook", "twitter", "kakao", 
    "banner", "footer", "header", "arrow", "line", "bg_", "common"
]

class departmentCrawler:
    """í•™ê³¼/ë™ì•„ë¦¬/ì •ì  ì†Œê°œ í˜ì´ì§€ ì „ìš© í¬ë¡¤ëŸ¬"""

    def __init__(self, enable_minio: bool = False, output_dir: Optional[Path] = None):
        
        # [New] ì„¸ì…˜ ë° ì¬ì‹œë„ ì„¤ì •
        self.session = requests.Session()
        retries = Retry(total=5, backoff_factor=1, status_forcelist=[500, 502, 503, 504], allowed_methods=["GET"])
        adapter = HTTPAdapter(max_retries=retries)
        self.session.mount("https://", adapter)
        self.session.mount("http://", adapter)

        # MinIO ì„¤ì •
        self.enable_minio = enable_minio
        if enable_minio:
            try:
                self.minio = MinIOStorage.from_env()
                logger.info("âœ… MinIO ìŠ¤í† ë¦¬ì§€ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸  MinIO ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.enable_minio = False
                self.minio = None
        else:
            self.minio = None

        # âœ… í¬ë¡¤ë§ ëŒ€ìƒ URL ëª©ë¡ (ê¸°ì¡´ ë¦¬ìŠ¤íŠ¸ ìœ ì§€)
        self.department_static_urls = [
            # ì—ë””ìŠ¨ì¹¼ë¦¬ì§€ ì²¨ë‹¨ì‚°ì—…ìœµí•©í•™ë¶€
            {"url": "https://edison.kumoh.ac.kr/edison/sub0101.do", "name": "ì—ë””ìŠ¨ì¹¼ë¦¬ì§€ ì²¨ë‹¨ì‚°ì—…ìœµí•©í•™ë¶€ ì†Œê°œ"},
            {"url": "https://edison.kumoh.ac.kr/edison/sub0102.do", "name": "ì—ë””ìŠ¨ì¹¼ë¦¬ì§€ ì²¨ë‹¨ì‚°ì—…ìœµí•©í•™ë¶€ êµìœ¡ëª©í‘œ"},
            {"url": "https://edison.kumoh.ac.kr/edison/sub0104.do", "name": "ì—ë””ìŠ¨ì¹¼ë¦¬ì§€ ì²¨ë‹¨ì‚°ì—…ìœµí•©í•™ë¶€ ë¹„ì „"},
            # ê±´ì¶•í† ëª©í™˜ê²½ê³µí•™ë¶€
            {"url": "https://archi.kumoh.ac.kr/archi/sub0102.do", "name": "ê±´ì¶•í† ëª©í™˜ê²½ê³µí•™ë¶€ ì†Œê°œ"},
            {"url": "https://archi.kumoh.ac.kr/archi/sub0103.do", "name": "ê±´ì¶•í† ëª©í™˜ê²½ê³µí•™ë¶€ ê±´ì¶•í•™ì „ê³µ ì†Œê°œ"},
            {"url": "https://archi.kumoh.ac.kr/archi/sub0104.do", "name": "ê±´ì¶•í† ëª©í™˜ê²½ê³µí•™ë¶€ ê±´ì¶•ê³µí•™ì „ê³µ ì†Œê°œ"},
            {"url": "https://civil.kumoh.ac.kr/civil/sub0101.do", "name": "ê±´ì¶•í† ëª©í™˜ê²½ê³µí•™ë¶€ í† ëª©ê³µí•™ì „ê³µ ì†Œê°œ"},
            {"url": "https://env.kumoh.ac.kr/env/sub0101.do", "name": "ê±´ì¶•í† ëª©í™˜ê²½ê³µí•™ë¶€ í™˜ê²½ê³µí•™ì „ê³µ ì†Œê°œ"},
            {"url": "https://env.kumoh.ac.kr/env/sub0202_01.do", "name": "ê±´ì¶•í† ëª©í™˜ê²½ê³µí•™ë¶€ í™˜ê²½ê³µí•™ì „ê³µ ë™ì•„ë¦¬ ì§€êµ¬í™˜ê²½ì—°êµ¬íšŒ ì†Œê°œ"},
            {"url": "https://env.kumoh.ac.kr/env/sub0202_02.do", "name": "ê±´ì¶•í† ëª©í™˜ê²½ê³µí•™ë¶€ í™˜ê²½ê³µí•™ì „ê³µ ë™ì•„ë¦¬ ì•„ë¦„ë“œë¦¬ ì†Œê°œ"},
            {"url": "https://env.kumoh.ac.kr/env/sub0202_03.do", "name": "ê±´ì¶•í† ëª©í™˜ê²½ê³µí•™ë¶€ í™˜ê²½ê³µí•™ì „ê³µ ë™ì•„ë¦¬ ESC ì†Œê°œ"},
            {"url": "https://env.kumoh.ac.kr/env/sub0202_04.do", "name": "ê±´ì¶•í† ëª©í™˜ê²½ê³µí•™ë¶€ í™˜ê²½ê³µí•™ì „ê³µ ë™ì•„ë¦¬ BOD ì†Œê°œ"},
            # ê¸°ê³„ê³µí•™ë¶€
            {"url": "https://mecheng.kumoh.ac.kr/mecheng/sub0101.do", "name": "ê¸°ê³„ê³µí•™ë¶€ ê¸°ê³„ê³µí•™ì „ê³µ ì†Œê°œ"},
            {"url": "https://mx.kumoh.ac.kr/md/sub0101.do", "name": "ê¸°ê³„ê³µí•™ë¶€ ê¸°ê³„ì‹œìŠ¤í…œê³µí•™ì „ê³µ ì†Œê°œ"},
            {"url": "https://mobility.kumoh.ac.kr/smartmobility/sub0101.do", "name": "ê¸°ê³„ê³µí•™ë¶€ ìŠ¤ë§ˆíŠ¸ëª¨ë¹Œë¦¬í‹°ì „ê³µ ì¸ì‚¬ë§"},
            {"url": "https://mobility.kumoh.ac.kr/smartmobility/sub0102.do", "name": "ê¸°ê³„ê³µí•™ë¶€ ìŠ¤ë§ˆíŠ¸ëª¨ë¹Œë¦¬í‹°ì „ê³µ êµìœ¡ ëª©í‘œ"},
            {"url": "https://mobility.kumoh.ac.kr/smartmobility/sub0301.do", "name": "ê¸°ê³„ê³µí•™ë¶€ ìŠ¤ë§ˆíŠ¸ëª¨ë¹Œë¦¬í‹°ì „ê³µ ê³µë™í•™ê³¼ êµìœ¡ ê³¼ì •"},
            {"url": "https://mobility.kumoh.ac.kr/smartmobility/sub0304.do", "name": "ê¸°ê³„ê³µí•™ë¶€ ìŠ¤ë§ˆíŠ¸ëª¨ë¹Œë¦¬í‹°ì „ê³µ ì´ìˆ˜ì²´ê³„ë„"},
            # ì‚°ì—…ë¹…ë°ì´í„°ê³µí•™ë¶€
            {"url": "https://ie.kumoh.ac.kr/ie/sub0102.do", "name": "ì‚°ì—…ë¹…ë°ì´í„°ê³µí•™ë¶€ ì‚°ì—…ê³µí•™ì „ê³µ ì†Œê°œ"},
            {"url": "https://ie.kumoh.ac.kr/ie/sub0603.do", "name": "ì‚°ì—…ë¹…ë°ì´í„°ê³µí•™ë¶€ ì‚°ì—…ê³µí•™ì „ê³µ ë™ì•„ë¦¬/í•™ìƒíšŒ"},
            {"url": "https://www.kumoh.ac.kr/bigdata/sub0102.do", "name": "ì‚°ì—…ë¹…ë°ì´í„°ê³µí•™ë¶€ ìˆ˜ë¦¬ë¹…ë°ì´í„°ì „ê³µ ê°œìš” ë° ì—°í˜"},
            {"url": "https://www.kumoh.ac.kr/bigdata/sub0502.do", "name": "ì‚°ì—…ë¹…ë°ì´í„°ê³µí•™ë¶€ ìˆ˜ë¦¬ë¹…ë°ì´í„°ì „ê³µ ì „ê³µë™ì•„ë¦¬"},
            # ì¬ë£Œê³µí•™ë¶€
            {"url": "https://polymer.kumoh.ac.kr/polymer/sub0202.do", "name": "ì¬ë£Œê³µí•™ë¶€ ê³ ë¶„ìê³µí•™ì „ê³µ ì „ê³µì†Œê°œ"},
            {"url": "https://polymer.kumoh.ac.kr/polymer/sub0502.do", "name": "ì¬ë£Œê³µí•™ë¶€ ê³ ë¶„ìê³µí•™ì „ê³µ ë™ì•„ë¦¬"},
            {"url": "https://mse.kumoh.ac.kr/mse/sub0102.do", "name": "ì¬ë£Œê³µí•™ë¶€ ì‹ ì†Œì¬ê³µí•™ì „ê³µ ì „ê³µì†Œê°œ"},
            {"url": "https://mse.kumoh.ac.kr/mse/sub020102.do", "name": "ì¬ë£Œê³µí•™ë¶€ ì‹ ì†Œì¬ê³µí•™ì „ê³µ êµìœ¡ê³¼ì • í¸ì„±í‘œ"},
            {"url": "https://mse.kumoh.ac.kr/mse/sub0602.do", "name": "ì¬ë£Œê³µí•™ë¶€ ì‹ ì†Œì¬ê³µí•™ì „ê³µ ë™ì•„ë¦¬"},
            # ì „ìê³µí•™ë¶€
            {"url": "https://see.kumoh.ac.kr/see/sub0101.do", "name": "ì „ìê³µí•™ë¶€ ë°˜ë„ì²´ì‹œìŠ¤í…œì „ê³µ ì „ìì‹œìŠ¤í…œì „ê³µ ì†Œê°œ"},
            {"url": "https://see.kumoh.ac.kr/see/sub0501.do", "name": "ì „ìê³µí•™ë¶€ ë°˜ë„ì²´ì‹œìŠ¤í…œì „ê³µ ì „ìì‹œìŠ¤í…œì „ê³µ ë™ì•„ë¦¬"},
            # ì»´í“¨í„°ê³µí•™ë¶€ - ì†Œí”„íŠ¸ì›¨ì–´ì „ê³µ
            {"url": "https://cs.kumoh.ac.kr/cs/sub0101.do", "name": "ì»´í“¨í„°ê³µí•™ë¶€ ì†Œí”„íŠ¸ì›¨ì–´ì „ê³µ ì†Œê°œ"},
            {"url": "https://cs.kumoh.ac.kr/cs/sub0105_2.do", "name": "ì»´í“¨í„°ê³µí•™ë¶€ ì†Œí”„íŠ¸ì›¨ì–´ì „ê³µ êµìœ¡ê³¼ì •"},
            {"url": "https://cs.kumoh.ac.kr/cs/sub0504.do", "name": "ì»´í“¨í„°ê³µí•™ë¶€ ì†Œí”„íŠ¸ì›¨ì–´ì „ê³µ ë™ì•„ë¦¬"},
            # ì»´í“¨í„°ê³µí•™ë¶€ - ì¸ê³µì§€ëŠ¥ê³µí•™ì „ê³µ
            {"url": "https://ai.kumoh.ac.kr/ai/sub0102.do", "name": "ì»´í“¨í„°ê³µí•™ë¶€ ì¸ê³µì§€ëŠ¥ê³µí•™ì „ê³µ ê°œìš” ë° ì—°í˜"},
            {"url": "https://ai.kumoh.ac.kr/ai/sub0302.do", "name": "ì»´í“¨í„°ê³µí•™ë¶€ ì¸ê³µì§€ëŠ¥ê³µí•™ì „ê³µ êµìœ¡ê³¼ì •í‘œ"},
            {"url": "https://ai.kumoh.ac.kr/ai/sub0602.do", "name": "ì»´í“¨í„°ê³µí•™ë¶€ ì¸ê³µì§€ëŠ¥ê³µí•™ì „ê³µ ì „ê³µë™ì•„ë¦¬"},
            # ì»´í“¨í„°ê³µí•™ë¶€ - ì»´í“¨í„°ê³µí•™ì „ê³µ
            {"url": "https://ce.kumoh.ac.kr/ce/sub0102.do", "name": "ì»´í“¨í„°ê³µí•™ë¶€ ì»´í“¨í„°ê³µí•™ì „ê³µ ê°œìš” ë° ì—°í˜"},
            {"url": "https://ce.kumoh.ac.kr/ce/sub0205.do", "name": "ì»´í“¨í„°ê³µí•™ë¶€ ì»´í“¨í„°ê³µí•™ì „ê³µ ë™ì•„ë¦¬"},
            {"url": "https://ce.kumoh.ac.kr/ce/sub0301.do", "name": "ì»´í“¨í„°ê³µí•™ë¶€ ì»´í“¨í„°ê³µí•™ì „ê³µ êµê³¼ê³¼ì •"},
            # í™”í•™ì†Œì¬ê³µí•™ë¶€ - ì†Œì¬ë””ìì¸ê³µí•™ì „ê³µ
            {"url": "https://textile.kumoh.ac.kr/textile/sub0101.do", "name": "í™”í•™ì†Œì¬ê³µí•™ë¶€ ì†Œì¬ë””ìì¸ê³µí•™ì „ê³µ ì „ê³µì¥ ì¸ì‚¬ë§"},
            {"url": "https://textile.kumoh.ac.kr/textile/sub0203.do", "name": "í™”í•™ì†Œì¬ê³µí•™ë¶€ ì†Œì¬ë””ìì¸ê³µí•™ì „ê³µ êµìœ¡ê³¼ì •"},
            {"url": "https://textile.kumoh.ac.kr/textile/sub0501.do", "name": "í™”í•™ì†Œì¬ê³µí•™ë¶€ ì†Œì¬ë””ìì¸ê³µí•™ì „ê³µ ì „ê³µë™ì•„ë¦¬"},
            # í™”í•™ì†Œì¬ê³µí•™ë¶€ - í™”í•™ê³µí•™ì „ê³µ
            {"url": "https://che.kumoh.ac.kr/che/sub0102.do", "name": "í™”í•™ì†Œì¬ê³µí•™ë¶€ í™”í•™ê³µí•™ì „ê³µ í•™ê³¼ì†Œê°œ"},
            {"url": "https://che.kumoh.ac.kr/che/sub0502.do", "name": "í™”í•™ì†Œì¬ê³µí•™ë¶€ í™”í•™ê³µí•™ì „ê³µ ë™ì•„ë¦¬"},
            # í™”í•™ì†Œì¬ê³µí•™ë¶€ - í™”í•™ìƒëª…ì†Œì¬ì „ê³µ
            {"url": "https://chembio.kumoh.ac.kr/chembio/sub0102.do", "name": "í™”í•™ì†Œì¬ê³µí•™ë¶€ í™”í•™ìƒëª…ì†Œì¬ì „ê³µ ì „ê³µê°œìš”"},
            # ê´‘ì‹œìŠ¤í…œê³µí•™ê³¼
            {"url": "https://optics.kumoh.ac.kr/optics/sub0101.do", "name": "ê´‘ì‹œìŠ¤í…œê³µí•™ê³¼ í•™ê³¼ì†Œê°œ"},
            # ë°”ì´ì˜¤ë©”ë””ì»¬ê³µí•™ê³¼
            {"url": "https://medicalit.kumoh.ac.kr/medicalit/sub0101.do", "name": "ë°”ì´ì˜¤ë©”ë””ì»¬ê³µí•™ê³¼ í•™ê³¼ì†Œê°œ"},
            {"url": "https://medicalit.kumoh.ac.kr/medicalit/sub020102.do", "name": "ë°”ì´ì˜¤ë©”ë””ì»¬ê³µí•™ê³¼ êµê³¼ì†Œê°œ"},
            # ITìœµí•©í•™ê³¼
            {"url": "https://itc.kumoh.ac.kr/itc/sub0101.do", "name": "ITìœµí•©í•™ê³¼ í•™ê³¼ì†Œê°œ"},
            {"url": "https://itc.kumoh.ac.kr/itc/sub0103.do#accordion-menu-title", "name": "ITìœµí•©í•™ê³¼ êµê³¼ëª©ê°œìš”"},
            # ììœ¨ì „ê³µí•™ë¶€
            {"url": "https://sls.kumoh.ac.kr/sls/sub0101.do", "name": "ììœ¨ì „ê³µí•™ë¶€ ì†Œê°œ"},
            {"url": "https://sls.kumoh.ac.kr/sls/sub0301.do", "name": "ììœ¨ì „ê³µí•™ë¶€ êµê³¼ê³¼ì •"},
            {"url": "https://sls.kumoh.ac.kr/sls/sub0302.do", "name": "ììœ¨ì „ê³µí•™ë¶€ ì „ê³µì„ íƒ"},
            # ê²½ì˜í•™ê³¼
            {"url": "https://biz.kumoh.ac.kr/biz/sub0102.do", "name": "ê²½ì˜í•™ê³¼ ì†Œê°œ"},
            {"url": "https://biz.kumoh.ac.kr/biz/sub0702.do", "name": "ê²½ì˜í•™ê³¼ ë™ì•„ë¦¬"},
        ]

        self.department_board_urls = [
            {"url": "https://archi.kumoh.ac.kr/archi/sub0201.do", "name": "ê±´ì¶•í† ëª©í™˜ê²½ê³µí•™ë¶€ ê±´ì¶•í•™ì „ê³µ êµìœ¡ê³¼ì •"},
            {"url": "https://archi.kumoh.ac.kr/archi/sub0202.do", "name": "ê±´ì¶•í† ëª©í™˜ê²½ê³µí•™ë¶€ ê±´ì¶•ê³µí•™ì „ê³µ êµìœ¡ê³¼ì •"},
            {"url": "https://civil.kumoh.ac.kr/civil/sub030101.do", "name": "ê±´ì¶•í† ëª©í™˜ê²½ê³µí•™ë¶€ í† ëª©ê³µí•™ì „ê³µ êµìœ¡ê³¼ì •"},
            {"url": "https://ie.kumoh.ac.kr/ie/sub030101.do", "name": "ì‚°ì—…ë¹…ë°ì´í„°ê³µí•™ë¶€ ì‚°ì—…ê³µí•™ì „ê³µ êµìœ¡ê³¼ì •"},
            {"url": "https://www.kumoh.ac.kr/bigdata/sub030102.do", "name": "ì‚°ì—…ë¹…ë°ì´í„°ê³µí•™ë¶€ ìˆ˜ë¦¬ë¹…ë°ì´í„°ì „ê³µ êµìœ¡ê³¼ì •í‘œ"},
            {"url": "https://polymer.kumoh.ac.kr/polymer/sub0404.do", "name": "ì¬ë£Œê³µí•™ë¶€ ê³ ë¶„ìê³µí•™ì „ê³µ êµê³¼ê³¼ì •"},
            {"url": "https://che.kumoh.ac.kr/che/sub0304.do", "name": "í™”í•™ì†Œì¬ê³µí•™ë¶€ í™”í•™ê³µí•™ì „ê³µ êµê³¼ê³¼ì •"},
            {"url": "https://chembio.kumoh.ac.kr/chembio/sub030101.do", "name": "í™”í•™ì†Œì¬ê³µí•™ë¶€ í™”í•™ìƒëª…ì†Œì¬ì „ê³µ êµìœ¡ê³¼ì • ë° êµê³¼ëª© ê°œìš”"},
            {"url": "https://optics.kumoh.ac.kr/optics/sub020102.do", "name": "ê´‘ì‹œìŠ¤í…œê³µí•™ê³¼ í•™ë¶€êµìœ¡ê³¼ì •"},
            {"url": "https://biz.kumoh.ac.kr/biz/sub030101.do", "name": "ê²½ì˜í•™ê³¼ êµê³¼ê³¼ì •"},
        ]

        # í•„í„° ë° ì €ì¥ì†Œ ì´ˆê¸°í™”
        self.quality_filter = QualityFilter(
            min_text_length=50,
            max_text_length=500000,
            min_word_count=10
        )

        if output_dir is None:
            output_dir = Path(__file__).parent.parent / "data" / "raw" / "departments"

        self.output_dir = Path(output_dir)
        self.storage = JSONStorage(self.output_dir, pretty_print=True)

        self.content_extractor = ContentExtractor(
            keep_links=True,
            keep_images=False
        )

        self.stats = {"total": 0, "success": 0, "failed": 0, "filtered": 0, "skipped": 0, "attachments_found": 0, "attachments_uploaded": 0}
        self.saved_pages = []
        self.existing_urls = set()
        self._load_existing_index()

    def _load_existing_index(self):
        index_file = self.output_dir / "crawl_index.json"
        if index_file.exists():
            try:
                import json
                with open(index_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    for page in data.get('pages', []):
                        url = page.get('url')
                        if url:
                            self.existing_urls.add(url)
                            self.saved_pages.append(page)
                logger.info(f"ğŸ“‚ ê¸°ì¡´ first í¬ë¡¤ë§ ë°ì´í„° ë¡œë“œ: {len(self.existing_urls)}ê°œ URL")
            except Exception as e:
                logger.warning(f"ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")

    def _convert_tables_to_text(self, html_content: str) -> str:
        if not html_content: return ""
        soup = BeautifulSoup(html_content, "html.parser")
        tables = soup.find_all("table")
        if not tables: return html_content
        for table in tables:
            rows_text = []
            for tr in table.find_all("tr"):
                cells = [cell.get_text(strip=True) for cell in tr.find_all(["th", "td"])]
                if any(cells): rows_text.append(" | ".join(cells))
            if rows_text:
                new_div = soup.new_tag("div")
                new_div.string = f"\n[í‘œ ë°ì´í„° ì‹œì‘]\n" + "\n".join(rows_text) + "\n[í‘œ ë°ì´í„° ë]\n"
                table.replace_with(new_div)
        return str(soup)

    def crawl_url(self, url: str, page_info: dict) -> bool:
        self.stats["total"] += 1
        if url in self.existing_urls:
            logger.info(f"â­ï¸  ì´ë¯¸ í¬ë¡¤ë§ëœ URL - ê±´ë„ˆëœ€: {url}")
            self.stats["skipped"] += 1
            return False

        logger.info(f"í¬ë¡¤ë§ ì‹œì‘: {url}")

        try:
            headers = {'User-Agent': 'KITBot/2.0'}
            # [Update] session.get ì‚¬ìš©
            response = self.session.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            html = response.text

            is_quality, reason = self.quality_filter.is_high_quality(html, url)
            if not is_quality:
                logger.warning(f"í’ˆì§ˆ í•„í„° ì‹¤íŒ¨: {reason}")
                self.stats["filtered"] += 1
                return False

            html_with_tables = self._convert_tables_to_text(html)
            content_data = self.content_extractor.extract_with_metadata(html_with_tables)
            attachments = self._process_attachments(url, html)

            page_type = page_info.get("page_type", "static_intro")
            if page_type == "static_intro":
                board_name = content_data["title"] or page_info["name"]
                title = page_info["name"]
                display_title = title
            else:
                board_name = page_info.get("board_name") or page_info["name"]
                title = content_data["title"] or page_info["name"]
                display_title = title           

            author, view_count, created_at = None, None, None
            if "board_notice" in page_type or "latest" in page_info["name"]:
                try:
                    soup = BeautifulSoup(html, "html.parser")
                    el_author = soup.find(text="ì‘ì„±ì")
                    if el_author and el_author.parent: author = el_author.parent.find_next().get_text(strip=True)
                    el_view = soup.find(text="ì¡°íšŒ")
                    if el_view and el_view.parent:
                        view_count = el_view.parent.find_next().get_text(strip=True)
                        view_count = int(view_count) if view_count.isdigit() else None
                    el_date = soup.find(text="ì‘ì„±ì¼")
                    if el_date and el_date.parent:
                        created_raw = el_date.parent.find_next().get_text(strip=True)
                        created_at = created_raw.replace('.', '-').strip()
                        try: created_at = datetime.strptime(created_at, "%Y-%m-%d").isoformat()
                        except: created_at = None
                except: pass

            metadata = {
                "text_length": len(content_data['text']), "word_count": content_data['word_count'], "title": title,
                "board_name": board_name, "display_title": display_title, "paragraphs": content_data['paragraphs'],
                "link_count": len(content_data['links']), "attachments_count": len(attachments), "attachments": attachments,
                "images": content_data['images'], "quality_check": reason, "crawled_at": datetime.now().isoformat(),
                "source_url": url, "page_type": page_type, "name": page_info["name"],
                "author": author, "view_count": view_count, "created_at": created_at,
            }

            filepath = self.storage.save_page(url, html, metadata)
            self.saved_pages.append({"url": url, "file": filepath, "title": content_data['title'], "text_length": len(content_data['text']), "page_type": metadata["page_type"]})
            self.existing_urls.add(url)
            self.stats["success"] += 1
            logger.info(f"âœ… ì €ì¥ ì™„ë£Œ: {Path(filepath).name}")
            return True

        except requests.RequestException as e:
            logger.error(f"âŒ ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬: {e}")
            self.stats["failed"] += 1
            return False
        except Exception as e:
            logger.error(f"âŒ ì²˜ë¦¬ ì—ëŸ¬: {e}")
            self.stats["failed"] += 1
            return False

    def _process_attachments(self, page_url: str, html: str) -> list:
        attachments = []
        try:
            soup = BeautifulSoup(html, 'html.parser')
            for link in soup.find_all('a', href=True):
                href = link['href']
                link_text = link.get_text(strip=True)
                is_download = ('mode=download' in href or 'download' in href.lower() or any(href.lower().endswith(x) for x in ['.pdf','.hwp','.docx','.xlsx','.zip']))
                if any(p in href for p in exclude_patterns): is_download = False
                if not is_download: continue

                abs_url = urllib.parse.urljoin(page_url, href)
                self.stats["attachments_found"] += 1
                att_info = {"page_url": page_url, "link_text": link_text, "download_url": abs_url, "detected_at": datetime.now().isoformat()}

                if self.enable_minio and self.minio:
                    try:
                        headers = {'User-Agent': 'KITBot/2.0', 'Referer': page_url}
                        resp = self.session.get(abs_url, headers=headers, timeout=30)
                        resp.raise_for_status()
                        
                        content_type = resp.headers.get('Content-Type', '').split(';')[0].strip()
                        content_disp = resp.headers.get('Content-Disposition', '')
                        if 'filename=' in content_disp: filename = content_disp.split('filename=')[-1].strip('"\'')
                        else:
                            filename = abs_url.split('/')[-1].split('?')[0]
                            if not filename or '.' not in filename: filename = link_text if '.' in link_text else f"attachment_{hashlib.md5(abs_url.encode()).hexdigest()[:8]}.bin"
                        try: filename = urllib.parse.unquote(filename)
                        except: pass
                        filename = fix_text(filename)
                        
                        # [Update] í™•ì¥ì ë³´ì •
                        if filename.lower().endswith('.do') or '.' not in filename:
                            guessed_ext = mimetypes.guess_extension(content_type)
                            if guessed_ext:
                                if guessed_ext == '.jpe': guessed_ext = '.jpg'
                                filename = Path(filename).stem + guessed_ext

                        clean_name = filename.replace('/', '_').replace('\\', '_')
                        obj_name = f"attachments/{clean_name}"
                        
                        success, res = self.minio.upload_file(resp.content, obj_name, content_type, metadata={"source_url": abs_url, "original_filename": filename})
                        if success:
                            att_info.update({"minio_url": res, "minio_object": obj_name, "filename": clean_name, "status": "uploaded"})
                            self.stats["attachments_uploaded"] += 1
                            logger.info(f"   ğŸ“ ì²¨ë¶€íŒŒì¼ ì—…ë¡œë“œ: {clean_name}")
                        else: att_info.update({"status": "upload_failed", "error": res})
                    except Exception as e: att_info.update({"status": "download_failed", "error": str(e)})
                else: att_info["status"] = "metadata_only"
                attachments.append(att_info)

            image_exts = ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.bmp', '.svg']
            for img in soup.find_all('img', src=True):
                src, alt = img['src'], img.get('alt', '').strip()
                if any(k in src for k in ICON_IMAGE_KEYWORDS): continue
                
                src_no_query = src.split('?', 1)[0].lower()
                is_image = any(src_no_query.endswith(ext) for ext in image_exts)
                is_editor = 'editorimage.do' in src_no_query
                if not (is_image or is_editor): continue

                abs_url = urllib.parse.urljoin(page_url, src)
                self.stats["attachments_found"] += 1
                att_info = {"page_url": page_url, "link_text": alt, "download_url": abs_url, "type": "image"}
                
                if self.enable_minio and self.minio:
                    try:
                        resp = self.session.get(abs_url, headers={'User-Agent': 'KITBot/2.0'}, timeout=30)
                        content_type = resp.headers.get('Content-Type', '').split(';')[0].strip()
                        fname = abs_url.split('/')[-1].split('?')[0] or "image.jpg"
                        
                        if fname.lower().endswith('.do') or '.' not in fname:
                            guessed_ext = mimetypes.guess_extension(content_type)
                            if guessed_ext: fname = Path(fname).stem + guessed_ext
                            else: fname = Path(fname).stem + ".jpg"
                        
                        clean_name = fname.replace('/', '_')
                        obj_name = f"images/{clean_name}"
                        success, res = self.minio.upload_file(resp.content, obj_name, content_type, metadata={"source_url": abs_url, "alt_text": alt, "original_filename": fname})
                        if success:
                            att_info.update({"minio_url": res, "minio_object": obj_name, "filename": clean_name, "status": "uploaded"})
                            self.stats["attachments_uploaded"] += 1
                            logger.info(f"   ğŸ–¼ ì´ë¯¸ì§€ ì—…ë¡œë“œ: {clean_name}")
                    except Exception as e: att_info.update({"status": "download_failed", "error": str(e)})
                else: att_info["status"] = "metadata_only"
                attachments.append(att_info)
        except Exception as e: logger.error(f"âŒ ì²¨ë¶€íŒŒì¼ ì²˜ë¦¬ ì—ëŸ¬: {e}")
        return attachments

    def crawl_latest_from_department_board(self, board_info):
        url = board_info["url"]
        name = board_info["name"]
        logger.info(f"\nğŸ“˜ [êµìœ¡ê³¼ì •] {name}: {url}")
        try:
            headers = {'User-Agent': 'KITBot/2.0 (CSEcapstone)'}
            response = self.session.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            article_links = []
            for link in soup.find_all('a', href=True):
                href = link['href']
                if ('mode=view' in href) or ('articleNo' in href):
                    if href.startswith('/'):
                        site_root = url.split('/', 3)[:3]
                        base = "/".join(site_root)
                        full = base + href
                    elif href.startswith('?'): full = url.split('?')[0] + href
                    else: full = url.rsplit('/', 1)[0] + '/' + href
                    article_links.append(full)

            if not article_links:
                logger.warning(f"âŒ ê²Œì‹œê¸€ì„ ì°¾ì§€ ëª»í•¨: {url}")
                return False

            latest_url = article_links[0]
            logger.info(f"   ğŸ“Œ ìµœì‹  ê²Œì‹œê¸€: {latest_url}")
            if latest_url in self.existing_urls:
                logger.info(f"   â­ï¸ ìµœì‹  ê²Œì‹œê¸€ ì´ë¯¸ í¬ë¡¤ë§ë¨ â†’ ìŠ¤í‚µ")
                self.stats["skipped"] += 1
                return False

            page_info = {"url": latest_url, "name": f"{name} (ìµœì‹  ê²Œì‹œê¸€)", "page_type": "board_notice", "board_name": name}
            success = self.crawl_url(latest_url, page_info)
            if success: self.existing_urls.add(latest_url)
            return success
        except Exception as e:
            logger.error(f"âŒ êµìœ¡ê³¼ì • ê²Œì‹œíŒ ìµœì‹ ê¸€ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return False

    def run(self):
        print("=" * 80); print("departmentCrawler ì‹œì‘"); print("=" * 80)
        start_time = datetime.now()
        for page in self.department_static_urls:
            print(f"\nğŸ“ ëŒ€ìƒ ì‚¬ì´íŠ¸ ì´ë¦„ : [{page['name']}]")
            print("-" * 80)
            self.crawl_url(page['url'], page)
            import time; time.sleep(0.5)

        print("\n" + "=" * 80); print("ğŸ“˜ í•™ê³¼ë³„ êµìœ¡ê³¼ì • ê²Œì‹œíŒ ìµœì‹ ê¸€ í¬ë¡¤ë§"); print("=" * 80)
        for board in self.department_board_urls:
            print(f"\nğŸ“ ëŒ€ìƒ ê²Œì‹œíŒ ì´ë¦„ : [{board['name']}]")
            print("-" * 80)
            self.crawl_latest_from_department_board(board)
            import time; time.sleep(0.5)

        if self.saved_pages:
            index_data = {"crawl_date": datetime.now().isoformat(), "total_pages": len(self.saved_pages), "pages": self.saved_pages}
            self.storage.save_index(index_data)
        
        print("\n" + "=" * 80); print("departmentCrawler í¬ë¡¤ë§ ì™„ë£Œ!")

def main():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--enable-minio', action='store_true')
    parser.add_argument('--output-dir', type=str)
    args = parser.parse_args()
    out = Path(args.output_dir) if args.output_dir else None
    departmentCrawler(args.enable_minio, out).run()

if __name__ == "__main__":
    main()