#!/usr/bin/env python3
"""
ì‹¤í—˜ ê²°ê³¼ ë¹„êµ ìŠ¤í¬ë¦½íŠ¸ (ë…ë¦½ ì‹¤í–‰)
"""

import pandas as pd
import json
from pathlib import Path
from datetime import datetime

def compare_results():
    """ê²°ê³¼ ë¹„êµ ë° ì¶œë ¥"""
    print("\n" + "="*80)
    print("ğŸ“Š ìµœì¢… ê²°ê³¼ ë¹„êµ")
    print("="*80 + "\n")
    
    results = {}
    timing_results = {}
    
    # ê° ê²°ê³¼ íŒŒì¼ ë¡œë“œ
    files = {
        "ë² ì´ìŠ¤ë¼ì¸ (Boost)": "eval/evaluation_result.csv",
        "í•˜ì´ë¸Œë¦¬ë“œ (BM25+Semantic)": "eval/evaluation_result_hybrid.csv",
        "ë¦¬ë­ì»¤ (BGE-reranker)": "eval/evaluation_result_reranker.csv",
        "Full (Hybrid+Reranker)": "eval/evaluation_result_full.csv",
    }
    
    timing_files = {
        "ë² ì´ìŠ¤ë¼ì¸ (Boost)": "eval/timing_result.json",
        "í•˜ì´ë¸Œë¦¬ë“œ (BM25+Semantic)": "eval/timing_result_hybrid.json",
        "ë¦¬ë­ì»¤ (BGE-reranker)": "eval/timing_result_reranker.json",
        "Full (Hybrid+Reranker)": "eval/timing_result_full.json",
    }
    
    for name, filepath in files.items():
        path = Path(filepath)
        if path.exists():
            try:
                df = pd.read_csv(path)
                results[name] = {
                    'context_precision': df['context_precision'].mean(),
                    'context_recall': df['context_recall'].mean(),
                    'faithfulness': df['faithfulness'].mean(),
                    'answer_relevancy': df['answer_relevancy'].mean(),
                }
                print(f"âœ… {name}: ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ {name} ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        else:
            print(f"âŒ {name}: íŒŒì¼ ì—†ìŒ ({filepath})")
    
    # ì‘ë‹µ ì‹œê°„ ë°ì´í„° ë¡œë“œ
    for name, filepath in timing_files.items():
        path = Path(filepath)
        if path.exists():
            try:
                with open(path, "r", encoding="utf-8") as f:
                    timing_data = json.load(f)
                    timing_results[name] = timing_data
            except Exception as e:
                print(f"âš ï¸ {name} ì‘ë‹µ ì‹œê°„ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    if not results:
        print("âŒ ë¹„êµí•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print()
    
    # ê²°ê³¼ í…Œì´ë¸” ìƒì„±
    comparison_df = pd.DataFrame(results).T
    comparison_df = comparison_df.round(4)
    
    print(comparison_df.to_string())
    print("\n")
    
    # ì‘ë‹µ ì‹œê°„ í…Œì´ë¸” ìƒì„±
    if timing_results:
        print("\n" + "="*80)
        print("â±ï¸ ì‘ë‹µ ì‹œê°„ ë¹„êµ")
        print("="*80 + "\n")
        
        timing_df_data = {}
        for name, timing in timing_results.items():
            timing_df_data[name] = {
                'í‰ê·  (sec)': timing.get('avg_response_time', 0),
                'ì¤‘ì•™ê°’ (sec)': timing.get('median_response_time', 0),
                'ìµœì†Œ (sec)': timing.get('min_response_time', 0),
                'ìµœëŒ€ (sec)': timing.get('max_response_time', 0),
            }
        
        timing_df = pd.DataFrame(timing_df_data).T
        timing_df = timing_df.round(4)
        print(timing_df.to_string())
        print("\n")
    
    # ê°œì„ ìœ¨ ê³„ì‚° (ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„)
    if "ë² ì´ìŠ¤ë¼ì¸ (Boost)" in results:
        baseline = results["ë² ì´ìŠ¤ë¼ì¸ (Boost)"]
        print("ğŸ“ˆ ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„ ê°œì„ ìœ¨:\n")
        
        for name, scores in results.items():
            if name == "ë² ì´ìŠ¤ë¼ì¸ (Boost)":
                continue
            
            print(f"[{name}]")
            for metric in ['context_precision', 'context_recall', 'faithfulness', 'answer_relevancy']:
                baseline_score = baseline[metric]
                current_score = scores[metric]
                if baseline_score > 0:
                    improvement = ((current_score - baseline_score) / baseline_score) * 100
                    symbol = "ğŸ“ˆ" if improvement > 0 else "ğŸ“‰" if improvement < 0 else "â¡ï¸"
                    print(f"  {symbol} {metric}: {improvement:+.2f}%")
            print()
    
    # ê²°ê³¼ ì €ì¥
    save_path = Path("eval/comparison_result.csv")
    comparison_df.to_csv(save_path, encoding="utf-8-sig")
    print(f"âœ… ë¹„êµ ê²°ê³¼ ì €ì¥: {save_path}")
    
    # ì‘ë‹µ ì‹œê°„ ì €ì¥
    if timing_results:
        timing_save_path = Path("eval/comparison_timing.csv")
        timing_df.to_csv(timing_save_path, encoding="utf-8-sig")
        print(f"âœ… ì‘ë‹µ ì‹œê°„ ì €ì¥: {timing_save_path}")
    
    # ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±
    report_path = Path("eval/comparison_report.md")
    with open(report_path, "w", encoding="utf-8") as f:
        f.write(f"# RAG ê²€ìƒ‰ ë°©ë²• ë¹„êµ ì‹¤í—˜ ê²°ê³¼\n\n")
        f.write(f"**ì‹¤í—˜ ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write("## ğŸ“Š ì„±ëŠ¥ ë¹„êµ\n\n")
        f.write(comparison_df.to_markdown())
        f.write("\n\n")
        
        # ì‘ë‹µ ì‹œê°„ í…Œì´ë¸” ì¶”ê°€
        if timing_results:
            f.write("## â±ï¸ ì‘ë‹µ ì‹œê°„ ë¹„êµ\n\n")
            f.write(timing_df.to_markdown())
            f.write("\n\n")
        
        f.write("## ğŸ“ˆ ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„ ê°œì„ ìœ¨\n\n")
        
        if "ë² ì´ìŠ¤ë¼ì¸ (Boost)" in results:
            baseline = results["ë² ì´ìŠ¤ë¼ì¸ (Boost)"]
            for name, scores in results.items():
                if name == "ë² ì´ìŠ¤ë¼ì¸ (Boost)":
                    continue
                
                f.write(f"### {name}\n\n")
                for metric in ['context_precision', 'context_recall', 'faithfulness', 'answer_relevancy']:
                    baseline_score = baseline[metric]
                    current_score = scores[metric]
                    if baseline_score > 0:
                        improvement = ((current_score - baseline_score) / baseline_score) * 100
                        symbol = "ğŸ“ˆ" if improvement > 0 else "ğŸ“‰" if improvement < 0 else "â¡ï¸"
                        f.write(f"- {symbol} **{metric}**: {improvement:+.2f}%\n")
                f.write("\n")
    
    print(f"âœ… ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ì €ì¥: {report_path}\n")
    
    # ìµœê³  ì„±ëŠ¥ ë²„ì „ ì°¾ê¸°
    print("ğŸ† ìµœê³  ì„±ëŠ¥ ë²„ì „:")
    for metric in ['context_precision', 'context_recall', 'faithfulness', 'answer_relevancy']:
        best_name = max(results.items(), key=lambda x: x[1][metric])[0]
        best_score = results[best_name][metric]
        print(f"   {metric}: {best_name} ({best_score:.4f})")


if __name__ == "__main__":
    compare_results()
