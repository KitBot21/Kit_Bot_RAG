#!/usr/bin/env python3
"""
RAG ê²€ìƒ‰ ë°©ë²• ë¹„êµ ì‹¤í—˜ ìŠ¤í¬ë¦½íŠ¸

4ê°€ì§€ ë²„ì „ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ì—¬ ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤:
1. ë² ì´ìŠ¤ë¼ì¸ (ê¸°ì¡´ Boost ê¸°ë°˜ ê²€ìƒ‰)
2. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (BM25 + Semantic)
3. ë¦¬ë­ì»¤ (BGE-reranker-v2-m3)
4. Full (í•˜ì´ë¸Œë¦¬ë“œ + ë¦¬ë­ì»¤)
"""

import subprocess
import sys
import pandas as pd
from pathlib import Path
from datetime import datetime

def run_evaluation(script_name, version_name):
    """í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰"""
    print("\n" + "="*80)
    print(f"ğŸš€ [{version_name}] í‰ê°€ ì‹œì‘...")
    print("="*80)
    
    script_path = Path(__file__).parent / script_name
    
    try:
        result = subprocess.run(
            [sys.executable, str(script_path)],
            cwd=str(Path(__file__).parent.parent),
            capture_output=True,
            text=True,
            timeout=1800  # 30ë¶„ íƒ€ì„ì•„ì›ƒ
        )
        
        print(result.stdout)
        if result.stderr:
            print("STDERR:", result.stderr)
        
        if result.returncode == 0:
            print(f"âœ… [{version_name}] í‰ê°€ ì™„ë£Œ!")
            return True
        else:
            print(f"âŒ [{version_name}] í‰ê°€ ì‹¤íŒ¨ (exit code: {result.returncode})")
            return False
            
    except subprocess.TimeoutExpired:
        print(f"â° [{version_name}] íƒ€ì„ì•„ì›ƒ (30ë¶„ ì´ˆê³¼)")
        return False
    except Exception as e:
        print(f"âŒ [{version_name}] ì—ëŸ¬ ë°œìƒ: {e}")
        return False


def compare_results():
    """ê²°ê³¼ ë¹„êµ ë° ì¶œë ¥"""
    print("\n" + "="*80)
    print("ğŸ“Š ìµœì¢… ê²°ê³¼ ë¹„êµ")
    print("="*80 + "\n")
    
    results = {}
    
    # ê° ê²°ê³¼ íŒŒì¼ ë¡œë“œ
    files = {
        "ë² ì´ìŠ¤ë¼ì¸ (Boost)": "eval/evaluation_result.csv",
        "í•˜ì´ë¸Œë¦¬ë“œ (BM25+Semantic)": "eval/evaluation_result_hybrid.csv",
        "ë¦¬ë­ì»¤ (BGE-reranker)": "eval/evaluation_result_reranker.csv",
        "Full (Hybrid+Reranker)": "eval/evaluation_result_full.csv",
    }
    
    for name, filepath in files.items():
        path = Path(filepath)
        if path.exists():
            try:
                df = pd.read_csv(path)
                results[name] = {
                    'context_precision': df['context_precision'].mean(),
                    'context_recall': df['context_recall'].mean(),
                    'faithfulness': df['faithfulness'].mean(),
                    'answer_relevancy': df['answer_relevancy'].mean(),
                }
            except Exception as e:
                print(f"âš ï¸ {name} ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    if not results:
        print("âŒ ë¹„êµí•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ê²°ê³¼ í…Œì´ë¸” ìƒì„±
    comparison_df = pd.DataFrame(results).T
    comparison_df = comparison_df.round(4)
    
    print(comparison_df.to_string())
    print("\n")
    
    # ê°œì„ ìœ¨ ê³„ì‚° (ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„)
    if "ë² ì´ìŠ¤ë¼ì¸ (Boost)" in results:
        baseline = results["ë² ì´ìŠ¤ë¼ì¸ (Boost)"]
        print("ğŸ“ˆ ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„ ê°œì„ ìœ¨:\n")
        
        for name, scores in results.items():
            if name == "ë² ì´ìŠ¤ë¼ì¸ (Boost)":
                continue
            
            print(f"[{name}]")
            for metric in ['context_precision', 'context_recall', 'faithfulness', 'answer_relevancy']:
                baseline_score = baseline[metric]
                current_score = scores[metric]
                if baseline_score > 0:
                    improvement = ((current_score - baseline_score) / baseline_score) * 100
                    symbol = "ğŸ“ˆ" if improvement > 0 else "ğŸ“‰" if improvement < 0 else "â¡ï¸"
                    print(f"  {symbol} {metric}: {improvement:+.2f}%")
            print()
    
    # ê²°ê³¼ ì €ì¥
    save_path = Path("eval/comparison_result.csv")
    comparison_df.to_csv(save_path, encoding="utf-8-sig")
    print(f"âœ… ë¹„êµ ê²°ê³¼ ì €ì¥: {save_path}")
    
    # ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±
    report_path = Path("eval/comparison_report.md")
    with open(report_path, "w", encoding="utf-8") as f:
        f.write(f"# RAG ê²€ìƒ‰ ë°©ë²• ë¹„êµ ì‹¤í—˜ ê²°ê³¼\n\n")
        f.write(f"**ì‹¤í—˜ ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write("## ğŸ“Š ì„±ëŠ¥ ë¹„êµ\n\n")
        f.write(comparison_df.to_markdown())
        f.write("\n\n## ğŸ“ˆ ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„ ê°œì„ ìœ¨\n\n")
        
        if "ë² ì´ìŠ¤ë¼ì¸ (Boost)" in results:
            baseline = results["ë² ì´ìŠ¤ë¼ì¸ (Boost)"]
            for name, scores in results.items():
                if name == "ë² ì´ìŠ¤ë¼ì¸ (Boost)":
                    continue
                
                f.write(f"### {name}\n\n")
                for metric in ['context_precision', 'context_recall', 'faithfulness', 'answer_relevancy']:
                    baseline_score = baseline[metric]
                    current_score = scores[metric]
                    if baseline_score > 0:
                        improvement = ((current_score - baseline_score) / baseline_score) * 100
                        symbol = "ğŸ“ˆ" if improvement > 0 else "ğŸ“‰" if improvement < 0 else "â¡ï¸"
                        f.write(f"- {symbol} **{metric}**: {improvement:+.2f}%\n")
                f.write("\n")
    
    print(f"âœ… ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")


def main():
    print("ğŸ¯ RAG ê²€ìƒ‰ ë°©ë²• ë¹„êµ ì‹¤í—˜ ì‹œì‘")
    print("=" * 80)
    print("ì‹¤í—˜ ë²„ì „:")
    print("  1. ë² ì´ìŠ¤ë¼ì¸ (Boost ê¸°ë°˜)")
    print("  2. í•˜ì´ë¸Œë¦¬ë“œ (BM25 + Semantic)")
    print("  3. ë¦¬ë­ì»¤ (BGE-reranker-v2-m3)")
    print("  4. Full (í•˜ì´ë¸Œë¦¬ë“œ + ë¦¬ë­ì»¤)")
    print("=" * 80)
    
    # ë² ì´ìŠ¤ë¼ì¸ì€ ì´ë¯¸ ì‹¤í–‰ë˜ì—ˆë‹¤ê³  ê°€ì • (evaluation_result.csv ì¡´ì¬)
    baseline_exists = Path("eval/evaluation_result.csv").exists()
    
    if not baseline_exists:
        print("\nâš ï¸ ë² ì´ìŠ¤ë¼ì¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì‹¤í–‰í•©ë‹ˆë‹¤...")
        run_evaluation("evaluate_rag.py", "ë² ì´ìŠ¤ë¼ì¸ (Boost)")
    else:
        print("\nâœ… ë² ì´ìŠ¤ë¼ì¸ ê²°ê³¼ ì¡´ì¬ (ê±´ë„ˆë›°ê¸°)")
    
    # ë‚˜ë¨¸ì§€ 3ê°œ ë²„ì „ ì‹¤í–‰
    experiments = [
        ("evaluate_rag_hybrid.py", "í•˜ì´ë¸Œë¦¬ë“œ (BM25+Semantic)"),
        ("evaluate_rag_reranker.py", "ë¦¬ë­ì»¤ (BGE-reranker)"),
        ("evaluate_rag_full.py", "Full (Hybrid+Reranker)"),
    ]
    
    success_count = 0
    for script, name in experiments:
        if run_evaluation(script, name):
            success_count += 1
    
    print("\n" + "="*80)
    print(f"ğŸ‰ ì‹¤í—˜ ì™„ë£Œ! ({success_count}/{len(experiments)}ê°œ ì„±ê³µ)")
    print("="*80)
    
    # ê²°ê³¼ ë¹„êµ
    compare_results()
    
    print("\nâœ¨ ëª¨ë“  ì‹¤í—˜ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")


if __name__ == "__main__":
    main()
