import json
import sys
import os
import time
from pathlib import Path
from datasets import Dataset
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
)

from dotenv import load_dotenv
load_dotenv()

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€ (core ëª¨ë“ˆ importë¥¼ ìœ„í•´)
sys.path.append(str(Path(__file__).parent.parent))

# ì±—ë´‡ í•¨ìˆ˜ import
from core.rag_core import rag_with_sources 

def run_evaluation():
    # 1. í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ
    data_path = Path("eval/golden_dataset.json")
    if not data_path.exists():
        print("âŒ í‰ê°€ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤: eval/golden_dataset.json")
        return

    with open(data_path, "r", encoding="utf-8") as f:
        test_data = json.load(f)

    print(f"ğŸ“Š ì´ {len(test_data)}ê°œì˜ ì§ˆë¬¸ì— ëŒ€í•´ í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

    # 2. ì±—ë´‡ì—ê²Œ ì§ˆë¬¸í•˜ê³  ê²°ê³¼ ìˆ˜ì§‘
    questions = []
    ground_truths = []
    answers = []
    contexts = []
    response_times = []  # ğŸ”´ ì‘ë‹µ ì‹œê°„ ì¶”ê°€

    for idx, item in enumerate(test_data):
        q = item["question"]
        gt = item["ground_truth"]
        
        print(f"   [{idx+1}/{len(test_data)}] ì§ˆë¬¸: {q}")
        
        # --- RAG í˜¸ì¶œ (ì‹œê°„ ì¸¡ì •) ---
        start_time = time.time()
        try:
            # rag_core í•¨ìˆ˜ í˜¸ì¶œ (ë‹µë³€, ì†ŒìŠ¤, ì¼ì •ì •ë³´)
            answer_text, sources, schedule_info = rag_with_sources(q)
            elapsed_time = time.time() - start_time
                
        except Exception as e:
            elapsed_time = time.time() - start_time
            print(f"      âŒ ì—ëŸ¬ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            answer_text = "ì—ëŸ¬ ë°œìƒ"
            sources = []

        print(f"      â±ï¸ ì‘ë‹µ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
        response_times.append(elapsed_time)

        # ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©ë§Œ ë¦¬ìŠ¤íŠ¸ë¡œ ì¶”ì¶œ
        retrieved_docs = []
        for s in sources:
            try:
                if isinstance(s, dict):
                    # dict í˜•ì‹: {"title": ..., "url": ..., "text": ...}
                    text = s.get("text", "")
                elif hasattr(s, 'payload'):
                    # qdrant Point ê°ì²´
                    text = s.payload.get("text", "")
                else:
                    text = str(s)
                
                if text and text.strip():  # ë¹ˆ ë¬¸ìì—´ ì œì™¸
                    retrieved_docs.append(text)
            except Exception as e:
                print(f"      âš ï¸ ì†ŒìŠ¤ íŒŒì‹± ì—ëŸ¬: {e}")
                continue
        
        # contextsê°€ ë¹„ì–´ìˆìœ¼ë©´ ë”ë¯¸ í…ìŠ¤íŠ¸ ì¶”ê°€ (Ragas ì˜¤ë¥˜ ë°©ì§€)
        if not retrieved_docs:
            retrieved_docs = ["ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."]
            print(f"      âš ï¸ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (ë”ë¯¸ í…ìŠ¤íŠ¸ ì¶”ê°€)")

        questions.append(q)
        
        # ğŸ”´ [ìˆ˜ì •] ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ˆë¼ ë¬¸ìì—´ ê·¸ëŒ€ë¡œ ì¶”ê°€ (Ragas ìµœì‹  ê·œê²©)
        ground_truths.append(gt) 
        
        answers.append(answer_text)
        contexts.append(retrieved_docs)

    # 3. Ragas í‰ê°€ ë°ì´í„°ì…‹ ìƒì„±
    eval_dict = {
        "question": questions,
        "answer": answers,
        "contexts": contexts,
        "ground_truth": ground_truths
    }
    hf_dataset = Dataset.from_dict(eval_dict)

    # 4. í‰ê°€ ì‹¤í–‰ (OpenAI API ì‚¬ìš©)
    evaluator_llm = ChatOpenAI(model="gpt-4o") 
    evaluator_embeddings = OpenAIEmbeddings()

    print("\nâš–ï¸  AI ì‹¬íŒì´ ì±„ì ì„ ì‹œì‘í•©ë‹ˆë‹¤... (OpenAI ë¹„ìš© ë°œìƒ)")

    try:
        results = evaluate(
            hf_dataset,
            metrics=[
                context_precision,
                context_recall,
                faithfulness,
                answer_relevancy,
            ],
            # ğŸ”´ [ì¶”ê°€] ì—¬ê¸°ì— ëª…ì‹œì ìœ¼ë¡œ ëª¨ë¸ì„ ë„£ì–´ì¤ë‹ˆë‹¤.
            llm=evaluator_llm,
            embeddings=evaluator_embeddings,
        )

        # 5. ê²°ê³¼ ì¶œë ¥ ë° ì €ì¥
        print("\n" + "="*40)
        print("ğŸ† ìµœì¢… í‰ê°€ ì ìˆ˜")
        print("="*40)
        print(results)
        
        # ğŸ”´ ì‘ë‹µ ì‹œê°„ í†µê³„ ì¶”ê°€
        import numpy as np
        avg_time = np.mean(response_times)
        median_time = np.median(response_times)
        min_time = np.min(response_times)
        max_time = np.max(response_times)
        
        print("\n" + "="*40)
        print("â±ï¸  ì‘ë‹µ ì‹œê°„ í†µê³„")
        print("="*40)
        print(f"í‰ê· : {avg_time:.2f}ì´ˆ")
        print(f"ì¤‘ì•™ê°’: {median_time:.2f}ì´ˆ")
        print(f"ìµœì†Œ: {min_time:.2f}ì´ˆ")
        print(f"ìµœëŒ€: {max_time:.2f}ì´ˆ")
        
        df = results.to_pandas()
        # ì‘ë‹µ ì‹œê°„ ì»¬ëŸ¼ ì¶”ê°€
        df['response_time'] = response_times
        
        save_path = "eval/evaluation_result.csv"
        df.to_csv(save_path, index=False, encoding="utf-8-sig")
        print(f"\nâœ… ìƒì„¸ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {save_path}")
        
        # ì‘ë‹µ ì‹œê°„ í†µê³„ë¥¼ ë³„ë„ íŒŒì¼ë¡œ ì €ì¥
        timing_stats = {
            "version": "ë² ì´ìŠ¤ë¼ì¸ (Boost)",
            "avg_response_time": avg_time,
            "median_response_time": median_time,
            "min_response_time": min_time,
            "max_response_time": max_time,
            "total_queries": len(response_times)
        }
        
        timing_path = Path("eval/timing_result.json")
        with open(timing_path, "w", encoding="utf-8") as f:
            json.dump(timing_stats, f, indent=2, ensure_ascii=False)
        print(f"âœ… ì‘ë‹µ ì‹œê°„ í†µê³„ ì €ì¥: {timing_path}")
        
    except Exception as e:
        print(f"\nâŒ í‰ê°€ ì‹¤í–‰ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        print("   -> OpenAI API Keyê°€ ì˜¬ë°”ë¥¸ì§€, Ragas ë²„ì „ì´ ìµœì‹ ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    run_evaluation()