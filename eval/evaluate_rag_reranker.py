import json
import sys
import os
import time
from pathlib import Path
from datasets import Dataset
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
)

from dotenv import load_dotenv
load_dotenv()

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))

# ğŸ”´ ë¦¬ë­ì»¤ ë²„ì „ import
from core.rag_core_reranker import rag_with_sources 

def run_evaluation():
    data_path = Path("eval/golden_dataset.json")
    if not data_path.exists():
        print("âŒ í‰ê°€ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤: eval/golden_dataset.json")
        return

    with open(data_path, "r", encoding="utf-8") as f:
        test_data = json.load(f)

    print(f"ğŸ“Š [ë¦¬ë­ì»¤] ì´ {len(test_data)}ê°œì˜ ì§ˆë¬¸ì— ëŒ€í•´ í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

    questions = []
    ground_truths = []
    answers = []
    contexts = []
    response_times = []

    for idx, item in enumerate(test_data):
        q = item["question"]
        gt = item["ground_truth"]
        
        print(f"   [{idx+1}/{len(test_data)}] ì§ˆë¬¸: {q}")
        
        start_time = time.time()
        try:
            answer_text, sources, schedule_info = rag_with_sources(q)
            elapsed_time = time.time() - start_time
        except Exception as e:
            elapsed_time = time.time() - start_time
            print(f"      âŒ ì—ëŸ¬ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            answer_text = "ì—ëŸ¬ ë°œìƒ"
            sources = []

        print(f"      â±ï¸ ì‘ë‹µ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
        response_times.append(elapsed_time)

        retrieved_docs = []
        for s in sources:
            try:
                if isinstance(s, dict):
                    text = s.get("text", "")
                elif hasattr(s, 'payload'):
                    text = s.payload.get("text", "")
                else:
                    text = str(s)
                
                if text and text.strip():
                    retrieved_docs.append(text)
            except Exception as e:
                print(f"      âš ï¸ ì†ŒìŠ¤ íŒŒì‹± ì—ëŸ¬: {e}")
                continue
        
        if not retrieved_docs:
            retrieved_docs = ["ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."]
            print(f"      âš ï¸ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (ë”ë¯¸ í…ìŠ¤íŠ¸ ì¶”ê°€)")

        questions.append(q)
        ground_truths.append(gt) 
        answers.append(answer_text)
        contexts.append(retrieved_docs)

    eval_dict = {
        "question": questions,
        "answer": answers,
        "contexts": contexts,
        "ground_truth": ground_truths
    }
    hf_dataset = Dataset.from_dict(eval_dict)

    evaluator_llm = ChatOpenAI(model="gpt-4o") 
    evaluator_embeddings = OpenAIEmbeddings()

    print("\nâš–ï¸  AI ì‹¬íŒì´ ì±„ì ì„ ì‹œì‘í•©ë‹ˆë‹¤... (OpenAI ë¹„ìš© ë°œìƒ)")

    try:
        results = evaluate(
            hf_dataset,
            metrics=[
                context_precision,
                context_recall,
                faithfulness,
                answer_relevancy,
            ],
            llm=evaluator_llm,
            embeddings=evaluator_embeddings,
        )

        print("\n" + "="*40)
        print("ğŸ† [ë¦¬ë­ì»¤] ìµœì¢… í‰ê°€ ì ìˆ˜")
        print("="*40)
        print(results)
        
        import numpy as np
        avg_time = np.mean(response_times)
        median_time = np.median(response_times)
        min_time = np.min(response_times)
        max_time = np.max(response_times)
        
        print("\n" + "="*40)
        print("â±ï¸  ì‘ë‹µ ì‹œê°„ í†µê³„")
        print("="*40)
        print(f"í‰ê· : {avg_time:.2f}ì´ˆ")
        print(f"ì¤‘ì•™ê°’: {median_time:.2f}ì´ˆ")
        print(f"ìµœì†Œ: {min_time:.2f}ì´ˆ")
        print(f"ìµœëŒ€: {max_time:.2f}ì´ˆ")
        
        df = results.to_pandas()
        df['response_time'] = response_times
        save_path = "eval/evaluation_result_reranker.csv"
        df.to_csv(save_path, index=False, encoding="utf-8-sig")
        print(f"\nâœ… ìƒì„¸ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {save_path}")
        
        timing_stats = {
            "version": "ë¦¬ë­ì»¤ (BGE-reranker)",
            "avg_response_time": avg_time,
            "median_response_time": median_time,
            "min_response_time": min_time,
            "max_response_time": max_time,
            "total_queries": len(response_times)
        }
        
        timing_path = Path("eval/timing_result_reranker.json")
        with open(timing_path, "w", encoding="utf-8") as f:
            json.dump(timing_stats, f, indent=2, ensure_ascii=False)
        print(f"âœ… ì‘ë‹µ ì‹œê°„ í†µê³„ ì €ì¥: {timing_path}")
        
    except Exception as e:
        print(f"\nâŒ í‰ê°€ ì‹¤í–‰ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    run_evaluation()
