#!/usr/bin/env python3
"""
ë¦¬ë­í‚¹ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

1ë‹¨ê³„: BGE-M3ë¡œ Top-K ê²€ìƒ‰ (ë¹ ë¦„)
2ë‹¨ê³„: Cross-Encoderë¡œ ì¬ì •ë ¬ (ì •í™•)
"""

import pandas as pd
import numpy as np
from pathlib import Path
from sentence_transformers import SentenceTransformer, CrossEncoder
from qdrant_client import QdrantClient
import warnings
warnings.filterwarnings('ignore')

PROJECT_ROOT = Path(__file__).resolve().parents[1]
DATA_DIR = PROJECT_ROOT / "data"

COLLECTION_NAME = "kit_corpus_bge_all"
QDRANT_URL = "http://localhost:6333"

# ë¦¬ë­í‚¹ ëª¨ë¸ ì˜µì…˜
RERANKER_MODELS = {
    # BGE ë¦¬ë­ì»¤ (BGE-M3ì™€ ê°™ì€ ì œì‘ì‚¬, ë‹¤êµ­ì–´ ì§€ì›)
    'bge-reranker': 'BAAI/bge-reranker-base',  # BGE ë¦¬ë­ì»¤ (ë‹¤êµ­ì–´) â­ ì¶”ì²œ!
    'bge-reranker-large': 'BAAI/bge-reranker-large',  # ë” ì •í™• (ëŠë¦¼)
    
    # MS MARCO (ë‹¤êµ­ì–´)
    'mmarco-multi': 'cross-encoder/mmarco-mMiniLMv2-L12-H384-v1',  # ë‹¤êµ­ì–´ íŠ¹í™”
    'mmarco-korean': 'cross-encoder/ms-marco-MiniLM-L-6-v2',  # ì˜ì–´ ê¸°ë°˜
    
    # MS MARCO (ì˜ì–´ ê¸°ë°˜, ì°¸ê³ ìš©)
    'ms-marco-mini': 'cross-encoder/ms-marco-MiniLM-L-6-v2',  # ë¹ ë¦„
    'ms-marco-base': 'cross-encoder/ms-marco-MiniLM-L-12-v2',  # ê· í˜•
}

def load_ground_truth():
    """Ground Truth ë¡œë“œ"""
    gt_path = DATA_DIR / "ground_truth_100.csv"
    gt_df = pd.read_csv(gt_path)
    
    # rank > 0ì¸ ê²ƒë§Œ (ì •ë‹µ ìˆëŠ” ê²ƒ)
    gt_valid = gt_df[gt_df['rank'] > 0].copy()
    
    print(f"ğŸ“‹ Ground Truth: {len(gt_valid)}ê°œ")
    return gt_valid

def initial_search(client, model, query, top_k=20):
    """1ë‹¨ê³„: Bi-Encoderë¡œ ì´ˆê¸° ê²€ìƒ‰"""
    query_vector = model.encode(query, normalize_embeddings=True).tolist()
    
    results = client.search(
        collection_name=COLLECTION_NAME,
        query_vector=query_vector,
        limit=top_k
    )
    
    return results

def rerank_results(reranker, query, results, top_n=5):
    """2ë‹¨ê³„: Cross-Encoderë¡œ ì¬ì •ë ¬"""
    # ì¿¼ë¦¬-ë¬¸ì„œ ìŒ ìƒì„±
    pairs = []
    for hit in results:
        text = hit.payload.get('text', '')
        pairs.append([query, text])
    
    # ì¬ì ìˆ˜ ê³„ì‚°
    scores = reranker.predict(pairs)
    
    # ì ìˆ˜ë¡œ ì¬ì •ë ¬
    ranked_indices = np.argsort(scores)[::-1][:top_n]
    
    # ì¬ì •ë ¬ëœ ê²°ê³¼
    reranked = [results[i] for i in ranked_indices]
    reranked_scores = [scores[i] for i in ranked_indices]
    
    return reranked, reranked_scores

def evaluate_with_reranking(reranker_name='ms-marco-mini', initial_k=20, final_k=5):
    """ë¦¬ë­í‚¹ í¬í•¨ í‰ê°€"""
    print("\n" + "=" * 80)
    print(f"ğŸ”„ ë¦¬ë­í‚¹ í‰ê°€: {reranker_name}")
    print("=" * 80)
    print(f"   1ë‹¨ê³„: BGE-M3 â†’ Top-{initial_k}")
    print(f"   2ë‹¨ê³„: {reranker_name} â†’ Top-{final_k}")
    
    # ëª¨ë¸ ë¡œë“œ
    print("\nğŸ“¦ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    bi_encoder = SentenceTransformer('BAAI/bge-m3')
    reranker = CrossEncoder(RERANKER_MODELS[reranker_name])
    
    # Qdrant í´ë¼ì´ì–¸íŠ¸
    client = QdrantClient(url=QDRANT_URL)
    
    # Ground Truth ë¡œë“œ
    gt_df = load_ground_truth()
    
    # Corpus ë¡œë“œ (document_name ë§¤í•‘ìš©)
    corpus = pd.read_csv(DATA_DIR / "corpus_all.csv")
    
    # Corpusì— base_document_name ì»¬ëŸ¼ ì¶”ê°€ (chunk ì œê±°)
    def get_base_doc_name(doc_name):
        """chunk ì ‘ë¯¸ì‚¬ ì œê±°í•˜ê³  í™•ì¥ìë„ ì œê±°"""
        if not isinstance(doc_name, str):
            return ""
        # _chunkN ì œê±°
        base = doc_name.rsplit('_chunk', 1)[0] if '_chunk' in doc_name else doc_name
        # í™•ì¥ì ì œê±°
        base = base.replace('.pdf', '').replace('.xlsx', '').replace('.docx', '').strip()
        return base
    
    corpus['base_doc_name'] = corpus['document_name'].apply(get_base_doc_name)
    
    # document_name â†’ corpus_index ë§¤í•‘ (Qdrant ê²€ìƒ‰ ê²°ê³¼ ë§¤í•‘ìš©)
    doc_name_to_idx = {}
    for idx, row in corpus.iterrows():
        # document_nameì´ ìˆìœ¼ë©´ ì‚¬ìš© (ì²¨ë¶€íŒŒì¼)
        if isinstance(row.get('document_name'), str) and row['document_name']:
            doc_name_to_idx[row['document_name']] = idx
        # document_nameì´ ì—†ìœ¼ë©´ title ì‚¬ìš© (í¬ë¡¤ë§ ë°ì´í„°)
        elif isinstance(row.get('title'), str) and row['title']:
            doc_name_to_idx[row['title']] = idx
    
    # í‰ê°€
    recall_at_1_baseline = []
    recall_at_5_baseline = []
    recall_at_1_reranked = []
    recall_at_5_reranked = []
    mrr_baseline = []
    mrr_reranked = []
    
    evaluated = 0
    
    for _, row in gt_df.iterrows():
        query = row['query']
        gt_doc_name = row['document_name']
        
        if not isinstance(query, str) or not isinstance(gt_doc_name, str):
            continue
        
        # GT ë¬¸ì„œëª… ì •ê·œí™” (í™•ì¥ì ì œê±°)
        gt_base = gt_doc_name.replace('.pdf', '').replace('.xlsx', '').replace('.docx', '').strip()
        
        # GTì— í•´ë‹¹í•˜ëŠ” corpus ì¸ë±ìŠ¤ ì°¾ê¸°
        # 1. base_doc_nameìœ¼ë¡œ ë§¤ì¹­ (ì²¨ë¶€íŒŒì¼)
        gt_indices = set(corpus[corpus['base_doc_name'] == gt_base].index.tolist())
        
        # 2. titleë¡œ ë§¤ì¹­ (í¬ë¡¤ë§ ë°ì´í„°)
        if not gt_indices:
            gt_indices = set(corpus[corpus['title'] == gt_doc_name].index.tolist())
        
        if not gt_indices:
            # ë””ë²„ê¹…: ë§¤ì¹­ ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ì¶œë ¥
            # print(f"âš ï¸ ë§¤ì¹­ ì‹¤íŒ¨: {query[:40]}...")
            # print(f"   GT: '{gt_doc_name}'")
            # print(f"   GT base: '{gt_base}'")
            continue
        
        # 1ë‹¨ê³„: ì´ˆê¸° ê²€ìƒ‰ (Top-K)
        initial_results = initial_search(client, bi_encoder, query, top_k=initial_k)
        
        # Baseline í‰ê°€ (Top-5)
        baseline_top5 = initial_results[:final_k]
        baseline_indices = []
        for hit in baseline_top5:
            # document_name ë˜ëŠ” titleë¡œ ë§¤ì¹­
            doc_name = hit.payload.get('document_name', '')
            if doc_name and doc_name in doc_name_to_idx:
                baseline_indices.append(doc_name_to_idx[doc_name])
            else:
                # titleë¡œ ì‹œë„
                title = hit.payload.get('title', '')
                if title and title in doc_name_to_idx:
                    baseline_indices.append(doc_name_to_idx[title])
        
        # Baseline Recall
        found_in_baseline = any(idx in gt_indices for idx in baseline_indices[:1])
        recall_at_1_baseline.append(1.0 if found_in_baseline else 0.0)
        
        found_in_baseline_5 = any(idx in gt_indices for idx in baseline_indices[:final_k])
        recall_at_5_baseline.append(1.0 if found_in_baseline_5 else 0.0)
        
        # Baseline MRR
        rank = 0
        for i, idx in enumerate(baseline_indices[:final_k], 1):
            if idx in gt_indices:
                rank = i
                break
        mrr_baseline.append(1.0 / rank if rank > 0 else 0.0)
        
        # 2ë‹¨ê³„: ë¦¬ë­í‚¹
        reranked_results, reranked_scores = rerank_results(reranker, query, initial_results, top_n=final_k)
        
        # ë¦¬ë­í‚¹ í›„ í‰ê°€
        reranked_indices = []
        for hit in reranked_results:
            # document_name ë˜ëŠ” titleë¡œ ë§¤ì¹­
            doc_name = hit.payload.get('document_name', '')
            if doc_name and doc_name in doc_name_to_idx:
                reranked_indices.append(doc_name_to_idx[doc_name])
            else:
                # titleë¡œ ì‹œë„
                title = hit.payload.get('title', '')
                if title and title in doc_name_to_idx:
                    reranked_indices.append(doc_name_to_idx[title])
        
        # Reranked Recall
        found_in_reranked = any(idx in gt_indices for idx in reranked_indices[:1])
        recall_at_1_reranked.append(1.0 if found_in_reranked else 0.0)
        
        found_in_reranked_5 = any(idx in gt_indices for idx in reranked_indices[:final_k])
        recall_at_5_reranked.append(1.0 if found_in_reranked_5 else 0.0)
        
        # Reranked MRR
        rank = 0
        for i, idx in enumerate(reranked_indices[:final_k], 1):
            if idx in gt_indices:
                rank = i
                break
        mrr_reranked.append(1.0 / rank if rank > 0 else 0.0)
        
        evaluated += 1
        
        # ì²« 3ê°œ ì¿¼ë¦¬ëŠ” ìƒì„¸ ë””ë²„ê¹…
        if evaluated <= 3:
            print(f"\nğŸ” ë””ë²„ê¹… #{evaluated}: {query[:50]}...")
            print(f"   GT ë¬¸ì„œ: '{gt_doc_name}' (base: '{gt_base}')")
            print(f"   GT ì¸ë±ìŠ¤: {list(gt_indices)[:3]}...")
            print(f"   Baseline Top-5 ì¸ë±ìŠ¤: {baseline_indices}")
            print(f"   Reranked Top-5 ì¸ë±ìŠ¤: {reranked_indices}")
            print(f"   Baseline Hit: {any(idx in gt_indices for idx in baseline_indices)}")
            print(f"   Reranked Hit: {any(idx in gt_indices for idx in reranked_indices)}")
        
        if evaluated % 10 == 0:
            print(f"   ì§„í–‰: {evaluated}/{len(gt_df)}...")
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 80)
    print("ğŸ“Š ê²°ê³¼")
    print("=" * 80)
    
    print(f"\ní‰ê°€ ì¿¼ë¦¬: {evaluated}ê°œ")
    
    print("\nğŸ”¹ Baseline (BGE-M3ë§Œ)")
    print(f"   Recall@1: {np.mean(recall_at_1_baseline):.2%}")
    print(f"   Recall@5: {np.mean(recall_at_5_baseline):.2%}")
    print(f"   MRR: {np.mean(mrr_baseline):.4f}")
    
    print(f"\nğŸ”¸ Reranked (BGE-M3 + {reranker_name})")
    print(f"   Recall@1: {np.mean(recall_at_1_reranked):.2%}")
    print(f"   Recall@5: {np.mean(recall_at_5_reranked):.2%}")
    print(f"   MRR: {np.mean(mrr_reranked):.4f}")
    
    print("\nğŸ“ˆ ê°œì„ ë„")
    r1_improve = np.mean(recall_at_1_reranked) - np.mean(recall_at_1_baseline)
    r5_improve = np.mean(recall_at_5_reranked) - np.mean(recall_at_5_baseline)
    mrr_improve = np.mean(mrr_reranked) - np.mean(mrr_baseline)
    
    print(f"   Recall@1: {r1_improve:+.2%}")
    print(f"   Recall@5: {r5_improve:+.2%}")
    print(f"   MRR: {mrr_improve:+.4f}")
    
    return {
        'baseline_r1': np.mean(recall_at_1_baseline),
        'baseline_r5': np.mean(recall_at_5_baseline),
        'baseline_mrr': np.mean(mrr_baseline),
        'reranked_r1': np.mean(recall_at_1_reranked),
        'reranked_r5': np.mean(recall_at_5_reranked),
        'reranked_mrr': np.mean(mrr_reranked),
    }

def main():
    print("=" * 80)
    print("ğŸ”„ ë¦¬ë­í‚¹ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    print("\në¦¬ë­í‚¹ ëª¨ë¸ ì˜µì…˜:")
    print("\nğŸ”¥ BGE ë¦¬ë­ì»¤ (ì¶”ì²œ! BGE-M3ì™€ ê°™ì€ ì œì‘ì‚¬):")
    print("   1. bge-reranker â­ - BGE-M3ì™€ í˜¸í™˜ì„± ìµœê³ , ë‹¤êµ­ì–´ ì§€ì›")
    print("   2. bge-reranker-large - ë” ì •í™• (ëŠë¦¼)")
    
    print("\nğŸ‡°ğŸ‡· MS MARCO ë‹¤êµ­ì–´:")
    print("   3. mmarco-multi - ë‹¤êµ­ì–´ íŠ¹í™”")
    print("   4. mmarco-korean - ì˜ì–´ ê¸°ë°˜")
    
    print("\nğŸ‡ºğŸ‡¸ MS MARCO ì˜ì–´ (ì°¸ê³ ):")
    print("   5. ms-marco-mini - ë¹ ë¦„")
    print("   6. ms-marco-base - ë” ì •í™•")
    
    print("\nì¶”ì²œ: bge-reranker (BGE-M3ì™€ ìµœê³  í˜¸í™˜)")
    
    choice = input("\nì„ íƒ (1-6 ë˜ëŠ” ëª¨ë¸ëª…) [Enter=1]: ").strip() or '1'
    
    # ìˆ«ì ì„ íƒ ì²˜ë¦¬
    model_map = {
        '1': 'bge-reranker',
        '2': 'bge-reranker-large',
        '3': 'mmarco-multi',
        '4': 'mmarco-korean',
        '5': 'ms-marco-mini',
        '6': 'ms-marco-base',
    }
    
    if choice in model_map:
        choice = model_map[choice]
    
    if choice not in RERANKER_MODELS:
        print(f"âŒ ì˜ëª»ëœ ì„ íƒ. bge-reranker ì‚¬ìš©")
        choice = 'bge-reranker'
    
    # í‰ê°€ ì‹¤í–‰
    results = evaluate_with_reranking(
        reranker_name=choice,
        initial_k=20,  # 1ë‹¨ê³„: Top-20
        final_k=5      # 2ë‹¨ê³„: Top-5
    )
    
    print("\n" + "=" * 80)
    print("âœ… í‰ê°€ ì™„ë£Œ!")
    print("=" * 80)

if __name__ == "__main__":
    main()
