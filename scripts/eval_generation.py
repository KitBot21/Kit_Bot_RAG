#!/usr/bin/env python3
"""
RAG ë‹µë³€ í’ˆì§ˆ í‰ê°€
- Retrieval ì„±ëŠ¥: Top-1, Top-5 ì •í™•ë„
- Generation í’ˆì§ˆ: LLM í‰ê°€ ê¸°ë°˜ (ì •í™•ì„±, ê´€ë ¨ì„±, ì™„ì„±ë„)
"""
import pandas as pd
from pathlib import Path
from dotenv import load_dotenv
import json
from rag_demo import RAGSystem
from openai import OpenAI
import os

load_dotenv()

PROJECT_ROOT = Path(__file__).resolve().parents[1]  # scriptsì˜ ìƒìœ„ í´ë”

class RAGEvaluator:
    def __init__(self, rag_system, evaluator_model='gpt-4o-mini'):
        """
        RAG í‰ê°€ ì‹œìŠ¤í…œ
        
        Args:
            rag_system: RAGSystem ì¸ìŠ¤í„´ìŠ¤
            evaluator_model: í‰ê°€ìš© LLM ëª¨ë¸
        """
        self.rag = rag_system
        self.evaluator = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        self.evaluator_model = evaluator_model
    
    def evaluate_retrieval(self, ground_truth_path='data/ground_truth.csv'):
        """
        Retrieval ì„±ëŠ¥ í‰ê°€ (ê¸°ì¡´ ë°©ì‹)
        
        Returns:
            dict: Top-1, Top-5 ì •í™•ë„, MRR
        """
        gt_df = pd.read_csv(PROJECT_ROOT / ground_truth_path)
        queries = gt_df['query'].tolist()
        correct_ids = gt_df['chunk_id'].tolist()
        
        top1_correct = 0
        top5_correct = 0
        mrr_sum = 0
        
        print("ğŸ” Retrieval ì„±ëŠ¥ í‰ê°€ ì¤‘...")
        
        for i, (query, correct_id) in enumerate(zip(queries, correct_ids)):
            contexts = self.rag.retrieve(query, top_k=5)
            retrieved_ids = [ctx['chunk_id'] for ctx in contexts]
            
            if retrieved_ids[0] == correct_id:
                top1_correct += 1
                top5_correct += 1
                mrr_sum += 1.0
            elif correct_id in retrieved_ids:
                top5_correct += 1
                rank = retrieved_ids.index(correct_id) + 1
                mrr_sum += 1.0 / rank
            
            if (i + 1) % 20 == 0:
                print(f"  Progress: {i+1}/{len(queries)}")
        
        return {
            'top1_accuracy': top1_correct / len(queries),
            'top5_accuracy': top5_correct / len(queries),
            'mrr': mrr_sum / len(queries),
            'total_queries': len(queries)
        }
    
    def evaluate_answer_quality(self, query, answer, contexts, reference_answer=None):
        """
        LLMì„ ì‚¬ìš©í•˜ì—¬ ìƒì„±ëœ ë‹µë³€ì˜ í’ˆì§ˆ í‰ê°€
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            answer: LLM ìƒì„± ë‹µë³€
            contexts: ê²€ìƒ‰ëœ ë¬¸ì„œë“¤
            reference_answer: ì°¸ì¡° ë‹µë³€ (ìˆëŠ” ê²½ìš°)
            
        Returns:
            dict: í‰ê°€ ì ìˆ˜ (ì •í™•ì„±, ê´€ë ¨ì„±, ì™„ì„±ë„)
        """
        context_str = "\n\n".join([f"[ë¬¸ì„œ {i+1}]\n{ctx['text']}" for i, ctx in enumerate(contexts)])
        
        eval_prompt = f"""ë‹¤ìŒ RAG ì‹œìŠ¤í…œì˜ ë‹µë³€ì„ í‰ê°€í•´ì£¼ì„¸ìš”.

<ì§ˆë¬¸>
{query}
</ì§ˆë¬¸>

<ê²€ìƒ‰ëœ ë¬¸ì„œ>
{context_str}
</ê²€ìƒ‰ëœ ë¬¸ì„œ>

<ìƒì„±ëœ ë‹µë³€>
{answer}
</ìƒì„±ëœ ë‹µë³€>

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ 1-5ì  ì²™ë„ë¡œ í‰ê°€í•˜ê³  JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:

1. **ì •í™•ì„± (Accuracy)**: ë‹µë³€ì´ ë¬¸ì„œì˜ ì •ë³´ë¥¼ ì •í™•í•˜ê²Œ ë°˜ì˜í•˜ëŠ”ê°€?
   - 5ì : ì™„ë²½íˆ ì •í™•
   - 3ì : ëŒ€ì²´ë¡œ ì •í™•í•˜ë‚˜ ì¼ë¶€ ì˜¤ë¥˜
   - 1ì : ë¶€ì •í™•í•˜ê±°ë‚˜ ì˜ëª»ëœ ì •ë³´

2. **ê´€ë ¨ì„± (Relevance)**: ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆëŠ”ê°€?
   - 5ì : ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ë‹µë³€
   - 3ì : ê´€ë ¨ì€ ìˆìœ¼ë‚˜ ë¶€ë¶„ì ìœ¼ë¡œë§Œ ë‹µë³€
   - 1ì : ì§ˆë¬¸ê³¼ ë¬´ê´€

3. **ì™„ì„±ë„ (Completeness)**: ë‹µë³€ì´ ì¶©ë¶„íˆ ìƒì„¸í•œê°€?
   - 5ì : í•„ìš”í•œ ëª¨ë“  ì •ë³´ í¬í•¨
   - 3ì : ê¸°ë³¸ ì •ë³´ëŠ” ìˆìœ¼ë‚˜ ë¶€ì¡±
   - 1ì : ë¶ˆì™„ì „í•˜ê±°ë‚˜ ë„ˆë¬´ ê°„ëµ

4. **ê·¼ê±°ì„± (Groundedness)**: ë‹µë³€ì´ ì œê³µëœ ë¬¸ì„œì—ë§Œ ê¸°ë°˜í•˜ëŠ”ê°€?
   - 5ì : ëª¨ë“  ë‚´ìš©ì´ ë¬¸ì„œì— ê¸°ë°˜
   - 3ì : ëŒ€ë¶€ë¶„ ë¬¸ì„œ ê¸°ë°˜ì´ë‚˜ ì¼ë¶€ ì¶”ë¡ 
   - 1ì : ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš© í¬í•¨

JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”:
{{
  "accuracy": <1-5>,
  "relevance": <1-5>,
  "completeness": <1-5>,
  "groundedness": <1-5>,
  "reasoning": "<í‰ê°€ ì´ìœ >"
}}"""
        
        response = self.evaluator.chat.completions.create(
            model=self.evaluator_model,
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ RAG ì‹œìŠ¤í…œ ë‹µë³€ì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": eval_prompt}
            ],
            temperature=0.3,
            response_format={"type": "json_object"}
        )
        
        return json.loads(response.choices[0].message.content)
    
    def evaluate_all(self, sample_size=10):
        """
        ì „ì²´ í‰ê°€: Retrieval + Generation
        
        Args:
            sample_size: í‰ê°€í•  ìƒ˜í”Œ ìˆ˜ (Noneì´ë©´ ì „ì²´)
        """
        print("\n" + "="*80)
        print("ğŸ“Š RAG ì‹œìŠ¤í…œ ì „ì²´ í‰ê°€")
        print("="*80)
        
        # 1. Retrieval í‰ê°€
        print("\n[1ë‹¨ê³„] Retrieval ì„±ëŠ¥ í‰ê°€")
        print("-"*80)
        retrieval_metrics = self.evaluate_retrieval()
        
        print(f"\nâœ… Retrieval ê²°ê³¼:")
        print(f"  Top-1 ì •í™•ë„: {retrieval_metrics['top1_accuracy']:.1%}")
        print(f"  Top-5 ì •í™•ë„: {retrieval_metrics['top5_accuracy']:.1%}")
        print(f"  MRR: {retrieval_metrics['mrr']:.3f}")
        
        # 2. Generation í’ˆì§ˆ í‰ê°€
        print(f"\n[2ë‹¨ê³„] Generation í’ˆì§ˆ í‰ê°€ (ìƒ˜í”Œ {sample_size}ê°œ)")
        print("-"*80)
        
        gt_df = pd.read_csv(PROJECT_ROOT / 'data/ground_truth.csv')
        if sample_size:
            gt_df = gt_df.sample(n=min(sample_size, len(gt_df)), random_state=42)
        
        all_scores = []
        
        for i, row in gt_df.iterrows():
            query = row['query']
            print(f"\n{'='*80}")
            print(f"[{i+1}/{len(gt_df)}] ì§ˆë¬¸: {query}")
            print(f"{'='*80}")
            
            # RAG ì‹¤í–‰
            result = self.rag.query(query, top_k=5, verbose=False)
            
            # ë‹µë³€ ì¶œë ¥
            print(f"\nğŸ’¬ ìƒì„±ëœ ë‹µë³€:")
            print(f"{result['answer']}")
            
            # ê²€ìƒ‰ëœ ë¬¸ì„œ ì¶œë ¥
            print(f"\nğŸ“š ê²€ìƒ‰ëœ ë¬¸ì„œ (Top-5):")
            for j, ctx in enumerate(result['contexts']):
                print(f"  [{j+1}] {ctx['chunk_id'][:40]}... (ìœ ì‚¬ë„: {ctx['score']:.3f})")
            
            # ë‹µë³€ í’ˆì§ˆ í‰ê°€
            scores = self.evaluate_answer_quality(
                query, 
                result['answer'], 
                result['contexts']
            )
            
            all_scores.append(scores)
            
            print(f"\nğŸ“Š í‰ê°€ ì ìˆ˜:")
            print(f"  ì •í™•ì„±: {scores['accuracy']}/5")
            print(f"  ê´€ë ¨ì„±: {scores['relevance']}/5")
            print(f"  ì™„ì„±ë„: {scores['completeness']}/5")
            print(f"  ê·¼ê±°ì„±: {scores['groundedness']}/5")
            print(f"  ì´ìœ : {scores['reasoning'][:100]}...")

        
        # ì§‘ê³„
        avg_scores = {
            'accuracy': sum(s['accuracy'] for s in all_scores) / len(all_scores),
            'relevance': sum(s['relevance'] for s in all_scores) / len(all_scores),
            'completeness': sum(s['completeness'] for s in all_scores) / len(all_scores),
            'groundedness': sum(s['groundedness'] for s in all_scores) / len(all_scores),
        }
        
        # ìµœì¢… ê²°ê³¼
        print("\n" + "="*80)
        print("ğŸ“Š ìµœì¢… í‰ê°€ ê²°ê³¼")
        print("="*80)
        
        print(f"\nğŸ” Retrieval ì„±ëŠ¥:")
        print(f"  Top-1 ì •í™•ë„: {retrieval_metrics['top1_accuracy']:.1%}")
        print(f"  Top-5 ì •í™•ë„: {retrieval_metrics['top5_accuracy']:.1%}")
        print(f"  MRR: {retrieval_metrics['mrr']:.3f}")
        
        print(f"\nğŸ’¬ Generation í’ˆì§ˆ (í‰ê· ):")
        print(f"  ì •í™•ì„±: {avg_scores['accuracy']:.2f}/5.0")
        print(f"  ê´€ë ¨ì„±: {avg_scores['relevance']:.2f}/5.0")
        print(f"  ì™„ì„±ë„: {avg_scores['completeness']:.2f}/5.0")
        print(f"  ê·¼ê±°ì„±: {avg_scores['groundedness']:.2f}/5.0")
        print(f"  Overall: {sum(avg_scores.values())/4:.2f}/5.0")
        
        print("="*80 + "\n")
        
        return {
            'retrieval': retrieval_metrics,
            'generation': avg_scores,
            'overall_score': sum(avg_scores.values()) / 4
        }

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='RAG ì‹œìŠ¤í…œ í‰ê°€')
    parser.add_argument('--provider', default='openai', choices=['openai', 'ollama'])
    parser.add_argument('--model', default='gpt-4o-mini')
    parser.add_argument('--sample-size', type=int, default=10,
                        help='Generation í‰ê°€ ìƒ˜í”Œ ìˆ˜ (0ì´ë©´ ì „ì²´)')
    args = parser.parse_args()
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    print("ğŸš€ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
    rag = RAGSystem(
        llm_provider=args.provider,
        llm_model=args.model
    )
    
    # í‰ê°€ê¸° ì´ˆê¸°í™”
    evaluator = RAGEvaluator(rag)
    
    # ì „ì²´ í‰ê°€ ì‹¤í–‰
    sample_size = None if args.sample_size == 0 else args.sample_size
    results = evaluator.evaluate_all(sample_size=sample_size)

if __name__ == "__main__":
    main()
