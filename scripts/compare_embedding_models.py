#!/usr/bin/env python3
"""
ì„ë² ë”© ëª¨ë¸ ë¹„êµ í‰ê°€ ë„êµ¬

ì—¬ëŸ¬ ì„ë² ë”© ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ì—¬ ìµœì ì˜ ëª¨ë¸ì„ ì„ íƒí•©ë‹ˆë‹¤.

í‰ê°€ ì§€í‘œ:
1. ê²€ìƒ‰ ì„±ëŠ¥ (Recall@K, MRR)
2. ì„ë² ë”© ì†ë„
3. ë²¡í„° í¬ê¸° (ë©”ëª¨ë¦¬)
4. ì„ë² ë”© í’ˆì§ˆ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë¶„í¬)
"""

import numpy as np
import pandas as pd
import time
from pathlib import Path
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.http import models as qm
import warnings
warnings.filterwarnings('ignore')

PROJECT_ROOT = Path(__file__).resolve().parents[1]
DATA_DIR = PROJECT_ROOT / "data"

# í‰ê°€í•  ëª¨ë¸ ëª©ë¡
MODELS = {
    'bge-m3': {
        'name': 'BAAI/bge-m3',
        'dim': 1024,
        'description': 'BGE-M3 (ë‹¤êµ­ì–´, ë²”ìš©)'
    },
    'e5-base': {
        'name': 'intfloat/multilingual-e5-base',
        'dim': 768,
        'description': 'E5-Base (ë‹¤êµ­ì–´, ê· í˜•)'
    },
    'kr-sbert': {
        'name': 'snunlp/KR-SBERT-V40K-klueNLI-augSTS',
        'dim': 768,
        'description': 'KR-SBERT (í•œêµ­ì–´ íŠ¹í™”)'
    },
    'kosimcse': {
        'name': 'BM-K/KoSimCSE-roberta',
        'dim': 768,
        'description': 'KoSimCSE (í•œêµ­ì–´ íŠ¹í™”)'
    },
}

def load_test_data(sample_size=None):
    """í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ"""
    print(f"\nğŸ“‚ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ")
    
    # Corpus ë¡œë“œ (ì „ì²´)
    corpus_path = DATA_DIR / "corpus_all.csv"
    df = pd.read_csv(corpus_path)
    df = df[df['text'].notna()].reset_index(drop=True)
    
    # document_nameì´ ì—†ìœ¼ë©´ ìƒì„±
    if 'document_name' not in df.columns:
        df['document_name'] = df['title'].fillna('Unknown')
    
    texts = df['text'].astype(str).tolist()
    
    # ì¿¼ë¦¬ + Ground Truth ë¡œë“œ
    # 100ê°œ GTê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ Manual ì‚¬ìš©
    if (DATA_DIR / "ground_truth_100.csv").exists():
        queries_path = DATA_DIR / "queries_100.txt"
        gt_path = DATA_DIR / "ground_truth_100.csv"
        print(f"   âœ… 100ê°œ ìˆ˜ë™ GT ì‚¬ìš©")
    else:
        queries_path = DATA_DIR / "queries_manual.txt"
        gt_path = DATA_DIR / "ground_truth_manual.csv"
        print(f"   âš ï¸ Manual GT ì‚¬ìš© (ì°¸ê³ ìš©)")
    
    with queries_path.open('r', encoding='utf-8') as f:
        queries = [line.strip() for line in f if line.strip()]
    
    # Ground Truth ë¡œë“œ
    gt_df = pd.read_csv(gt_path)
    
    # Query â†’ Document Name ë§¤í•‘ (rank > 0ì¸ ê²ƒë§Œ, NaN ì œì™¸)
    query_to_doc = {}
    for _, row in gt_df.iterrows():
        # rankê°€ -1ì´ë©´ ì •ë‹µ ì—†ìŒ (ìŠ¤í‚µ)
        if 'rank' in row and row['rank'] <= 0:
            continue
        
        query = row['query']
        doc_name = row['document_name']
        
        # NaNì´ë‚˜ float íƒ€ì… ì œì™¸
        if not isinstance(query, str) or not isinstance(doc_name, str):
            continue
        
        query_to_doc[query] = doc_name
    
    print(f"   ë¬¸ì„œ: {len(texts):,}ê°œ (ì „ì²´ ì½”í¼ìŠ¤)")
    print(f"   ì¿¼ë¦¬: {len(queries)}ê°œ")
    print(f"   Ground Truth: {len(query_to_doc)}ê°œ ë§¤í•‘")
    
    return texts, queries, df, query_to_doc

def evaluate_embedding_speed(model, texts, batch_size=32):
    """ì„ë² ë”© ìƒì„± ì†ë„ ì¸¡ì •"""
    print(f"\nâ±ï¸  ì„ë² ë”© ì†ë„ ì¸¡ì •...")
    
    start = time.time()
    embeddings = model.encode(
        texts,
        batch_size=batch_size,
        show_progress_bar=False,
        normalize_embeddings=True
    )
    elapsed = time.time() - start
    
    speed = len(texts) / elapsed
    
    print(f"   ì´ ì‹œê°„: {elapsed:.2f}ì´ˆ")
    print(f"   ì†ë„: {speed:.1f} docs/sec")
    
    return embeddings, elapsed, speed

def evaluate_embedding_quality(embeddings):
    """ì„ë² ë”© í’ˆì§ˆ ì¸¡ì •"""
    print(f"\nğŸ“Š ì„ë² ë”© í’ˆì§ˆ ë¶„ì„...")
    
    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë¶„í¬ ë¶„ì„
    similarities = []
    sample_size = min(100, len(embeddings))
    
    for i in range(sample_size):
        for j in range(i+1, sample_size):
            sim = np.dot(embeddings[i], embeddings[j])
            similarities.append(sim)
    
    similarities = np.array(similarities)
    
    stats = {
        'mean': float(np.mean(similarities)),
        'std': float(np.std(similarities)),
        'min': float(np.min(similarities)),
        'max': float(np.max(similarities)),
        'median': float(np.median(similarities))
    }
    
    print(f"   í‰ê·  ìœ ì‚¬ë„: {stats['mean']:.4f}")
    print(f"   í‘œì¤€í¸ì°¨: {stats['std']:.4f}")
    print(f"   ë²”ìœ„: [{stats['min']:.4f}, {stats['max']:.4f}]")
    
    return stats

def evaluate_retrieval_performance(model, texts, queries, df, query_to_doc):
    """ê²€ìƒ‰ ì„±ëŠ¥ ì¸¡ì • (Ground Truth ê¸°ë°˜)"""
    print(f"\nğŸ” ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€...")
    
    # ë¬¸ì„œ ì„ë² ë”©
    print(f"   ë¬¸ì„œ ì„ë² ë”© ì¤‘... ({len(texts):,}ê°œ)")
    doc_embeddings = model.encode(
        texts,
        batch_size=32,
        show_progress_bar=True,
        normalize_embeddings=True
    )
    
    # ì¿¼ë¦¬ ì„ë² ë”©
    print(f"   ì¿¼ë¦¬ ì„ë² ë”© ì¤‘... ({len(queries)}ê°œ)")
    query_embeddings = model.encode(
        queries,
        batch_size=32,
        show_progress_bar=False,
        normalize_embeddings=True
    )
    
    # Document name â†’ indices ë§¤í•‘
    # ì²­í¬ ë‹¨ìœ„ë¡œ ì €ì¥ë˜ì–´ ìˆìœ¼ë¯€ë¡œ (ì˜ˆ: "ë²„ìŠ¤.pdf_chunk0")
    # ì›ë³¸ ë¬¸ì„œëª…ìœ¼ë¡œ ê·¸ë£¹í™”
    doc_name_to_indices = {}
    for idx, row in df.iterrows():
        doc_name = row['document_name']
        # NaNì´ë‚˜ float íƒ€ì… ê±´ë„ˆë›°ê¸°
        if not isinstance(doc_name, str):
            continue
        if doc_name not in doc_name_to_indices:
            doc_name_to_indices[doc_name] = []
        doc_name_to_indices[doc_name].append(idx)
    
    # ê° ì¿¼ë¦¬ì— ëŒ€í•´ ê²€ìƒ‰
    recall_at_1 = []
    recall_at_5 = []
    mrr_scores = []
    ndcg_scores = []
    
    evaluated_queries = 0
    
    for q_idx, query in enumerate(queries):
        # Ground Truth í™•ì¸
        if query not in query_to_doc:
            continue
        
        gt_doc_name = query_to_doc[query]
        
        # GT ë¬¸ì„œì˜ ì¸ë±ìŠ¤ë“¤ ì°¾ê¸° (ë¶€ë¶„ ë§¤ì¹­!)
        # GT: "2023-2í•™ê¸° ë²„ìŠ¤.pdf" â†’ Corpus: "2023-2í•™ê¸° ë²„ìŠ¤.pdf_chunk0"
        gt_indices = set()
        for doc_name, indices in doc_name_to_indices.items():
            # NaNì´ë‚˜ float íƒ€ì… ê±´ë„ˆë›°ê¸°
            if not isinstance(doc_name, str):
                continue
            if not isinstance(gt_doc_name, str):
                continue
            
            # ë¶€ë¶„ ë§¤ì¹­: GTê°€ doc_nameì— í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ OK
            if gt_doc_name in doc_name or doc_name.startswith(gt_doc_name.replace('.pdf', '')):
                gt_indices.update(indices)
        
        if not gt_indices:
            # ë§¤ì¹­ë˜ëŠ” ì²­í¬ê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
            continue
        
        # ì¿¼ë¦¬ ì„ë² ë”©ìœ¼ë¡œ ê²€ìƒ‰
        q_emb = query_embeddings[q_idx]
        similarities = np.dot(doc_embeddings, q_emb)
        
        # Top-K ì¸ë±ìŠ¤
        top_k_indices = np.argsort(similarities)[::-1][:5]
        
        # Recall@K ê³„ì‚°
        found_at_1 = any(idx in gt_indices for idx in top_k_indices[:1])
        found_at_5 = any(idx in gt_indices for idx in top_k_indices[:5])
        
        recall_at_1.append(1.0 if found_at_1 else 0.0)
        recall_at_5.append(1.0 if found_at_5 else 0.0)
        
        # MRR ê³„ì‚°
        reciprocal_rank = 0.0
        for rank, idx in enumerate(top_k_indices, 1):
            if idx in gt_indices:
                reciprocal_rank = 1.0 / rank
                break
        mrr_scores.append(reciprocal_rank)
        
        # NDCG ê³„ì‚° (ê°„ë‹¨ ë²„ì „)
        dcg = 0.0
        for rank, idx in enumerate(top_k_indices, 1):
            if idx in gt_indices:
                dcg += 1.0 / np.log2(rank + 1)
        
        # Ideal DCG (ì •ë‹µì´ 1ìœ„ì¼ ë•Œ)
        idcg = 1.0 / np.log2(2)
        ndcg = dcg / idcg if idcg > 0 else 0.0
        ndcg_scores.append(ndcg)
        
        evaluated_queries += 1
    
    if evaluated_queries == 0:
        print("   âš ï¸ í‰ê°€ ê°€ëŠ¥í•œ ì¿¼ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return {
            'recall@1': 0.0,
            'recall@5': 0.0,
            'mrr': 0.0,
            'ndcg': 0.0
        }
    
    results = {
        'recall@1': np.mean(recall_at_1),
        'recall@5': np.mean(recall_at_5),
        'mrr': np.mean(mrr_scores),
        'ndcg': np.mean(ndcg_scores)
    }
    
    print(f"   í‰ê°€ ì¿¼ë¦¬: {evaluated_queries}ê°œ")
    print(f"   Recall@1: {results['recall@1']:.2%}")
    print(f"   Recall@5: {results['recall@5']:.2%}")
    print(f"   MRR: {results['mrr']:.4f}")
    print(f"   NDCG: {results['ndcg']:.4f}")
    
    return results

def calculate_memory_usage(embeddings):
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚°"""
    memory_mb = embeddings.nbytes / 1024 / 1024
    return memory_mb

def evaluate_model(model_key, model_info, texts, queries, df, query_to_doc):
    """ë‹¨ì¼ ëª¨ë¸ í‰ê°€"""
    print("\n" + "=" * 80)
    print(f"ğŸ¤– ëª¨ë¸: {model_key}")
    print(f"   ì´ë¦„: {model_info['name']}")
    print(f"   ì„¤ëª…: {model_info['description']}")
    print(f"   ì°¨ì›: {model_info['dim']}")
    print("=" * 80)
    
    # ëª¨ë¸ ë¡œë“œ
    print(f"\nğŸ“¦ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    model = SentenceTransformer(model_info['name'])
    
    # 1. ì„ë² ë”© ì†ë„ (ìƒ˜í”Œë¡œ ì¸¡ì •)
    sample_size = min(1000, len(texts))
    sample_texts = texts[:sample_size]
    embeddings, elapsed, speed = evaluate_embedding_speed(model, sample_texts)
    
    # 2. ì„ë² ë”© í’ˆì§ˆ
    quality_stats = evaluate_embedding_quality(embeddings)
    
    # 3. ê²€ìƒ‰ ì„±ëŠ¥ (ì „ì²´ ì½”í¼ìŠ¤ ì‚¬ìš©!)
    retrieval_results = evaluate_retrieval_performance(model, texts, queries, df, query_to_doc)
    
    # 4. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
    memory_mb = calculate_memory_usage(embeddings)
    print(f"\nğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_mb:.2f} MB ({sample_size:,}ê°œ ë¬¸ì„œ)")
    
    # ì „ì²´ corpus ë©”ëª¨ë¦¬ ì˜ˆì¸¡
    total_docs = len(texts)
    estimated_memory = memory_mb * (total_docs / sample_size)
    print(f"   ì „ì²´ corpus ì˜ˆìƒ: {estimated_memory:.2f} MB ({total_docs:,}ê°œ)")
    
    return {
        'model_key': model_key,
        'model_name': model_info['name'],
        'dimension': model_info['dim'],
        'embedding_time': elapsed,
        'embedding_speed': speed,
        'memory_mb': memory_mb,
        'estimated_total_memory_mb': estimated_memory,
        'quality_mean': quality_stats['mean'],
        'quality_std': quality_stats['std'],
        'recall@1': retrieval_results['recall@1'],
        'recall@5': retrieval_results['recall@5'],
        'mrr': retrieval_results['mrr'],
        'ndcg': retrieval_results['ndcg']
    }

def compare_models(results):
    """ëª¨ë¸ ë¹„êµ ë° ìˆœìœ„"""
    print("\n" + "=" * 80)
    print("ğŸ“Š ëª¨ë¸ ë¹„êµ ê²°ê³¼")
    print("=" * 80)
    
    df = pd.DataFrame(results)
    
    # ì •ë ¬ëœ í…Œì´ë¸” ì¶œë ¥
    print("\n1ï¸âƒ£ ê²€ìƒ‰ ì„±ëŠ¥ (Recall@5 ê¸°ì¤€)")
    print("-" * 80)
    df_sorted = df.sort_values('recall@5', ascending=False)
    for _, row in df_sorted.iterrows():
        print(f"{row['model_key']:<20} R@1: {row['recall@1']:.2%}  R@5: {row['recall@5']:.2%}  MRR: {row['mrr']:.4f}  NDCG: {row['ndcg']:.4f}")
    
    print("\n2ï¸âƒ£ ì„ë² ë”© ì†ë„")
    print("-" * 80)
    df_sorted = df.sort_values('embedding_speed', ascending=False)
    for _, row in df_sorted.iterrows():
        print(f"{row['model_key']:<20} {row['embedding_speed']:.1f} docs/sec  ({row['embedding_time']:.2f}ì´ˆ)")
    
    print("\n3ï¸âƒ£ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± (ì „ì²´ corpus ê¸°ì¤€)")
    print("-" * 80)
    df_sorted = df.sort_values('estimated_total_memory_mb', ascending=True)
    for _, row in df_sorted.iterrows():
        print(f"{row['model_key']:<20} {row['estimated_total_memory_mb']:.0f} MB  (ì°¨ì›: {row['dimension']})")
    
    print("\n4ï¸âƒ£ ì„ë² ë”© í’ˆì§ˆ (ìœ ì‚¬ë„ ë¶„í¬)")
    print("-" * 80)
    df_sorted = df.sort_values('quality_std', ascending=False)
    for _, row in df_sorted.iterrows():
        print(f"{row['model_key']:<20} í‰ê· : {row['quality_mean']:.4f}  í‘œì¤€í¸ì°¨: {row['quality_std']:.4f}")
    
    # ì¢…í•© ì ìˆ˜ ê³„ì‚°
    print("\n" + "=" * 80)
    print("ğŸ† ì¢…í•© í‰ê°€ (ê°€ì¤‘ ì ìˆ˜)")
    print("=" * 80)
    
    # ì •ê·œí™”
    df['score_retrieval'] = (df['recall@1'] * 0.3 + df['recall@5'] * 0.5 + df['mrr'] * 0.2) * 40  # 40ì 
    df['score_speed'] = (df['embedding_speed'] / df['embedding_speed'].max()) * 30  # 30ì 
    df['score_memory'] = (1 - (df['estimated_total_memory_mb'] / df['estimated_total_memory_mb'].max())) * 20  # 20ì 
    df['score_quality'] = (df['quality_std'] / df['quality_std'].max()) * 10  # 10ì 
    
    df['total_score'] = df['score_retrieval'] + df['score_speed'] + df['score_memory'] + df['score_quality']
    
    df_sorted = df.sort_values('total_score', ascending=False)
    
    print(f"\n{'ëª¨ë¸':<20} {'ì´ì ':<8} {'ê²€ìƒ‰':<8} {'ì†ë„':<8} {'ë©”ëª¨ë¦¬':<8} {'í’ˆì§ˆ':<8}")
    print("-" * 80)
    for _, row in df_sorted.iterrows():
        print(f"{row['model_key']:<20} {row['total_score']:.1f}    "
              f"{row['score_retrieval']:.1f}    "
              f"{row['score_speed']:.1f}    "
              f"{row['score_memory']:.1f}    "
              f"{row['score_quality']:.1f}")
    
    # ì¶”ì²œ
    print("\n" + "=" * 80)
    print("ğŸ’¡ ì¶”ì²œ")
    print("=" * 80)
    
    best_overall = df_sorted.iloc[0]
    best_speed = df.loc[df['embedding_speed'].idxmax()]
    best_memory = df.loc[df['estimated_total_memory_mb'].idxmin()]
    best_retrieval = df.loc[df['recall@5'].idxmax()]
    
    print(f"\nğŸ¥‡ ì¢…í•© 1ìœ„: {best_overall['model_key']}")
    print(f"   - Recall@1: {best_overall['recall@1']:.2%}, Recall@5: {best_overall['recall@5']:.2%}")
    print(f"   - MRR: {best_overall['mrr']:.4f}, NDCG: {best_overall['ndcg']:.4f}")
    print(f"   - ì†ë„: {best_overall['embedding_speed']:.1f} docs/sec")
    print(f"   - ë©”ëª¨ë¦¬: {best_overall['estimated_total_memory_mb']:.0f} MB")
    
    print(f"\nâš¡ ì†ë„ ìµœê³ : {best_speed['model_key']}")
    print(f"   - ì†ë„: {best_speed['embedding_speed']:.1f} docs/sec")
    
    print(f"\nğŸ’¾ ë©”ëª¨ë¦¬ ìµœê³ : {best_memory['model_key']}")
    print(f"   - ë©”ëª¨ë¦¬: {best_memory['estimated_total_memory_mb']:.0f} MB")
    
    print(f"\nğŸ¯ ê²€ìƒ‰ ìµœê³ : {best_retrieval['model_key']}")
    print(f"   - Recall@5: {best_retrieval['recall@5']:.2%}")
    print(f"   - MRR: {best_retrieval['mrr']:.4f}")
    
    return df_sorted

def main():
    print("=" * 80)
    print("ğŸ”¬ ì„ë² ë”© ëª¨ë¸ ë¹„êµ í‰ê°€")
    print("=" * 80)
    
    print("\nğŸ“‹ í‰ê°€í•  ëª¨ë¸:")
    for key, info in MODELS.items():
        print(f"   â€¢ {key}: {info['description']} (dim={info['dim']})")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    texts, queries, df, query_to_doc = load_test_data()
    
    print("\nğŸ“Œ í‰ê°€ ë°©ë²•:")
    print(f"   - ì „ì²´ ì½”í¼ìŠ¤ ì‚¬ìš©: {len(texts):,}ê°œ ë¬¸ì„œ")
    print(f"   - í‰ê°€ ì¿¼ë¦¬: {len(queries)}ê°œ (Manual ì„¸íŠ¸)")
    print(f"   - Ground Truth: {len(query_to_doc)}ê°œ ë§¤í•‘")
    print(f"   - ì§€í‘œ: Top-1, Top-5, MRR, NDCG")
    
    # ì‚¬ìš©ì í™•ì¸
    print("\n" + "=" * 80)
    response = input(f"\n{len(MODELS)}ê°œ ëª¨ë¸ í‰ê°€ë¥¼ ì‹œì‘í• ê¹Œìš”? (y/n): ").strip().lower()
    
    if response != 'y':
        print("ì·¨ì†Œë¨")
        return
    
    # ê° ëª¨ë¸ í‰ê°€
    results = []
    
    for model_key, model_info in MODELS.items():
        try:
            result = evaluate_model(model_key, model_info, texts, queries, df, query_to_doc)
            results.append(result)
        except Exception as e:
            print(f"\nâŒ {model_key} í‰ê°€ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    if not results:
        print("\nâŒ í‰ê°€ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
        return
    
    # ë¹„êµ ë¶„ì„
    df_results = compare_models(results)
    
    # ê²°ê³¼ ì €ì¥
    output_path = PROJECT_ROOT / "data" / "model_comparison_results.csv"
    df_results.to_csv(output_path, index=False)
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")
    
    print("\n" + "=" * 80)
    print("âœ… í‰ê°€ ì™„ë£Œ!")
    print("=" * 80)

if __name__ == "__main__":
    main()
