#!/usr/bin/env python3
"""
Ground Truth í™•ì¥: LLMì„ ì‚¬ìš©í•˜ì—¬ ì¶”ê°€ ì§ˆë¬¸ ìƒì„±
"""
import pandas as pd
from pathlib import Path
from openai import OpenAI
from dotenv import load_dotenv
import os
import random

load_dotenv()

PROJECT_ROOT = Path(__file__).resolve().parents[1]

def generate_questions_for_chunk(client, chunk_text, chunk_id, existing_queries, num_questions=2):
    """
    íŠ¹ì • chunkì— ëŒ€í•œ ì¶”ê°€ ì§ˆë¬¸ ìƒì„±
    """
    existing_str = "\n".join([f"- {q}" for q in existing_queries])
    
    prompt = f"""ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ì½ê³ , í•™ìƒë“¤ì´ ì‹¤ì œë¡œ ë¬¼ì–´ë³¼ ë§Œí•œ ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸ì„ {num_questions}ê°œ ìƒì„±í•´ì£¼ì„¸ìš”.

<ë¬¸ì„œ ë‚´ìš©>
{chunk_text[:800]}
</ë¬¸ì„œ ë‚´ìš©>

<ì´ë¯¸ ìˆëŠ” ì§ˆë¬¸ë“¤>
{existing_str}
</ì´ë¯¸ ìˆëŠ” ì§ˆë¬¸ë“¤>

ìš”êµ¬ì‚¬í•­:
1. ì´ë¯¸ ìˆëŠ” ì§ˆë¬¸ê³¼ ì¤‘ë³µë˜ì§€ ì•Šê²Œ
2. êµ¬ì–´ì²´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ (ì˜ˆ: "~ì¸ê°€ìš”?", "~í•´ì£¼ì„¸ìš”", "~ì–´ë–»ê²Œ ë˜ë‚˜ìš”?")
3. ë¬¸ì„œ ë‚´ìš©ì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ìˆëŠ” ì§ˆë¬¸ë§Œ
4. ê° ì§ˆë¬¸ì€ í•œ ì¤„ë¡œ, ë²ˆí˜¸ ì—†ì´

ì§ˆë¬¸ {num_questions}ê°œë¥¼ ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ì‘ì„±í•˜ì„¸ìš”:"""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ëŒ€í•™ìƒë“¤ì˜ ì‹¤ì œ ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.8,
        max_tokens=300
    )
    
    questions = response.choices[0].message.content.strip().split('\n')
    # ë¹ˆ ì¤„, ë²ˆí˜¸ ì œê±°
    questions = [q.strip() for q in questions if q.strip()]
    questions = [q.lstrip('123456789.- ') for q in questions]
    
    return questions[:num_questions]

def expand_ground_truth(target_size=100):
    """
    Ground truthë¥¼ target_sizeê°œë¡œ í™•ì¥
    """
    print(f"ğŸš€ Ground Truth í™•ì¥ ì‹œì‘ (ëª©í‘œ: {target_size}ê°œ)")
    
    # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    gt_df = pd.read_csv(PROJECT_ROOT / 'data/ground_truth.csv')
    corpus_df = pd.read_csv(PROJECT_ROOT / 'data/corpus_filtered.csv')
    
    print(f"  í˜„ì¬ queries: {len(gt_df)}ê°œ")
    print(f"  í•„ìš”í•œ ì¶”ê°€ queries: {target_size - len(gt_df)}ê°œ")
    
    # OpenAI í´ë¼ì´ì–¸íŠ¸
    client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
    
    # Chunkë³„ ê¸°ì¡´ ì§ˆë¬¸ ê·¸ë£¹í™”
    chunk_queries = gt_df.groupby('chunk_id')['query'].apply(list).to_dict()
    
    # ê° chunkì˜ í…ìŠ¤íŠ¸
    chunk_texts = corpus_df.set_index('chunk_id')['text'].to_dict()
    
    # ì§ˆë¬¸ì´ ì ì€ chunkë¶€í„° ìš°ì„  í™•ì¥
    chunk_counts = gt_df['chunk_id'].value_counts()
    chunks_to_expand = chunk_counts[chunk_counts < 5].index.tolist()
    
    # ì§ˆë¬¸ì´ 1ê°œë¿ì¸ chunk ìš°ì„ 
    chunks_to_expand.sort(key=lambda x: chunk_counts[x])
    
    new_rows = []
    needed = target_size - len(gt_df)
    
    print(f"\nğŸ“ ì§ˆë¬¸ ìƒì„± ì¤‘...")
    
    while needed > 0 and chunks_to_expand:
        for chunk_id in chunks_to_expand[:]:
            if needed <= 0:
                break
            
            if chunk_id not in chunk_texts:
                chunks_to_expand.remove(chunk_id)
                continue
            
            # ê° chunkë‹¹ 1-2ê°œì”© ìƒì„±
            num_to_gen = min(2, needed, 5 - len(chunk_queries.get(chunk_id, [])))
            
            if num_to_gen <= 0:
                chunks_to_expand.remove(chunk_id)
                continue
            
            try:
                new_questions = generate_questions_for_chunk(
                    client,
                    chunk_texts[chunk_id],
                    chunk_id,
                    chunk_queries.get(chunk_id, []),
                    num_questions=num_to_gen
                )
                
                for q in new_questions:
                    if q and len(q) > 5:  # ìœ íš¨í•œ ì§ˆë¬¸ë§Œ
                        new_rows.append({
                            'query': q,
                            'chunk_id': chunk_id
                        })
                        chunk_queries.setdefault(chunk_id, []).append(q)
                        needed -= 1
                        print(f"  [{len(gt_df) + len(new_rows)}/{target_size}] {chunk_id[:30]}... â†’ {q[:50]}...")
                        
                        if needed <= 0:
                            break
                
            except Exception as e:
                print(f"  âš ï¸  {chunk_id} ìƒì„± ì‹¤íŒ¨: {e}")
                continue
        
        # ëª¨ë“  chunkë¥¼ í•œ ë²ˆì”© ëŒì•˜ëŠ”ë°ë„ í•„ìš”í•˜ë©´, ëœë¤í•˜ê²Œ ì¶”ê°€
        if needed > 0 and not chunks_to_expand:
            chunks_to_expand = list(chunk_texts.keys())
            random.shuffle(chunks_to_expand)
    
    # ìƒˆ ë°ì´í„° ì¶”ê°€
    if new_rows:
        new_df = pd.DataFrame(new_rows)
        final_df = pd.concat([gt_df, new_df], ignore_index=True)
        
        # ì €ì¥
        final_df.to_csv(PROJECT_ROOT / 'data/ground_truth.csv', index=False)
        
        # queries.txtë„ ì—…ë°ì´íŠ¸
        with open(PROJECT_ROOT / 'data/queries.txt', 'w', encoding='utf-8') as f:
            for query in final_df['query'].unique():
                f.write(query + '\n')
        
        print(f"\nâœ… Ground Truth í™•ì¥ ì™„ë£Œ!")
        print(f"  ìµœì¢… queries: {len(final_df)}ê°œ")
        print(f"  ê³ ìœ  chunk_id: {final_df['chunk_id'].nunique()}ê°œ")
        print(f"  ìƒˆë¡œ ì¶”ê°€ëœ queries: {len(new_rows)}ê°œ")
        
        # ë¶„í¬ í™•ì¸
        final_counts = final_df['chunk_id'].value_counts()
        print(f"\nğŸ“Š Chunkë³„ query ë¶„í¬:")
        print(f"  í‰ê· : {final_counts.mean():.1f}ê°œ")
        print(f"  ìµœëŒ€: {final_counts.max()}ê°œ")
        print(f"  ìµœì†Œ: {final_counts.min()}ê°œ")
    else:
        print("\nâš ï¸  ìƒˆë¡œìš´ ì§ˆë¬¸ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--target', type=int, default=100, help='ëª©í‘œ ì§ˆë¬¸ ìˆ˜')
    args = parser.parse_args()
    
    expand_ground_truth(args.target)
