#!/usr/bin/env python3
"""
í†µí•© í¬ë¡¤ë§ ë°ì´í„°ë¡œë¶€í„° ì½”í¼ìŠ¤ ìƒì„±
data/crawled_data/ â†’ data/corpus.csv
"""
import json
import csv
import re
from pathlib import Path
from typing import List, Dict

# ì„¤ì •
CRAWLED_DIR = Path("data/crawled_data/pages")
OUT_CSV = Path("data/corpus.csv")
CHUNK_SIZE = 1000  # ì²­í¬ í¬ê¸° (ë¬¸ì)
OVERLAP = 150      # ì˜¤ë²„ë© (ë¬¸ì)

# ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ íŒ¨í„´
NOISE_PATTERNS = [
    r'ê³µì§€ì‚¬í•­.*?ë°”ë¡œê°€ê¸°',
    r'ë‹¤ìŒ\s*í˜ì´ì§€',
    r'ì´ì „\s*í˜ì´ì§€',
    r'í˜ì´ì§€\s*ì´ë™',
    r'ëª©ë¡ìœ¼ë¡œ',
    r'top\s*â†‘',
    r'ë§¨\s*ìœ„ë¡œ',
    r'Home\s*â€º',
    r'sitemap',
    r'copyright.*?all\s+rights\s+reserved',
    r'ê°œì¸ì •ë³´ì²˜ë¦¬ë°©ì¹¨',
    r'ì´ë©”ì¼ë¬´ë‹¨ìˆ˜ì§‘ê±°ë¶€',
    r'\[\s*ì¸ì‡„\s*\]',
    r'\[\s*ëª©ë¡\s*\]',
    r'\s{3,}',  # 3ê°œ ì´ìƒ ì—°ì† ê³µë°±
]

def clean_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ì œ"""
    if not text:
        return ""
    
    # ë…¸ì´ì¦ˆ íŒ¨í„´ ì œê±°
    for pattern in NOISE_PATTERNS:
        text = re.sub(pattern, ' ', text, flags=re.IGNORECASE)
    
    # ì—°ì† ê³µë°± ì •ë¦¬
    text = re.sub(r'\s+', ' ', text)
    
    # ì•ë’¤ ê³µë°± ì œê±°
    text = text.strip()
    
    return text

def chunk_text(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = OVERLAP) -> List[str]:
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
    if len(text) <= chunk_size:
        return [text] if text.strip() else []
    
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        
        # ë§ˆì§€ë§‰ ì²­í¬ê°€ ì•„ë‹ˆë©´ ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸°
        if end < len(text):
            # ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œë¡œ ëë‚˜ëŠ” ìœ„ì¹˜ ì°¾ê¸°
            last_period = max(
                chunk.rfind('.'),
                chunk.rfind('!'),
                chunk.rfind('?'),
                chunk.rfind('ã€‚')
            )
            
            if last_period > chunk_size * 0.5:  # ì²­í¬ì˜ ì ˆë°˜ ì´ìƒì´ë©´ ì‚¬ìš©
                end = start + last_period + 1
                chunk = text[start:end]
        
        chunk = chunk.strip()
        if chunk:
            chunks.append(chunk)
        
        # ë‹¤ìŒ ì‹œì‘ì  (ì˜¤ë²„ë© ì ìš©)
        start = end - overlap
        
        # ë¬´í•œ ë£¨í”„ ë°©ì§€
        if start <= 0 or start >= len(text):
            break
    
    return chunks

def create_corpus():
    """í¬ë¡¤ë§ ë°ì´í„°ë¡œë¶€í„° ì½”í¼ìŠ¤ ìƒì„±"""
    print("=" * 80)
    print("ğŸ“ ì½”í¼ìŠ¤ ìƒì„±")
    print("=" * 80)
    
    if not CRAWLED_DIR.exists():
        print(f"âŒ í¬ë¡¤ë§ ë°ì´í„° í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {CRAWLED_DIR}")
        return
    
    json_files = list(CRAWLED_DIR.glob("*.json"))
    print(f"\nğŸ“‚ ì…ë ¥: {CRAWLED_DIR}/")
    print(f"   JSON íŒŒì¼: {len(json_files)}ê°œ")
    
    print(f"\nğŸ“„ ì¶œë ¥: {OUT_CSV}")
    print(f"   ì²­í¬ í¬ê¸°: {CHUNK_SIZE}ì")
    print(f"   ì˜¤ë²„ë©: {OVERLAP}ì")
    
    # CSV íŒŒì¼ ìƒì„±
    rows = []
    stats = {
        'total_pages': 0,
        'total_chunks': 0,
        'skipped_empty': 0,
        'skipped_short': 0,
    }
    
    print(f"\nâ³ ì²˜ë¦¬ ì¤‘...")
    
    for json_file in sorted(json_files):
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            url = data.get('url', '')
            title = data.get('title', '')
            text = data.get('text', '')
            metadata = data.get('metadata', {})
            
            # í…ìŠ¤íŠ¸ ì •ì œ
            clean = clean_text(text)
            
            if not clean:
                stats['skipped_empty'] += 1
                continue
            
            if len(clean) < 50:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì œì™¸
                stats['skipped_short'] += 1
                continue
            
            # ì²­í¬ ë¶„í• 
            chunks = chunk_text(clean)
            
            for i, chunk in enumerate(chunks):
                row = {
                    'id': f"{json_file.stem}_chunk{i}",
                    'url': url,
                    'title': title,
                    'text': chunk,
                    'chunk_index': i,
                    'total_chunks': len(chunks),
                    'source': metadata.get('source', 'unknown'),
                    'domain': metadata.get('domain', ''),
                }
                
                # ì²¨ë¶€íŒŒì¼ ì •ë³´ ì¶”ê°€
                if 'attachments_count' in metadata:
                    row['attachments_count'] = metadata['attachments_count']
                
                rows.append(row)
            
            stats['total_pages'] += 1
            stats['total_chunks'] += len(chunks)
            
            if stats['total_pages'] % 50 == 0:
                print(f"   ì²˜ë¦¬ ì¤‘: {stats['total_pages']}ê°œ í˜ì´ì§€, {stats['total_chunks']}ê°œ ì²­í¬")
        
        except Exception as e:
            print(f"   âš ï¸  {json_file.name}: {e}")
    
    # CSV ì €ì¥
    if rows:
        fieldnames = ['id', 'url', 'title', 'text', 'chunk_index', 'total_chunks', 
                      'source', 'domain', 'attachments_count']
        
        with open(OUT_CSV, 'w', encoding='utf-8', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(rows)
        
        print(f"\nâœ… ì½”í¼ìŠ¤ ìƒì„± ì™„ë£Œ!")
    else:
        print(f"\nâŒ ìƒì„±ëœ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    # í†µê³„
    print("\n" + "=" * 80)
    print("ğŸ“Š í†µê³„")
    print("=" * 80)
    print(f"ì´ í˜ì´ì§€: {stats['total_pages']}ê°œ")
    print(f"ì´ ì²­í¬: {stats['total_chunks']}ê°œ")
    print(f"í˜ì´ì§€ë‹¹ í‰ê·  ì²­í¬: {stats['total_chunks'] / stats['total_pages']:.1f}ê°œ")
    print(f"ê±´ë„ˆë›´ í˜ì´ì§€:")
    print(f"  - ë¹ˆ í…ìŠ¤íŠ¸: {stats['skipped_empty']}ê°œ")
    print(f"  - ë„ˆë¬´ ì§§ìŒ: {stats['skipped_short']}ê°œ")
    print("=" * 80)

def main():
    create_corpus()

if __name__ == "__main__":
    main()
