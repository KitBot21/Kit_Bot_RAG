#!/usr/bin/env python3
"""
RAG ì‹œìŠ¤í…œ ì •ëŸ‰ì  í‰ê°€

1. Retrieval ì„±ëŠ¥: Recall@K, MRR
2. Generation í’ˆì§ˆ: ì •í™•ì„±, ê´€ë ¨ì„±, ì™„ì„±ë„, ê·¼ê±°ì„± (ìˆ˜ë™ í‰ê°€ìš© ìƒ˜í”Œ)
"""

import pandas as pd
import numpy as np
import sys
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

PROJECT_ROOT = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(PROJECT_ROOT))

from rag_demo import RAGSystem

DATA_DIR = PROJECT_ROOT / "data"

def load_ground_truth():
    """Ground Truth ë¡œë“œ"""
    gt_path = DATA_DIR / "ground_truth_100.csv"
    gt_df = pd.read_csv(gt_path)
    
    # rank > 0ì¸ ê²ƒë§Œ (ì •ë‹µ ìˆëŠ” ê²ƒ)
    gt_valid = gt_df[gt_df['rank'] > 0].copy()
    
    return gt_valid

def evaluate_retrieval():
    """Retrieval ì„±ëŠ¥ í‰ê°€"""
    print("=" * 80)
    print("ğŸ“Š RAG ì‹œìŠ¤í…œ ì •ëŸ‰ì  í‰ê°€")
    print("=" * 80)
    
    # Ground Truth ë¡œë“œ
    print("\nğŸ“‹ Ground Truth ë¡œë“œ ì¤‘...")
    gt_df = load_ground_truth()
    print(f"   âœ… {len(gt_df)}ê°œ ì¿¼ë¦¬ (ì •ë‹µ ìˆëŠ” ê²ƒë§Œ)")
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (Retrievalë§Œ)
    print(f"\nğŸš€ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”...")
    rag = RAGSystem(llm_provider='openai', llm_model='gpt-4o-mini')
    
    # Corpus ë¡œë“œ
    print(f"\nğŸ“š Corpus ë¡œë“œ...")
    corpus = pd.read_csv(DATA_DIR / "corpus_all.csv")
    
    # Document name â†’ indices ë§¤í•‘
    doc_name_to_idx = {}
    for idx, row in corpus.iterrows():
        # document_name ìš°ì„ 
        if pd.notna(row.get('document_name')) and row['document_name']:
            doc_name = row['document_name']
            if doc_name not in doc_name_to_idx:
                doc_name_to_idx[doc_name] = []
            doc_name_to_idx[doc_name].append(idx)
        # title ëŒ€ì²´
        elif pd.notna(row.get('title')) and row['title']:
            title = row['title']
            if title not in doc_name_to_idx:
                doc_name_to_idx[title] = []
            doc_name_to_idx[title].append(idx)
    
    print(f"   âœ… {len(corpus):,}ê°œ ë¬¸ì„œ")
    
    # í‰ê°€ ì‹œì‘
    print("\n" + "=" * 80)
    print("ğŸ” Retrieval ì„±ëŠ¥ í‰ê°€")
    print("=" * 80)
    
    recall_at_1 = []
    recall_at_3 = []
    recall_at_5 = []
    mrr_scores = []
    
    generation_results = []  # LLM ë‹µë³€ ìƒ˜í”Œ ì €ì¥
    
    evaluated = 0
    
    for _, row in gt_df.iterrows():
        query = row['query']
        gt_doc_name = row['document_name']
        
        if not isinstance(query, str) or not isinstance(gt_doc_name, str):
            continue
        
        # GT ì¸ë±ìŠ¤ ì°¾ê¸°
        gt_base = gt_doc_name.replace('.pdf', '').replace('.xlsx', '').replace('.docx', '').strip()
        
        # 1. base_doc_nameìœ¼ë¡œ ë§¤ì¹­
        corpus['base_doc_name'] = corpus['document_name'].fillna('').apply(
            lambda x: x.rsplit('_chunk', 1)[0].replace('.pdf', '').replace('.xlsx', '').replace('.docx', '').strip() if x else ''
        )
        gt_indices = set(corpus[corpus['base_doc_name'] == gt_base].index.tolist())
        
        # 2. titleë¡œ ë§¤ì¹­
        if not gt_indices:
            gt_indices = set(corpus[corpus['title'] == gt_doc_name].index.tolist())
        
        if not gt_indices:
            continue
        
        # Retrieval
        contexts = rag.retrieve(query, top_k=5)
        
        # ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
        retrieved_indices = []
        for ctx in contexts:
            doc_name = ctx.get('title', '')
            if doc_name in doc_name_to_idx:
                # ì²« ë²ˆì§¸ ì¸ë±ìŠ¤ë§Œ ì‚¬ìš©
                retrieved_indices.append(doc_name_to_idx[doc_name][0])
        
        # Recall ê³„ì‚°
        found_at_1 = any(idx in gt_indices for idx in retrieved_indices[:1])
        found_at_3 = any(idx in gt_indices for idx in retrieved_indices[:3])
        found_at_5 = any(idx in gt_indices for idx in retrieved_indices[:5])
        
        recall_at_1.append(1.0 if found_at_1 else 0.0)
        recall_at_3.append(1.0 if found_at_3 else 0.0)
        recall_at_5.append(1.0 if found_at_5 else 0.0)
        
        # MRR ê³„ì‚°
        rank = 0
        for i, idx in enumerate(retrieved_indices[:5], 1):
            if idx in gt_indices:
                rank = i
                break
        mrr_scores.append(1.0 / rank if rank > 0 else 0.0)
        
        evaluated += 1
        
        # ì§„í–‰ ìƒí™©
        if evaluated % 10 == 0:
            print(f"   ì§„í–‰: {evaluated}/{len(gt_df)}...")
        
        # ì²˜ìŒ 10ê°œëŠ” LLM ë‹µë³€ë„ ìƒì„± (ìˆ˜ë™ í‰ê°€ìš©)
        if evaluated <= 10:
            answer = rag.generate(query, contexts)
            generation_results.append({
                'query_id': evaluated,
                'query': query,
                'answer': answer,
                'top_context': contexts[0]['text'][:200] if contexts else '',
                'found_in_top1': found_at_1,
                'found_in_top5': found_at_5
            })
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 80)
    print("ğŸ“Š Retrieval ì„±ëŠ¥ ê²°ê³¼")
    print("=" * 80)
    
    print(f"\ní‰ê°€ ì¿¼ë¦¬: {evaluated}ê°œ\n")
    
    recall_1 = np.mean(recall_at_1)
    recall_3 = np.mean(recall_at_3)
    recall_5 = np.mean(recall_at_5)
    mrr = np.mean(mrr_scores)
    
    print("ğŸ” Retrieval ì„±ëŠ¥:")
    print(f"   Top-1 ì •í™•ë„: {recall_1:.1%} (1ìœ„ì—ì„œ ì •ë‹µ ì°¾ê¸°)")
    print(f"   Top-3 ì •í™•ë„: {recall_3:.1%} (ìƒìœ„ 3ê°œ ì¤‘ ì •ë‹µ í¬í•¨)")
    print(f"   Top-5 ì •í™•ë„: {recall_5:.1%} (ìƒìœ„ 5ê°œ ì¤‘ ì •ë‹µ í¬í•¨)")
    print(f"   MRR: {mrr:.3f}")
    
    # í‰ê°€ ê¸°ì¤€
    print("\nğŸ“ í‰ê°€ ê¸°ì¤€:")
    if recall_5 >= 0.9:
        print(f"   Top-5: â­â­â­â­â­ ìš°ìˆ˜ ({recall_5:.1%})")
    elif recall_5 >= 0.7:
        print(f"   Top-5: â­â­â­â­ ì–‘í˜¸ ({recall_5:.1%})")
    elif recall_5 >= 0.5:
        print(f"   Top-5: â­â­â­ ë³´í†µ ({recall_5:.1%})")
    else:
        print(f"   Top-5: â­â­ ê°œì„  í•„ìš” ({recall_5:.1%})")
    
    # Generation ìƒ˜í”Œ ì €ì¥
    if generation_results:
        gen_df = pd.DataFrame(generation_results)
        gen_path = DATA_DIR / "rag_generation_samples.csv"
        gen_df.to_csv(gen_path, index=False, encoding='utf-8')
        
        print("\n" + "=" * 80)
        print("ğŸ’¬ Generation í’ˆì§ˆ í‰ê°€ (ìˆ˜ë™)")
        print("=" * 80)
        print(f"\n{len(generation_results)}ê°œ ìƒ˜í”Œ ë‹µë³€ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"íŒŒì¼: {gen_path}")
        print("\në‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ìˆ˜ë™ í‰ê°€í•´ì£¼ì„¸ìš” (5ì  ì²™ë„):")
        print("   1. ì •í™•ì„±: ë‹µë³€ì´ ì‚¬ì‹¤ì ìœ¼ë¡œ ì •í™•í•œê°€?")
        print("   2. ê´€ë ¨ì„±: ì§ˆë¬¸ê³¼ ê´€ë ¨ìˆëŠ” ë‹µë³€ì¸ê°€?")
        print("   3. ì™„ì„±ë„: ì¶©ë¶„íˆ ìƒì„¸í•˜ê³  ì™„ì „í•œê°€?")
        print("   4. ê·¼ê±°ì„±: ì œê³µëœ ë¬¸ì„œì— ê·¼ê±°í•˜ëŠ”ê°€?")
        
        # ìƒ˜í”Œ ì¶œë ¥
        print("\nğŸ“ ë‹µë³€ ìƒ˜í”Œ (ì²˜ìŒ 3ê°œ):")
        for i in range(min(3, len(generation_results))):
            sample = generation_results[i]
            print(f"\n[{i+1}] {sample['query']}")
            print(f"ë‹µë³€: {sample['answer'][:150]}...")
            print(f"Top-1 ì •ë‹µ: {'âœ…' if sample['found_in_top1'] else 'âŒ'}")
            print(f"Top-5 ì •ë‹µ: {'âœ…' if sample['found_in_top5'] else 'âŒ'}")
    
    # ê°œì„  ë°©í–¥ ì œì‹œ
    print("\n" + "=" * 80)
    print("ğŸ’¡ ê°œì„  ë°©í–¥")
    print("=" * 80)
    
    improvements = []
    
    if recall_1 < 0.4:
        improvements.append(f"1. Top-1 ì •í™•ë„ ê°œì„ : {recall_1:.1%} â†’ 40%+ ëª©í‘œ")
        improvements.append("   - ê²€ìƒ‰ ëª¨ë¸ fine-tuning")
        improvements.append("   - ì¿¼ë¦¬ í™•ì¥ (ë™ì˜ì–´, ìœ ì‚¬ì–´)")
        improvements.append("   - ë¬¸ì„œ ë©”íƒ€ë°ì´í„° í™œìš©")
    
    if recall_5 < 0.8:
        improvements.append(f"2. Top-5 ì •í™•ë„ ê°œì„ : {recall_5:.1%} â†’ 80%+ ëª©í‘œ")
        improvements.append("   - ë” ë§ì€ ê´€ë ¨ ë¬¸ì„œ ìˆ˜ì§‘")
        improvements.append("   - ì²­í¬ í¬ê¸° ì¬ì¡°ì •")
        improvements.append("   - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê³ ë ¤")
    
    if mrr < 0.5:
        improvements.append(f"3. MRR ê°œì„ : {mrr:.3f} â†’ 0.5+ ëª©í‘œ")
        improvements.append("   - ë¦¬ë­í‚¹ ì¬ê²€í†  (ë‹¤ë¥¸ ëª¨ë¸)")
        improvements.append("   - ì¿¼ë¦¬-ë¬¸ì„œ ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜ ì¡°ì •")
    
    improvements.append("4. Generation í’ˆì§ˆ ê°œì„ :")
    improvements.append("   - í”„ë¡¬í”„íŠ¸ ê°œì„ : LLMì—ê²Œ ë” ìƒì„¸í•œ ë‹µë³€ ìš”ì²­")
    improvements.append("   - ì»¨í…ìŠ¤íŠ¸ í™•ì¥: Top-3 â†’ Top-5 ë¬¸ì„œ ì œê³µ")
    improvements.append("   - max_tokens ì¦ê°€: 800 â†’ 1200 (ë” ì™„ì„±ë„ ë†’ì€ ë‹µë³€)")
    
    if improvements:
        for imp in improvements:
            print(imp)
    else:
        print("âœ… í˜„ì¬ ì„±ëŠ¥ ìš°ìˆ˜! ìœ ì§€ ê¶Œì¥")
    
    print("\n" + "=" * 80)
    print("âœ… í‰ê°€ ì™„ë£Œ!")
    print("=" * 80)
    
    # ê²°ê³¼ ìš”ì•½ ì €ì¥
    summary = {
        'evaluated_queries': evaluated,
        'recall_at_1': recall_1,
        'recall_at_3': recall_3,
        'recall_at_5': recall_5,
        'mrr': mrr
    }
    
    summary_df = pd.DataFrame([summary])
    summary_path = DATA_DIR / "rag_quantitative_evaluation.csv"
    summary_df.to_csv(summary_path, index=False)
    print(f"\nğŸ’¾ í‰ê°€ ê²°ê³¼ ì €ì¥: {summary_path}")
    
    return summary

def main():
    evaluate_retrieval()

if __name__ == "__main__":
    main()
