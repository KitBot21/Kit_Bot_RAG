#!/usr/bin/env python3
"""
BM25 ê¸°ë°˜ Sparse Vector ìƒì„±
Qdrantì˜ sparse vector ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì—¬ í‚¤ì›Œë“œ ê²€ìƒ‰ ì§€ì›
"""
import numpy as np
import pandas as pd
from pathlib import Path
from collections import Counter, defaultdict
import math
import pickle
from konlpy.tag import Okt
import re

# í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸°
okt = Okt()

def tokenize_korean(text):
    """í•œêµ­ì–´ í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•"""
    # ìˆ«ì, ì˜ë¬¸, í•œê¸€ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ ì œê±°
    text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
    # í˜•íƒœì†Œ ë¶„ì„ (ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬ë§Œ)
    tokens = okt.pos(text, norm=True, stem=True)
    words = [word for word, pos in tokens if pos in ['Noun', 'Verb', 'Adjective']]
    # ë‹¨ì¼ ë¬¸ì ì œê±°
    words = [w for w in words if len(w) > 1]
    return words

class BM25Vectorizer:
    def __init__(self, k1=1.5, b=0.75):
        self.k1 = k1
        self.b = b
        self.vocab = {}  # word -> index
        self.idf = {}    # word -> idf score
        self.avgdl = 0   # average document length
        
    def fit(self, documents):
        """BM25 íŒŒë¼ë¯¸í„° ê³„ì‚°"""
        print(f"ğŸ“Š BM25 í•™ìŠµ ì¤‘... ({len(documents)}ê°œ ë¬¸ì„œ)")
        
        # ë¬¸ì„œë³„ í† í°í™”
        tokenized_docs = []
        doc_lengths = []
        
        for i, doc in enumerate(documents):
            if i % 100 == 0:
                print(f"  í† í°í™”: {i}/{len(documents)}")
            tokens = tokenize_korean(doc)
            tokenized_docs.append(tokens)
            doc_lengths.append(len(tokens))
        
        self.avgdl = sum(doc_lengths) / len(doc_lengths) if doc_lengths else 0
        print(f"  í‰ê·  ë¬¸ì„œ ê¸¸ì´: {self.avgdl:.1f} í† í°")
        
        # ì–´íœ˜ êµ¬ì¶• ë° DF ê³„ì‚°
        word_df = Counter()  # document frequency
        all_words = set()
        
        for tokens in tokenized_docs:
            unique_tokens = set(tokens)
            all_words.update(unique_tokens)
            for word in unique_tokens:
                word_df[word] += 1
        
        # ì–´íœ˜ ì¸ë±ìŠ¤ ìƒì„±
        self.vocab = {word: idx for idx, word in enumerate(sorted(all_words))}
        print(f"  ì–´íœ˜ í¬ê¸°: {len(self.vocab):,}ê°œ ë‹¨ì–´")
        
        # IDF ê³„ì‚°
        N = len(documents)
        for word, df in word_df.items():
            self.idf[word] = math.log((N - df + 0.5) / (df + 0.5) + 1.0)
        
        return tokenized_docs
    
    def transform(self, tokenized_docs):
        """BM25 sparse vector ìƒì„±"""
        print(f"\nğŸ”¢ Sparse Vector ìƒì„± ì¤‘...")
        sparse_vectors = []
        
        for i, tokens in enumerate(tokenized_docs):
            if i % 100 == 0:
                print(f"  ë²¡í„°í™”: {i}/{len(tokenized_docs)}")
            
            doc_len = len(tokens)
            term_freq = Counter(tokens)
            
            # Sparse vector: {index: score}
            sparse_vec = {}
            for word, tf in term_freq.items():
                if word in self.vocab:
                    idx = self.vocab[word]
                    idf = self.idf.get(word, 0)
                    
                    # BM25 score
                    score = idf * (tf * (self.k1 + 1)) / (tf + self.k1 * (1 - self.b + self.b * doc_len / self.avgdl))
                    
                    if score > 0:
                        sparse_vec[idx] = score
            
            sparse_vectors.append(sparse_vec)
        
        return sparse_vectors
    
    def transform_query(self, query):
        """ì¿¼ë¦¬ë¥¼ sparse vectorë¡œ ë³€í™˜"""
        tokens = tokenize_korean(query)
        term_freq = Counter(tokens)
        
        sparse_vec = {}
        for word, tf in term_freq.items():
            if word in self.vocab:
                idx = self.vocab[word]
                idf = self.idf.get(word, 0)
                # ì¿¼ë¦¬ëŠ” ê°„ë‹¨íˆ tf * idf
                score = tf * idf
                if score > 0:
                    sparse_vec[idx] = score
        
        return sparse_vec

def main():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--corpus', default='data/corpus_filtered.csv', help='Corpus CSV file path')
    parser.add_argument('--output', default='embeddings/bm25_filtered', help='Output prefix (without extension)')
    args = parser.parse_args()
    
    # Corpus ë¡œë“œ
    df = pd.read_csv(args.corpus)
    texts = df['text'].tolist()
    
    print(f"ì´ {len(texts)}ê°œ ë¬¸ì„œ from {args.corpus}")
    
    # BM25 ë²¡í„°í™”
    vectorizer = BM25Vectorizer()
    tokenized_docs = vectorizer.fit(texts)
    sparse_vectors = vectorizer.transform(tokenized_docs)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_path = Path(args.output)
    output_path.parent.mkdir(exist_ok=True, parents=True)
    
    # ë²¡í„°í™”ê¸° ì €ì¥
    vectorizer_path = f"{args.output}_vectorizer.pkl"
    with open(vectorizer_path, 'wb') as f:
        pickle.dump(vectorizer, f)
    print(f"\nâœ… BM25 ë²¡í„°í™”ê¸° ì €ì¥: {vectorizer_path}")
    
    # Sparse vectors ì €ì¥
    vectors_path = f"{args.output}_vectors.pkl"
    with open(vectors_path, 'wb') as f:
        pickle.dump(sparse_vectors, f)
    print(f"âœ… Sparse vectors ì €ì¥: {vectors_path}")
    
    # í†µê³„ ì¶œë ¥
    non_zero_counts = [len(vec) for vec in sparse_vectors]
    print(f"\nğŸ“Š Sparse Vector í†µê³„")
    print(f"  í‰ê·  non-zero ìš”ì†Œ: {np.mean(non_zero_counts):.1f}ê°œ")
    print(f"  ìµœëŒ€ non-zero ìš”ì†Œ: {max(non_zero_counts)}ê°œ")
    print(f"  ìµœì†Œ non-zero ìš”ì†Œ: {min(non_zero_counts)}ê°œ")
    print(f"  ì–´íœ˜ í¬ê¸°: {len(vectorizer.vocab):,}ê°œ")

if __name__ == "__main__":
    main()
