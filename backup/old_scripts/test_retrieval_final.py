#!/usr/bin/env python3
"""
ì„ë² ë”© ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€
Ground Truth ê¸°ë°˜ìœ¼ë¡œ Top-K ì •í™•ë„ ì¸¡ì •
"""
import pandas as pd
from pathlib import Path
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

PROJECT_ROOT = Path(__file__).resolve().parents[1]
DATA_DIR = PROJECT_ROOT / "data"

# ì„¤ì •
GROUND_TRUTH_DEV_CSV = DATA_DIR / "ground_truth_test.csv"
GROUND_TRUTH_TEST_CSV = DATA_DIR / "ground_truth_test.csv"
QDRANT_URL = "http://localhost:6333"
COLLECTION_NAME = "kit_corpus_bge_all"
RETRIEVER_MODEL = "BAAI/bge-m3"

def calculate_metrics(ground_truth_df, collection_name, retriever, client, top_k_values=[1, 3, 5, 10]):
    """
    Retrieval ì„±ëŠ¥ ì¸¡ì •
    
    Returns:
        dict: Top-Kë³„ ì •í™•ë„
    """
    results = {k: [] for k in top_k_values}
    
    print(f"\nğŸ” ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€ ì‹œì‘...")
    print(f"   í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: {len(ground_truth_df)}ê°œ")
    print(f"   í‰ê°€ ì§€í‘œ: Recall@K (K={top_k_values})")
    
    for idx, row in tqdm(ground_truth_df.iterrows(), total=len(ground_truth_df), desc="í‰ê°€"):
        query = row['query']
        expected_chunk_id = row['document_name']  # CSV ì»¬ëŸ¼ëª… ë³€ê²½
        
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_vector = retriever.encode(query, normalize_embeddings=True).tolist()
        
        # ìµœëŒ€ Kë¡œ ê²€ìƒ‰
        max_k = max(top_k_values)
        search_result = client.query_points(
            collection_name=collection_name,
            query=query_vector,
            limit=max_k
        ).points
        
        # ê° Kê°’ì— ëŒ€í•´ í‰ê°€
        for k in top_k_values:
            # Top-K ê²°ê³¼ì—ì„œ document_name ì¶”ì¶œ
            top_k_results = search_result[:k]
            retrieved_ids = []
            
            for hit in top_k_results:
                # document_name ì‚¬ìš©
                doc_name = hit.payload.get('document_name', '')
                retrieved_ids.append(doc_name)
            
            # ì •ë‹µì´ Top-K ì•ˆì— ìˆëŠ”ì§€ í™•ì¸
            is_correct = expected_chunk_id in retrieved_ids
            results[k].append(1 if is_correct else 0)
    
    # ì •í™•ë„ ê³„ì‚°
    metrics = {}
    for k in top_k_values:
        recall = sum(results[k]) / len(results[k]) * 100
        metrics[f"Recall@{k}"] = recall
    
    return metrics

def print_failed_queries(ground_truth_df, collection_name, retriever, client, k=5):
    """
    ì‹¤íŒ¨í•œ ì¿¼ë¦¬ë“¤ì„ ë¶„ì„í•˜ì—¬ ì¶œë ¥
    """
    print(f"\nâŒ ê²€ìƒ‰ ì‹¤íŒ¨ ì‚¬ë¡€ ë¶„ì„ (Top-{k}):")
    print("=" * 100)
    
    failed_count = 0
    
    for idx, row in ground_truth_df.iterrows():
        query = row['query']
        expected_chunk_id = row['document_name']  # CSV ì»¬ëŸ¼ëª… ë³€ê²½
        
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_vector = retriever.encode(query, normalize_embeddings=True).tolist()
        
        # ê²€ìƒ‰
        search_result = client.query_points(
            collection_name=collection_name,
            query=query_vector,
            limit=k
        ).points
        
        # ê²°ê³¼ í™•ì¸
        retrieved_ids = [hit.payload.get('document_name', '') 
                        for hit in search_result]
        
        if expected_chunk_id not in retrieved_ids:
            failed_count += 1
            if failed_count <= 10:  # ìƒìœ„ 10ê°œë§Œ ì¶œë ¥
                print(f"\n[{failed_count}] ì§ˆë¬¸: {query}")
                print(f"    ì˜ˆìƒ: {expected_chunk_id}")
                print(f"    ê²€ìƒ‰ëœ Top-{k}:")
                for i, hit in enumerate(search_result, 1):
                    doc_name = hit.payload.get('document_name', '')
                    title = hit.payload.get('title', '')
                    score = hit.score
                    print(f"      {i}. {doc_name} (score: {score:.3f}) - {title[:50]}")
    
    print(f"\nì´ ì‹¤íŒ¨: {failed_count}/{len(ground_truth_df)}ê°œ")
    print("=" * 100)

def main():
    print("=" * 80)
    print("ğŸ“Š ì„ë² ë”© ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€ (Test Set)")
    print("=" * 80)
    
    # 1. Ground Truth ë¡œë“œ
    print(f"\nğŸ“‚ Ground Truth ë¡œë“œ: {GROUND_TRUTH_DEV_CSV}")
    df = pd.read_csv(GROUND_TRUTH_DEV_CSV)
    print(f"   âœ… í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: {len(df)}ê°œ")
    
    # 2. Retriever ë¡œë“œ
    print(f"\nğŸ¤– Retriever ë¡œë“œ: {RETRIEVER_MODEL}")
    retriever = SentenceTransformer(RETRIEVER_MODEL)
    print(f"   âœ… ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ")
    
    # 3. Qdrant ì—°ê²°
    print(f"\nğŸ”Œ Qdrant ì—°ê²°: {QDRANT_URL}")
    client = QdrantClient(url=QDRANT_URL)
    print(f"   ì»¬ë ‰ì…˜: {COLLECTION_NAME}")
    
    # ì»¬ë ‰ì…˜ ì •ë³´
    collection_info = client.get_collection(COLLECTION_NAME)
    print(f"   ë²¡í„° ìˆ˜: {collection_info.points_count:,}ê°œ")
    
    # 4. ì„±ëŠ¥ í‰ê°€
    metrics = calculate_metrics(df, COLLECTION_NAME, retriever, client, top_k_values=[1, 3, 5, 10])
    
    # 5. ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 80)
    print("ğŸ“ˆ í‰ê°€ ê²°ê³¼ (Test Set)")
    print("=" * 80)
    for metric_name, value in metrics.items():
        print(f"  {metric_name}: {value:.2f}%")
    
    # 6. ì‹¤íŒ¨ ì‚¬ë¡€ ë¶„ì„
    print_failed_queries(df, COLLECTION_NAME, retriever, client, k=5)
    
    print("\n" + "=" * 80)
    print("âœ… í‰ê°€ ì™„ë£Œ!")
    print("=" * 80)
    print("\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
    print("  - Top-K ê°’ ì¡°ì •")
    print("  - Reranking ì¶”ê°€")
    print("  - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (BM25 + Dense)")
    print("  - ìµœì¢… í‰ê°€: python scripts/test_retrieval_final.py (Test Set)")
    print("=" * 80)

if __name__ == "__main__":
    main()
