#!/usr/bin/env python3
"""
í¬ë¡¤ë§ ë°ì´í„° í†µí•© ìŠ¤í¬ë¦½íŠ¸
test_crawledì™€ another_crawledë¥¼ í•˜ë‚˜ë¡œ í†µí•©
"""
import json
import shutil
from pathlib import Path
from datetime import datetime
import hashlib

def get_url_hash(url: str) -> str:
    """URLì„ í•´ì‹œê°’ìœ¼ë¡œ ë³€í™˜"""
    return hashlib.md5(url.encode()).hexdigest()[:16]

def merge_crawled_data(
    source_dir: Path,
    target_dir: Path,
    dry_run: bool = False
):
    """
    í¬ë¡¤ë§ ë°ì´í„° í†µí•©
    
    Args:
        source_dir: ì›ë³¸ ë””ë ‰í† ë¦¬ (another_crawled)
        target_dir: ëŒ€ìƒ ë””ë ‰í† ë¦¬ (test_crawled)
        dry_run: Trueë©´ ì‹¤ì œ ì‘ì—… ì—†ì´ ë¯¸ë¦¬ë³´ê¸°ë§Œ
    """
    print("=" * 80)
    print("ğŸ“¦ í¬ë¡¤ë§ ë°ì´í„° í†µí•©")
    print("=" * 80)
    print(f"\nì›ë³¸: {source_dir}")
    print(f"ëŒ€ìƒ: {target_dir}")
    print(f"ëª¨ë“œ: {'ë¯¸ë¦¬ë³´ê¸°' if dry_run else 'ì‹¤ì œ í†µí•©'}")
    print()
    
    # 1. ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ
    target_index_file = target_dir / "crawl_index.json"
    if target_index_file.exists():
        with open(target_index_file, 'r', encoding='utf-8') as f:
            target_index = json.load(f)
        existing_urls = {page['url'] for page in target_index.get('pages', [])}
        print(f"âœ… ê¸°ì¡´ ë°ì´í„°: {len(existing_urls)}ê°œ URL")
    else:
        target_index = {
            "crawl_date": datetime.now().isoformat(),
            "total_pages": 0,
            "meta": {},
            "pages": []
        }
        existing_urls = set()
        print(f"ğŸ“ ìƒˆ ì¸ë±ìŠ¤ ìƒì„±")
    
    # 2. ì›ë³¸ JSON íŒŒì¼ ë¡œë“œ
    source_pages = list((source_dir / "pages").glob("*.json"))
    print(f"ğŸ“‚ ì›ë³¸ í˜ì´ì§€: {len(source_pages)}ê°œ")
    
    # 3. í†µí•© ì²˜ë¦¬
    stats = {
        "total": len(source_pages),
        "copied": 0,
        "skipped": 0,
        "errors": 0
    }
    
    new_pages = []
    
    for source_file in sorted(source_pages):
        try:
            # JSON ë¡œë“œ
            with open(source_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            url = data.get('url', '')
            
            # ì¤‘ë³µ ì²´í¬
            if url in existing_urls:
                stats["skipped"] += 1
                continue
            
            # íŒŒì¼ëª…ì„ URL í•´ì‹œë¡œ ë³€ê²½
            url_hash = get_url_hash(url)
            target_filename = f"{url_hash}.json"
            target_file = target_dir / "pages" / target_filename
            
            # JSON êµ¬ì¡° ì •ê·œí™” (test_crawled í˜•ì‹ì— ë§ì¶”ê¸°)
            normalized_data = {
                "url": url,
                "title": data.get('title', ''),
                "text": data.get('text', ''),
                "html": data.get('html'),
                "crawled_at": data.get('crawled_at', datetime.now().isoformat()),
                "metadata": data.get('metadata', {})
            }
            
            # attachmentsê°€ ìˆìœ¼ë©´ metadataì— ì¶”ê°€
            if 'attachments' in data and data['attachments']:
                normalized_data['metadata']['attachments'] = data['attachments']
                normalized_data['metadata']['attachments_count'] = len(data['attachments'])
            
            if not dry_run:
                # íŒŒì¼ ì €ì¥
                with open(target_file, 'w', encoding='utf-8') as f:
                    json.dump(normalized_data, f, ensure_ascii=False, indent=2)
            
            # ì¸ë±ìŠ¤ì— ì¶”ê°€
            page_info = {
                "url": url,
                "file": str(target_file),
                "title": normalized_data['title'],
                "text_length": len(normalized_data['text']),
            }
            
            # ë©”íƒ€ë°ì´í„°ì—ì„œ ì¶”ê°€ ì •ë³´
            metadata = normalized_data.get('metadata', {})
            if 'attachments_count' in metadata:
                page_info['attachments_count'] = metadata['attachments_count']
            
            new_pages.append(page_info)
            existing_urls.add(url)
            stats["copied"] += 1
            
        except Exception as e:
            print(f"âŒ ì—ëŸ¬: {source_file.name} - {e}")
            stats["errors"] += 1
    
    # 4. ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
    if not dry_run and new_pages:
        target_index['pages'].extend(new_pages)
        target_index['total_pages'] = len(target_index['pages'])
        target_index['last_merged'] = datetime.now().isoformat()
        
        with open(target_index_file, 'w', encoding='utf-8') as f:
            json.dump(target_index, f, ensure_ascii=False, indent=2)
    
    # 5. ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 80)
    print("ğŸ“Š í†µí•© ê²°ê³¼")
    print("=" * 80)
    print(f"ì´ ì²˜ë¦¬: {stats['total']}ê°œ")
    print(f"  âœ… ë³µì‚¬ë¨: {stats['copied']}ê°œ")
    print(f"  â­ï¸  ì¤‘ë³µ ê±´ë„ˆëœ€: {stats['skipped']}ê°œ")
    print(f"  âŒ ì—ëŸ¬: {stats['errors']}ê°œ")
    print(f"\nìµœì¢… ë°ì´í„°: {len(target_index['pages'])}ê°œ í˜ì´ì§€")
    
    if dry_run:
        print("\nâš ï¸  ë¯¸ë¦¬ë³´ê¸° ëª¨ë“œ: ì‹¤ì œ íŒŒì¼ì€ ë³€ê²½ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   ì‹¤ì œ í†µí•©í•˜ë ¤ë©´ --execute ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
    
    print("=" * 80)
    
    return stats

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='í¬ë¡¤ë§ ë°ì´í„° í†µí•©')
    parser.add_argument('--source', type=str, default='data/another_crawled',
                        help='ì›ë³¸ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: data/another_crawled)')
    parser.add_argument('--target', type=str, default='data/test_crawled',
                        help='ëŒ€ìƒ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: data/test_crawled)')
    parser.add_argument('--execute', action='store_true',
                        help='ì‹¤ì œ í†µí•© ì‹¤í–‰ (ê¸°ë³¸ê°’: ë¯¸ë¦¬ë³´ê¸°)')
    
    args = parser.parse_args()
    
    source_dir = Path(args.source)
    target_dir = Path(args.target)
    
    # ë””ë ‰í† ë¦¬ í™•ì¸
    if not source_dir.exists():
        print(f"âŒ ì›ë³¸ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {source_dir}")
        return
    
    if not target_dir.exists():
        print(f"âŒ ëŒ€ìƒ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {target_dir}")
        return
    
    # pages í´ë” í™•ì¸ ë° ìƒì„±
    (target_dir / "pages").mkdir(exist_ok=True)
    
    # í†µí•© ì‹¤í–‰
    merge_crawled_data(
        source_dir=source_dir,
        target_dir=target_dir,
        dry_run=not args.execute
    )

if __name__ == "__main__":
    main()
