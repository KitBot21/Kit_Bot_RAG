#!/usr/bin/env python3
"""
MinIO ë¬¸ì„œ ì¶”ì¶œ + ì²­í‚¹ + í•„í„°ë§
- ì²­í¬ í¬ê¸°: 1000ì, ì˜¤ë²„ë©: 150ì
- ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ íŒ¨í„´ ì œê±°
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

import json
import tempfile
import re
import csv
from typing import Dict, List
from pdfminer.high_level import extract_text as extract_pdf_text
from docx import Document
from pptx import Presentation
import openpyxl
import xlrd
from crawler.storage.minio_storage import MinIOStorage

# ì²­í‚¹ ì„¤ì •
CHUNK_SIZE = 1000
OVERLAP = 150
MIN_CHUNK_LENGTH = 100  # ìµœì†Œ ì²­í¬ ê¸¸ì´

# í•„í„°ë§ íŒ¨í„´
FILTER_PATTERNS = [
    r'^\s*ì°¨\s*ë¡€\s*$',
    r'^\s*ëª©\s*ì°¨\s*$',
    r'^\s*ì°¸ê³ ë¬¸í—Œ\s*$',
    r'^\s*ë¶€\s*ë¡\s*$',
    r'^\s*Copyright.*$',
    r'^\s*ì €ì‘ê¶Œ.*$',
    r'^\s*All Rights Reserved.*$',
    r'^\s*í˜ì´ì§€\s*\d+\s*$',
    r'^\s*\d+\s*$',  # í˜ì´ì§€ ë²ˆí˜¸ë§Œ
    r'^\s*-\s*\d+\s*-\s*$',  # -1- í˜•ì‹
]

def clean_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ì œ ë° ë¶ˆí•„ìš”í•œ íŒ¨í„´ ì œê±°"""
    if not text:
        return ""
    
    lines = text.split('\n')
    cleaned_lines = []
    
    for line in lines:
        # í•„í„°ë§ íŒ¨í„´ ì²´í¬
        should_skip = False
        for pattern in FILTER_PATTERNS:
            if re.match(pattern, line.strip(), re.IGNORECASE):
                should_skip = True
                break
        
        if not should_skip and line.strip():
            cleaned_lines.append(line)
    
    # ì—°ì† ê³µë°± ì •ë¦¬
    text = '\n'.join(cleaned_lines)
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'\n\s*\n+', '\n', text)
    
    return text.strip()

def chunk_text(text: str) -> List[str]:
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
    if len(text) <= CHUNK_SIZE:
        return [text] if text.strip() and len(text) >= MIN_CHUNK_LENGTH else []
    
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + CHUNK_SIZE
        chunk = text[start:end]
        
        # ë§ˆì§€ë§‰ ì²­í¬ê°€ ì•„ë‹ˆë©´ ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸°
        if end < len(text):
            last_period = max(
                chunk.rfind('.'),
                chunk.rfind('!'),
                chunk.rfind('?'),
                chunk.rfind('ã€‚'),
                chunk.rfind('\n')
            )
            
            if last_period > CHUNK_SIZE * 0.5:
                end = start + last_period + 1
                chunk = text[start:end]
        
        chunk = chunk.strip()
        if chunk and len(chunk) >= MIN_CHUNK_LENGTH:
            chunks.append(chunk)
        
        start = end - OVERLAP
        if start <= 0 or start >= len(text):
            break
    
    return chunks

def extract_pdf_from_bytes(file_data: bytes) -> str:
    """PDF ë°”ì´íŠ¸ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as tmp:
            tmp.write(file_data)
            tmp_path = tmp.name
        
        text = extract_pdf_text(tmp_path)
        Path(tmp_path).unlink()
        return clean_text(text)
    except Exception as e:
        return ""

def extract_docx_from_bytes(file_data: bytes) -> str:
    """DOCX ë°”ì´íŠ¸ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        with tempfile.NamedTemporaryFile(suffix='.docx', delete=False) as tmp:
            tmp.write(file_data)
            tmp_path = tmp.name
        
        doc = Document(tmp_path)
        text = '\n'.join([para.text for para in doc.paragraphs])
        Path(tmp_path).unlink()
        return clean_text(text)
    except Exception as e:
        return ""

def extract_txt_from_bytes(file_data: bytes) -> str:
    """TXT ë°”ì´íŠ¸ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        text = file_data.decode('utf-8', errors='ignore')
        return clean_text(text)
    except:
        try:
            text = file_data.decode('cp949', errors='ignore')
            return clean_text(text)
        except:
            return ""

def extract_xlsx_from_bytes(file_data: bytes) -> str:
    """XLSX ë°”ì´íŠ¸ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        with tempfile.NamedTemporaryFile(suffix='.xlsx', delete=False) as tmp:
            tmp.write(file_data)
            tmp_path = tmp.name
        
        wb = openpyxl.load_workbook(tmp_path, data_only=True)
        texts = []
        
        for sheet in wb.worksheets:
            sheet_text = f"[ì‹œíŠ¸: {sheet.title}]\n"
            for row in sheet.iter_rows(values_only=True):
                row_text = ' | '.join([str(cell) if cell is not None else '' for cell in row])
                if row_text.strip():
                    sheet_text += row_text + '\n'
            texts.append(sheet_text)
        
        Path(tmp_path).unlink()
        return clean_text('\n'.join(texts))
    except Exception as e:
        return ""

def extract_xls_from_bytes(file_data: bytes) -> str:
    """XLS ë°”ì´íŠ¸ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        with tempfile.NamedTemporaryFile(suffix='.xls', delete=False) as tmp:
            tmp.write(file_data)
            tmp_path = tmp.name
        
        wb = xlrd.open_workbook(tmp_path)
        texts = []
        
        for sheet in wb.sheets():
            sheet_text = f"[ì‹œíŠ¸: {sheet.name}]\n"
            for row_idx in range(sheet.nrows):
                row_text = ' | '.join([str(cell.value) for cell in sheet.row(row_idx)])
                if row_text.strip():
                    sheet_text += row_text + '\n'
            texts.append(sheet_text)
        
        Path(tmp_path).unlink()
        return clean_text('\n'.join(texts))
    except Exception as e:
        return ""

def extract_pptx_from_bytes(file_data: bytes) -> str:
    """PPTX ë°”ì´íŠ¸ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        with tempfile.NamedTemporaryFile(suffix='.pptx', delete=False) as tmp:
            tmp.write(file_data)
            tmp_path = tmp.name
        
        prs = Presentation(tmp_path)
        texts = []
        
        for i, slide in enumerate(prs.slides, 1):
            slide_text = f"[ìŠ¬ë¼ì´ë“œ {i}]\n"
            for shape in slide.shapes:
                if hasattr(shape, "text"):
                    slide_text += shape.text + '\n'
            texts.append(slide_text)
        
        Path(tmp_path).unlink()
        return clean_text('\n'.join(texts))
    except Exception as e:
        return ""

def get_source_page_url(minio_storage, filename: str) -> str:
    """MinIO ë©”íƒ€ë°ì´í„°ì—ì„œ ì›ë³¸ URL ê°€ì ¸ì˜¤ê¸°"""
    try:
        stat = minio_storage.client.stat_object('kit-attachments', f'attachments/{filename}')
        metadata = stat.metadata
        return metadata.get('x-amz-meta-source-url', '') or metadata.get('source-url', '')
    except:
        return ''

def process_minio_documents():
    """MinIO ë¬¸ì„œ ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 80)
    print("ğŸ“¤ MinIO ë¬¸ì„œ ì¶”ì¶œ + ì²­í‚¹")
    print("=" * 80)
    
    print(f"\nâš™ï¸  ì„¤ì •:")
    print(f"   ì²­í¬ í¬ê¸°: {CHUNK_SIZE}ì")
    print(f"   ì˜¤ë²„ë©: {OVERLAP}ì")
    print(f"   ìµœì†Œ ì²­í¬ ê¸¸ì´: {MIN_CHUNK_LENGTH}ì")
    
    # MinIO ì—°ê²°
    minio_storage = MinIOStorage()
    
    # íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    print(f"\nğŸ“‚ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
    objects = minio_storage.client.list_objects('kit-attachments', prefix='attachments/', recursive=True)
    
    # ì²˜ë¦¬í•  íŒŒì¼ í•„í„°ë§
    target_extensions = {'.pdf', '.docx', '.txt', '.xlsx', '.xls', '.pptx'}
    files_to_process = []
    
    for obj in objects:
        filename = obj.object_name.replace('attachments/', '')
        ext = Path(filename).suffix.lower()
        if ext in target_extensions:
            files_to_process.append((filename, ext, obj.size))
    
    print(f"   ì´ íŒŒì¼: {len(files_to_process)}ê°œ")
    
    # íŒŒì¼ íƒ€ì…ë³„ í†µê³„
    from collections import Counter
    ext_counts = Counter([ext for _, ext, _ in files_to_process])
    for ext, count in sorted(ext_counts.items()):
        print(f"   {ext}: {count}ê°œ")
    
    # ì²˜ë¦¬
    results = []
    success_count = 0
    failed_count = 0
    total_chunks = 0
    
    print(f"\nâ³ ì²˜ë¦¬ ì¤‘...")
    
    for i, (filename, ext, size) in enumerate(files_to_process, 1):
        if i % 50 == 0:
            print(f"   ì§„í–‰: {i}/{len(files_to_process)} ({i/len(files_to_process)*100:.0f}%)")
        
        try:
            # íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            file_data = minio_storage.download_file(filename)
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text = ""
            if ext == '.pdf':
                text = extract_pdf_from_bytes(file_data)
            elif ext == '.docx':
                text = extract_docx_from_bytes(file_data)
            elif ext == '.txt':
                text = extract_txt_from_bytes(file_data)
            elif ext == '.xlsx':
                text = extract_xlsx_from_bytes(file_data)
            elif ext == '.xls':
                text = extract_xls_from_bytes(file_data)
            elif ext == '.pptx':
                text = extract_pptx_from_bytes(file_data)
            
            if not text or len(text) < MIN_CHUNK_LENGTH:
                failed_count += 1
                continue
            
            # ì²­í‚¹
            chunks = chunk_text(text)
            
            if not chunks:
                failed_count += 1
                continue
            
            # ì›ë³¸ URL ê°€ì ¸ì˜¤ê¸°
            source_url = get_source_page_url(minio_storage, filename)
            
            # ê° ì²­í¬ë¥¼ ê°œë³„ ë ˆì½”ë“œë¡œ ì €ì¥
            for chunk_idx, chunk in enumerate(chunks):
                results.append({
                    'text': chunk,
                    'title': filename,
                    'url': source_url,
                    'source_type': 'minio_document',
                    'document_name': f"{filename}_chunk{chunk_idx}",
                    'file_type': ext,
                    'file_size': size,
                    'chunk_index': chunk_idx,
                    'total_chunks': len(chunks)
                })
            
            success_count += 1
            total_chunks += len(chunks)
            
        except Exception as e:
            failed_count += 1
    
    print(f"\n4ï¸âƒ£ ê²°ê³¼:")
    print(f"   âœ… ì„±ê³µ: {success_count}ê°œ íŒŒì¼")
    print(f"   âŒ ì‹¤íŒ¨: {failed_count}ê°œ íŒŒì¼")
    print(f"   ğŸ“Š ì´ ì²­í¬: {total_chunks}ê°œ")
    
    # CSV ì €ì¥
    output_path = Path("data/corpus_minio_documents.csv")
    
    if results:
        with output_path.open('w', encoding='utf-8', newline='') as f:
            fieldnames = ['text', 'title', 'url', 'source_type', 'document_name', 
                         'file_type', 'file_size', 'chunk_index', 'total_chunks']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(results)
        
        print(f"\nâœ… ì €ì¥ ì™„ë£Œ: {output_path}")
        
        # í†µê³„
        total_text_length = sum(len(r['text']) for r in results)
        avg_chunk_length = total_text_length / len(results) if results else 0
        
        print(f"\nğŸ“Š í…ìŠ¤íŠ¸ í†µê³„:")
        print(f"   ì´ ë¬¸ì„œ: {len(results)}ê°œ")
        print(f"   ì´ í…ìŠ¤íŠ¸: {total_text_length:,}ì")
        print(f"   í‰ê·  ì²­í¬ ê¸¸ì´: {avg_chunk_length:.0f}ì")
    
    print("\n" + "=" * 80)
    print("ğŸ‰ ì™„ë£Œ!")
    print("=" * 80)

if __name__ == "__main__":
    process_minio_documents()
