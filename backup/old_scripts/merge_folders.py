#!/usr/bin/env python3
"""
í¬ë¡¤ë§ ë°ì´í„° í´ë” í†µí•©
test_crawled + another_crawled â†’ crawled_data (í†µí•©)
"""
import json
import shutil
from pathlib import Path
from datetime import datetime

def merge_folders(
    source_dirs: list,
    target_dir: Path,
    dry_run: bool = False
) -> dict:
    """
    ì—¬ëŸ¬ í¬ë¡¤ë§ í´ë”ë¥¼ í•˜ë‚˜ë¡œ í†µí•©
    
    Args:
        source_dirs: ì›ë³¸ ë””ë ‰í† ë¦¬ ë¦¬ìŠ¤íŠ¸
        target_dir: í†µí•© ëŒ€ìƒ ë””ë ‰í† ë¦¬
        dry_run: Trueë©´ ë¯¸ë¦¬ë³´ê¸°ë§Œ
    
    Returns:
        í†µê³„ ì •ë³´
    """
    stats = {
        "total_files": 0,
        "copied": 0,
        "duplicates": 0,
        "errors": 0
    }
    
    # URL ì¤‘ë³µ ì²´í¬ìš©
    seen_urls = set()
    all_pages = []
    
    print("=" * 80)
    print("ğŸ“¦ í¬ë¡¤ë§ ë°ì´í„° í´ë” í†µí•©")
    print("=" * 80)
    print(f"í†µí•© ëŒ€ìƒ: {target_dir}")
    print(f"ëª¨ë“œ: {'ë¯¸ë¦¬ë³´ê¸°' if dry_run else 'ì‹¤ì œ í†µí•©'}")
    print()
    
    # ëŒ€ìƒ í´ë” ìƒì„±
    if not dry_run:
        target_dir.mkdir(parents=True, exist_ok=True)
        (target_dir / "pages").mkdir(exist_ok=True)
    
    # ê° ì›ë³¸ í´ë” ì²˜ë¦¬
    for source_dir in source_dirs:
        source_path = Path(source_dir)
        pages_dir = source_path / "pages"
        
        if not pages_dir.exists():
            print(f"âš ï¸  pages í´ë” ì—†ìŒ: {pages_dir}")
            continue
        
        print(f"ğŸ“‚ ì²˜ë¦¬ ì¤‘: {source_path.name}")
        
        json_files = list(pages_dir.glob("*.json"))
        stats["total_files"] += len(json_files)
        
        for json_file in json_files:
            try:
                # JSON ë¡œë“œ
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                url = data.get("url", "")
                
                # ì¤‘ë³µ ì²´í¬
                if url in seen_urls:
                    stats["duplicates"] += 1
                    print(f"   â­ï¸  ì¤‘ë³µ: {json_file.name}")
                    continue
                
                seen_urls.add(url)
                
                # íŒŒì¼ ë³µì‚¬
                if not dry_run:
                    target_file = target_dir / "pages" / json_file.name
                    shutil.copy2(json_file, target_file)
                
                # í˜ì´ì§€ ì •ë³´ ìˆ˜ì§‘
                page_info = {
                    "url": url,
                    "file": str(target_dir / "pages" / json_file.name),
                    "title": data.get("title", ""),
                    "text_length": len(data.get("text", "")),
                    "source": data.get("metadata", {}).get("source", "unknown")
                }
                
                metadata = data.get("metadata", {})
                if "attachments_count" in metadata:
                    page_info["attachments_count"] = metadata["attachments_count"]
                if "domain" in metadata:
                    page_info["domain"] = metadata["domain"]
                
                all_pages.append(page_info)
                stats["copied"] += 1
                
            except Exception as e:
                print(f"   âŒ ì—ëŸ¬: {json_file.name} - {e}")
                stats["errors"] += 1
        
        print(f"   âœ… ì™„ë£Œ: {len(json_files)}ê°œ íŒŒì¼ ì²˜ë¦¬")
        print()
    
    # í†µí•© ì¸ë±ìŠ¤ ìƒì„±
    if not dry_run:
        index = {
            "crawl_date": datetime.now().isoformat(),
            "total_pages": len(all_pages),
            "sources": list(set(p["source"] for p in all_pages)),
            "meta": {
                "format_version": "1.0",
                "description": "Merged crawled data from multiple sources",
                "merged_at": datetime.now().isoformat()
            },
            "pages": all_pages
        }
        
        index_file = target_dir / "crawl_index.json"
        with open(index_file, 'w', encoding='utf-8') as f:
            json.dump(index, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… í†µí•© ì¸ë±ìŠ¤ ìƒì„±: {index_file}")
    else:
        print(f"ğŸ“ í†µí•© ì¸ë±ìŠ¤ ì˜ˆì •: {len(all_pages)}ê°œ í˜ì´ì§€")
    
    # ê²°ê³¼ ì¶œë ¥
    print()
    print("=" * 80)
    print("ğŸ“Š í†µí•© ê²°ê³¼")
    print("=" * 80)
    print(f"ì´ íŒŒì¼: {stats['total_files']}ê°œ")
    print(f"  âœ… ë³µì‚¬ë¨: {stats['copied']}ê°œ")
    print(f"  â­ï¸  ì¤‘ë³µ ì œì™¸: {stats['duplicates']}ê°œ")
    print(f"  âŒ ì—ëŸ¬: {stats['errors']}ê°œ")
    print(f"\nìµœì¢… ë°ì´í„°: {len(all_pages)}ê°œ í˜ì´ì§€")
    
    if not dry_run:
        print(f"\nğŸ’¾ ì €ì¥ ìœ„ì¹˜: {target_dir}")
    else:
        print("\nâš ï¸  ë¯¸ë¦¬ë³´ê¸° ëª¨ë“œ: ì‹¤ì œ íŒŒì¼ì€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   ì‹¤ì œ í†µí•©í•˜ë ¤ë©´ --execute ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
    
    print("=" * 80)
    
    return stats


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='í¬ë¡¤ë§ ë°ì´í„° í´ë” í†µí•©')
    parser.add_argument('--sources', nargs='+', 
                        default=['data/test_crawled', 'data/another_crawled'],
                        help='ì›ë³¸ ë””ë ‰í† ë¦¬ë“¤')
    parser.add_argument('--target', type=str, default='data/crawled_data',
                        help='í†µí•© ëŒ€ìƒ ë””ë ‰í† ë¦¬')
    parser.add_argument('--execute', action='store_true',
                        help='ì‹¤ì œ í†µí•© ì‹¤í–‰')
    
    args = parser.parse_args()
    
    source_dirs = args.sources
    target_dir = Path(args.target)
    
    # ì›ë³¸ í´ë” í™•ì¸
    for source in source_dirs:
        if not Path(source).exists():
            print(f"âŒ ì›ë³¸ í´ë” ì—†ìŒ: {source}")
            return
    
    # í†µí•© ì‹¤í–‰
    merge_folders(
        source_dirs=source_dirs,
        target_dir=target_dir,
        dry_run=not args.execute
    )


if __name__ == "__main__":
    main()
