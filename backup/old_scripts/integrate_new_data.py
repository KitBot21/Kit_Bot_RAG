#!/usr/bin/env python3
"""
ìƒˆë¡œ ì¶”ê°€ëœ JSON íŒŒì¼ í†µí•©
1. attachmentsë¥¼ metadataë¡œ ì´ë™ (ì–‘ì‹ í†µì¼)
2. ZIP íŒŒì¼ì—ì„œ ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ â†’ MinIO ì—…ë¡œë“œ (ì¶œì²˜ ì—°ê²°)
3. ì¸ë±ìŠ¤ ì¬ìƒì„±
"""
import sys
sys.path.insert(0, 'crawler')

import json
import zipfile
from pathlib import Path
from datetime import datetime
from storage.minio_storage import MinIOStorage

def normalize_new_files(pages_dir: Path):
    """ìƒˆë¡œ ì¶”ê°€ëœ JSON íŒŒì¼ ì–‘ì‹ í†µì¼"""
    print("=" * 80)
    print("ğŸ“‹ Step 1: JSON ì–‘ì‹ í†µì¼")
    print("=" * 80)
    
    json_files = list(pages_dir.glob("*.json"))
    
    stats = {
        "total": len(json_files),
        "normalized": 0,
        "already_ok": 0
    }
    
    for json_file in json_files:
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # attachmentsê°€ ìµœìƒìœ„ì— ìˆìœ¼ë©´ metadataë¡œ ì´ë™
            if 'attachments' in data:
                if 'metadata' not in data:
                    data['metadata'] = {}
                
                data['metadata']['attachments'] = data.pop('attachments')
                data['metadata']['attachments_count'] = len(data['metadata']['attachments'])
                
                # source, domain ì¶”ê°€
                if 'source' not in data['metadata']:
                    data['metadata']['source'] = 'new_batch'
                
                if 'domain' not in data['metadata']:
                    from urllib.parse import urlparse
                    if data.get('url'):
                        parsed = urlparse(data['url'])
                        data['metadata']['domain'] = parsed.netloc
                
                # ì €ì¥
                with open(json_file, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                
                stats["normalized"] += 1
            else:
                stats["already_ok"] += 1
        
        except Exception as e:
            print(f"  âš ï¸  {json_file.name}: {e}")
    
    print(f"\nâœ… ì–‘ì‹ í†µì¼ ì™„ë£Œ:")
    print(f"   ì´ íŒŒì¼: {stats['total']}ê°œ")
    print(f"   ì •ê·œí™”ë¨: {stats['normalized']}ê°œ")
    print(f"   ì´ë¯¸ ì •ìƒ: {stats['already_ok']}ê°œ")
    
    return stats

def upload_zip_attachments_to_minio(zip_path: str, crawled_data_dir: Path, minio_storage):
    """ZIP íŒŒì¼ë“¤ì—ì„œ ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ â†’ MinIO ì—…ë¡œë“œ"""
    
    print("\n" + "=" * 80)
    print("ğŸ“¦ Step 2: ZIP ì²¨ë¶€íŒŒì¼ â†’ MinIO ì—…ë¡œë“œ")
    print("=" * 80)
    
    # zip_pathê°€ ë””ë ‰í† ë¦¬ì¸ ê²½ìš° ëª¨ë“  ZIP íŒŒì¼ ì°¾ê¸°
    zip_files = []
    zip_path_obj = Path(zip_path)
    
    if zip_path_obj.is_dir():
        zip_files = list(zip_path_obj.glob("*.zip")) + list(zip_path_obj.glob("*.ZIP")) + list(zip_path_obj.glob("*.Zip"))
        print(f"\nZIP ë””ë ‰í† ë¦¬: {zip_path}")
        print(f"ì°¾ì€ ZIP íŒŒì¼: {len(zip_files)}ê°œ")
    elif zip_path_obj.is_file():
        zip_files = [zip_path_obj]
        print(f"\nZIP íŒŒì¼: {zip_path}")
    else:
        print(f"\nâŒ ZIP ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {zip_path}")
        return
    
    # JSON íŒŒì¼ë“¤ì—ì„œ ì²¨ë¶€íŒŒì¼ ì •ë³´ ìˆ˜ì§‘
    file_mapping = {}  # {íŒŒì¼ëª…: {page_url, download_url, original_name, size}}
    
    pages_dir = crawled_data_dir / "pages"
    for json_file in pages_dir.glob("*.json"):
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        page_url = data.get('url', '')
        attachments = data.get('metadata', {}).get('attachments', [])
        
        for att in attachments:
            saved_path = att.get('saved_path', '')
            if saved_path:
                # Windows ê²½ë¡œ â†’ Unix ê²½ë¡œ
                filename = saved_path.replace('\\', '/').split('/')[-1]
                
                file_mapping[filename] = {
                    'page_url': page_url,
                    'download_url': att.get('url', ''),
                    'original_name': att.get('name', ''),
                    'size': att.get('size', 0)
                }
    
    print(f"\nJSONì—ì„œ ì°¾ì€ ì²¨ë¶€íŒŒì¼ ì •ë³´: {len(file_mapping)}ê°œ")
    
    # ê° ZIP íŒŒì¼ ì²˜ë¦¬
    total_uploaded = 0
    total_skipped = 0
    
    for zip_file in zip_files:
        try:
            print(f"\nğŸ“¦ ì²˜ë¦¬ ì¤‘: {zip_file.name}")
            
            with zipfile.ZipFile(zip_file, 'r') as zip_ref:
                file_list = zip_ref.namelist()
                print(f"   ZIP ë‚´ íŒŒì¼: {len(file_list)}ê°œ")
                
                uploaded = 0
                skipped = 0
                
                for file_name in file_list:
                    # ë””ë ‰í† ë¦¬ ì—”íŠ¸ë¦¬ ìŠ¤í‚µ
                    if file_name.endswith('/'):
                        continue
                    
                    # í•œê¸€ íŒŒì¼ëª… ë””ì½”ë”© (EUC-KR â†’ UTF-8)
                    try:
                        # CP437ë¡œ ì¸ì½”ë”©ëœ ê²ƒì„ ë‹¤ì‹œ ë°”ì´íŠ¸ë¡œ ë³€í™˜ í›„ EUC-KRë¡œ ë””ì½”ë”©
                        decoded_name = file_name.encode('cp437').decode('euc-kr')
                    except:
                        # ë””ì½”ë”© ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì‚¬ìš©
                        decoded_name = file_name
                    
                    # íŒŒì¼ëª…ë§Œ ì¶”ì¶œ (ê²½ë¡œ ì œê±°)
                    base_name = decoded_name.replace('\\', '/').split('/')[-1]
                    
                    # ë§¤í•‘ ì •ë³´ í™•ì¸
                    if base_name not in file_mapping:
                        skipped += 1
                        continue
                    
                    info = file_mapping[base_name]
                    
                    # ZIPì—ì„œ íŒŒì¼ ì½ê¸° (ì›ë³¸ file_name ì‚¬ìš©)
                    file_data = zip_ref.read(file_name)
                    
                    # MinIOì— ì—…ë¡œë“œ
                    object_name = f"attachments/{base_name}"
                    minio_storage.upload_file(
                        file_data=file_data,
                        object_name=object_name,
                        content_type='application/octet-stream',
                        metadata={
                            'page-url': info['page_url'],
                            'download-url': info['download_url'],
                            'original-filename': info['original_name'],
                            'file-size': str(info['size'])
                        },
                        original_filename=info['original_name']
                    )
                    
                    uploaded += 1
                
                total_uploaded += uploaded
                total_skipped += skipped
                
                print(f"   âœ… ì—…ë¡œë“œ: {uploaded}ê°œ, ìŠ¤í‚µ: {skipped}ê°œ")
        
        except Exception as e:
            print(f"\n   âŒ ZIP íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ ({zip_file.name}): {e}")
            continue
    
    print(f"\nâœ… ì „ì²´ MinIO ì—…ë¡œë“œ ì™„ë£Œ:")
    print(f"   ì´ ì—…ë¡œë“œë¨: {total_uploaded}ê°œ")
    print(f"   ì´ ìŠ¤í‚µë¨: {total_skipped}ê°œ (ë§¤í•‘ ì •ë³´ ì—†ìŒ)")


def regenerate_index(pages_dir: Path, index_file: Path):
    """ì¸ë±ìŠ¤ ì¬ìƒì„±"""
    print("\n" + "=" * 80)
    print("ğŸ“‘ Step 3: ì¸ë±ìŠ¤ ì¬ìƒì„±")
    print("=" * 80)
    
    pages = []
    
    for json_file in sorted(pages_dir.glob("*.json")):
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            page_info = {
                "url": data.get("url", ""),
                "file": str(json_file),
                "title": data.get("title", ""),
                "text_length": len(data.get("text", "")),
            }
            
            metadata = data.get("metadata", {})
            if "attachments_count" in metadata:
                page_info["attachments_count"] = metadata["attachments_count"]
            if "domain" in metadata:
                page_info["domain"] = metadata["domain"]
            if "source" in metadata:
                page_info["source"] = metadata["source"]
            
            pages.append(page_info)
        
        except Exception as e:
            print(f"  âš ï¸  {json_file.name}: {e}")
    
    # ì¸ë±ìŠ¤ ì €ì¥
    index = {
        "crawl_date": datetime.now().isoformat(),
        "total_pages": len(pages),
        "meta": {
            "format_version": "1.0",
            "description": "Merged and normalized crawled data"
        },
        "pages": pages
    }
    
    with open(index_file, 'w', encoding='utf-8') as f:
        json.dump(index, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ì¸ë±ìŠ¤ ì¬ìƒì„± ì™„ë£Œ:")
    print(f"   ì´ í˜ì´ì§€: {len(pages)}ê°œ")
    print(f"   ì €ì¥ ìœ„ì¹˜: {index_file}")

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='ìƒˆ JSON íŒŒì¼ í†µí•© ë° ì²¨ë¶€íŒŒì¼ ì—…ë¡œë“œ')
    parser.add_argument('--zip', type=str, help='ì²¨ë¶€íŒŒì¼ ZIP ê²½ë¡œ')
    parser.add_argument('--skip-normalize', action='store_true', help='ì–‘ì‹ í†µì¼ ê±´ë„ˆë›°ê¸°')
    parser.add_argument('--skip-upload', action='store_true', help='ì²¨ë¶€íŒŒì¼ ì—…ë¡œë“œ ê±´ë„ˆë›°ê¸°')
    
    args = parser.parse_args()
    
    pages_dir = Path("data/crawled_data/pages")
    index_file = Path("data/crawled_data/crawl_index.json")
    
    # Step 1: ì–‘ì‹ í†µì¼
    if not args.skip_normalize:
        normalize_new_files(pages_dir)
    
    # Step 2: ì²¨ë¶€íŒŒì¼ ì—…ë¡œë“œ
    if not args.skip_upload and args.zip:
        minio = MinIOStorage(
            endpoint="localhost:9000",
            access_key="admin",
            secret_key="kitbot2025!",
            bucket_name="kit-attachments"
        )
        upload_zip_attachments_to_minio(args.zip, pages_dir.parent, minio)
    
    # Step 3: ì¸ë±ìŠ¤ ì¬ìƒì„±
    regenerate_index(pages_dir, index_file)
    
    print("\n" + "=" * 80)
    print("ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print("=" * 80)

if __name__ == "__main__":
    main()
