#!/usr/bin/env python3
"""
í¬ë¡¤ë§ ë°ì´í„° ì–‘ì‹ í†µì¼ ìŠ¤í¬ë¦½íŠ¸
ëª¨ë“  JSON íŒŒì¼ì„ ë™ì¼í•œ êµ¬ì¡°ë¡œ ë³€í™˜
"""
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, Any

def normalize_page_data(data: Dict[str, Any], source_dir: str) -> Dict[str, Any]:
    """
    í˜ì´ì§€ ë°ì´í„°ë¥¼ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    
    í‘œì¤€ í˜•ì‹:
    {
        "url": str,
        "title": str,
        "text": str,
        "html": str | null,
        "crawled_at": str (ISO format),
        "metadata": {
            "source": str,
            "domain": str,
            "attachments": [...] (ì„ íƒ),
            "attachments_count": int (ì„ íƒ),
            ...
        }
    }
    """
    # ê¸°ë³¸ í•„ë“œ
    normalized = {
        "url": data.get("url", ""),
        "title": data.get("title", ""),
        "text": data.get("text", ""),
        "html": data.get("html"),
        "crawled_at": data.get("crawled_at", datetime.now().isoformat()),
    }
    
    # ë©”íƒ€ë°ì´í„° ì´ˆê¸°í™”
    metadata = data.get("metadata", {}).copy()
    
    # ë„ë©”ì¸ ì¶”ì¶œ
    if normalized["url"]:
        from urllib.parse import urlparse
        parsed = urlparse(normalized["url"])
        metadata["domain"] = parsed.netloc
    
    # ì†ŒìŠ¤ ì •ë³´ ì¶”ê°€
    if "test_crawled" in source_dir:
        metadata["source"] = "test_crawled"
    elif "another_crawled" in source_dir:
        metadata["source"] = "another_crawled"
    else:
        metadata["source"] = "unknown"
    
    # attachments ì²˜ë¦¬ (ìµœìƒìœ„ì— ìˆìœ¼ë©´ metadataë¡œ ì´ë™)
    if "attachments" in data and data["attachments"]:
        metadata["attachments"] = data["attachments"]
        metadata["attachments_count"] = len(data["attachments"])
    
    normalized["metadata"] = metadata
    
    return normalized


def normalize_directory(
    directory: Path,
    dry_run: bool = False,
    backup: bool = True
) -> Dict[str, int]:
    """
    ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  JSON íŒŒì¼ì„ ì •ê·œí™”
    
    Args:
        directory: ëŒ€ìƒ ë””ë ‰í† ë¦¬ (pages í´ë” í¬í•¨)
        dry_run: Trueë©´ ì‹¤ì œ ë³€ê²½ ì—†ì´ ë¯¸ë¦¬ë³´ê¸°ë§Œ
        backup: Trueë©´ ì›ë³¸ ë°±ì—…
    
    Returns:
        í†µê³„ ì •ë³´ dict
    """
    pages_dir = directory / "pages"
    
    if not pages_dir.exists():
        print(f"âŒ pages í´ë” ì—†ìŒ: {pages_dir}")
        return {}
    
    stats = {
        "total": 0,
        "normalized": 0,
        "skipped": 0,
        "errors": 0
    }
    
    json_files = list(pages_dir.glob("*.json"))
    stats["total"] = len(json_files)
    
    print(f"\nğŸ“‚ ë””ë ‰í† ë¦¬: {directory}")
    print(f"ğŸ“„ JSON íŒŒì¼: {len(json_files)}ê°œ")
    
    if backup and not dry_run:
        backup_dir = directory / "pages_backup"
        backup_dir.mkdir(exist_ok=True)
        print(f"ğŸ’¾ ë°±ì—… ë””ë ‰í† ë¦¬: {backup_dir}")
    
    for json_file in json_files:
        try:
            # JSON ë¡œë“œ
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # ì •ê·œí™”
            normalized = normalize_page_data(data, str(directory))
            
            # ë³€ê²½ ì‚¬í•­ ì²´í¬
            if data == normalized:
                stats["skipped"] += 1
                continue
            
            if not dry_run:
                # ë°±ì—…
                if backup:
                    import shutil
                    backup_file = backup_dir / json_file.name
                    shutil.copy2(json_file, backup_file)
                
                # ì €ì¥
                with open(json_file, 'w', encoding='utf-8') as f:
                    json.dump(normalized, f, ensure_ascii=False, indent=2)
            
            stats["normalized"] += 1
            
        except Exception as e:
            print(f"âŒ ì—ëŸ¬: {json_file.name} - {e}")
            stats["errors"] += 1
    
    return stats


def update_index(directory: Path, dry_run: bool = False):
    """í¬ë¡¤ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸"""
    index_file = directory / "crawl_index.json"
    pages_dir = directory / "pages"
    
    if not pages_dir.exists():
        print(f"âŒ pages í´ë” ì—†ìŒ: {pages_dir}")
        return
    
    # ëª¨ë“  í˜ì´ì§€ ë¡œë“œ
    pages = []
    for json_file in sorted(pages_dir.glob("*.json")):
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            page_info = {
                "url": data.get("url", ""),
                "file": str(json_file),
                "title": data.get("title", ""),
                "text_length": len(data.get("text", "")),
            }
            
            # ë©”íƒ€ë°ì´í„°ì—ì„œ ì¶”ê°€ ì •ë³´
            metadata = data.get("metadata", {})
            if "attachments_count" in metadata:
                page_info["attachments_count"] = metadata["attachments_count"]
            if "domain" in metadata:
                page_info["domain"] = metadata["domain"]
            
            pages.append(page_info)
            
        except Exception as e:
            print(f"âŒ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {json_file.name} - {e}")
    
    # ì¸ë±ìŠ¤ ìƒì„±
    index = {
        "crawl_date": datetime.now().isoformat(),
        "total_pages": len(pages),
        "normalized": True,
        "meta": {
            "format_version": "1.0",
            "description": "Normalized crawled data"
        },
        "pages": pages
    }
    
    if not dry_run:
        with open(index_file, 'w', encoding='utf-8') as f:
            json.dump(index, f, ensure_ascii=False, indent=2)
        print(f"âœ… ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸: {index_file}")
    else:
        print(f"ğŸ“ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ ì˜ˆì •: {len(pages)}ê°œ í˜ì´ì§€")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='í¬ë¡¤ë§ ë°ì´í„° ì–‘ì‹ í†µì¼')
    parser.add_argument('directories', nargs='+', help='ëŒ€ìƒ ë””ë ‰í† ë¦¬ë“¤')
    parser.add_argument('--execute', action='store_true', help='ì‹¤ì œ ë³€ê²½ ì‹¤í–‰')
    parser.add_argument('--no-backup', action='store_true', help='ë°±ì—… ìƒëµ')
    
    args = parser.parse_args()
    
    print("=" * 80)
    print("ğŸ“‹ í¬ë¡¤ë§ ë°ì´í„° ì–‘ì‹ í†µì¼")
    print("=" * 80)
    print(f"ëª¨ë“œ: {'ì‹¤ì œ ë³€ê²½' if args.execute else 'ë¯¸ë¦¬ë³´ê¸°'}")
    print(f"ë°±ì—…: {'ì•„ë‹ˆì˜¤' if args.no_backup else 'ì˜ˆ'}")
    print()
    
    total_stats = {
        "total": 0,
        "normalized": 0,
        "skipped": 0,
        "errors": 0
    }
    
    for dir_path in args.directories:
        directory = Path(dir_path)
        
        if not directory.exists():
            print(f"âŒ ë””ë ‰í† ë¦¬ ì—†ìŒ: {directory}")
            continue
        
        # ì •ê·œí™”
        stats = normalize_directory(
            directory=directory,
            dry_run=not args.execute,
            backup=not args.no_backup
        )
        
        # í†µê³„ í•©ê³„
        for key in total_stats:
            total_stats[key] += stats.get(key, 0)
        
        # ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
        update_index(directory, dry_run=not args.execute)
        
        print()
    
    # ì „ì²´ ê²°ê³¼
    print("=" * 80)
    print("ğŸ“Š ì „ì²´ ê²°ê³¼")
    print("=" * 80)
    print(f"ì´ íŒŒì¼: {total_stats['total']}ê°œ")
    print(f"  âœ… ì •ê·œí™”ë¨: {total_stats['normalized']}ê°œ")
    print(f"  â­ï¸  ë³€ê²½ ì—†ìŒ: {total_stats['skipped']}ê°œ")
    print(f"  âŒ ì—ëŸ¬: {total_stats['errors']}ê°œ")
    
    if not args.execute:
        print("\nâš ï¸  ë¯¸ë¦¬ë³´ê¸° ëª¨ë“œ: ì‹¤ì œ íŒŒì¼ì€ ë³€ê²½ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   ì‹¤ì œ ë³€ê²½í•˜ë ¤ë©´ --execute ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
    
    print("=" * 80)


if __name__ == "__main__":
    main()
